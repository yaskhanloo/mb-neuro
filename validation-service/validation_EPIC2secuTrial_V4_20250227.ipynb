{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automozed EPIC validation process against secuTrial data entries\n",
    "\n",
    "created by: Yasaman Safarkhanlo on 2024.10.07\n",
    "\n",
    "last modified: file name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import chardet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_modify_secuTrial_export(df):\n",
    "    df = df.drop([7])                   # Remove row 8 in Excel\n",
    "    df = df.iloc[6:]                    # Skip the first 6 rows\n",
    "    df.columns = df.iloc[0]             # Use row 6 as the header\n",
    "    df = df[1:].reset_index(drop=True)  # Drop the header row and reset index\n",
    "    return df\n",
    "\n",
    "def safe_read_file(file_path, custom_reader=None):\n",
    "    \"\"\"\n",
    "    Safely reads a file (Excel or CSV), with an option for a custom reader function.\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str or Path)\n",
    "        custom_reader (function, optional)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame\n",
    "    \"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    file_extension = file_path.suffix.lower()\n",
    "\n",
    "    try:\n",
    "        # Read based on file extension\n",
    "        if file_extension in [\".xlsx\", \".xls\"]:\n",
    "            df = pd.read_excel(file_path, engine='openpyxl' if file_extension == \".xlsx\" else 'xlrd', header=None)\n",
    "        elif file_extension == \".csv\":\n",
    "            df = pd.read_csv(file_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
    "        \n",
    "        # Apply custom reader if provided\n",
    "        return custom_reader(df) if custom_reader else df\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file at {file_path}: {e}\")\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_encoding(file_path):\n",
    "    \"\"\"\n",
    "    Detect the encoding of a file using chardet.\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str or Path): Path to the file\n",
    "        \n",
    "    Returns:\n",
    "        str: Detected encoding\n",
    "    \"\"\"\n",
    "    # Read a sample of the file to detect encoding\n",
    "    with open(file_path, 'rb') as f:\n",
    "        raw_data = f.read(10000)  # Read up to 10000 bytes\n",
    "    result = chardet.detect(raw_data)\n",
    "    return result['encoding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yaskhanloo/Developer/bern-storke-center/mb-neuro/.venv/lib/python3.9/site-packages/openpyxl/styles/stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
      "/Users/yaskhanloo/Developer/bern-storke-center/mb-neuro/.venv/lib/python3.9/site-packages/openpyxl/styles/stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading file at /Users/yaskhanloo/Developer/bern-storke-center/mb-neuro/data/EPIC-files/export-20250404/encounters.csv: 'utf-8' codec can't decode byte 0xf6 in position 596: invalid start byte\n",
      "Failed to load one or both dataframes.\n"
     ]
    }
   ],
   "source": [
    "#base_dir = Path(\"/data/app\")\n",
    "base_dir = Path(\"/Users/yaskhanloo/Developer/bern-storke-center/mb-neuro/data\")\n",
    "\n",
    "# Dynamically find the latest export folder\n",
    "latest_sT_export = max((base_dir / \"sT-files\").glob(\"export-*\"), key=lambda x: x.stat().st_mtime, default=None)\n",
    "latest_EPIC_export = max((base_dir / \"EPIC-files\").glob(\"export-*\"), key=lambda x: x.stat().st_mtime, default=None)\n",
    "\n",
    "if latest_sT_export:\n",
    "    secuTrial_base_dir = latest_sT_export\n",
    "    REVASC_base_dir = secuTrial_base_dir / \"REVASC\"\n",
    "else:\n",
    "    raise FileNotFoundError(\"No valid secuTrial export directory found.\")\n",
    "\n",
    "if latest_EPIC_export:\n",
    "    epic_base_dir = latest_EPIC_export\n",
    "else:\n",
    "    raise FileNotFoundError(\"No valid EPIC export directory found.\")\n",
    "\n",
    "# Define file paths\n",
    "file_path_secuTrial = secuTrial_base_dir / 'SSR_cases_of_2024.xlsx'\n",
    "file_path_REVASC = REVASC_base_dir / 'report_SSR01_20250218-105747.xlsx'\n",
    "\n",
    "#file_path_EPIC = epic_base_dir / 'encounters.xlsx'\n",
    "file_path_EPIC = epic_base_dir / 'encounters.csv'\n",
    "\n",
    "# Read files\n",
    "df_secuTrial = safe_read_file(file_path_secuTrial, custom_reader=read_and_modify_secuTrial_export)\n",
    "df_REVASC = safe_read_file(file_path_REVASC, custom_reader=read_and_modify_secuTrial_export)\n",
    "\n",
    "df_EPIC = safe_read_file(file_path_EPIC)\n",
    "\n",
    "# Print data frame sizes\n",
    "if df_secuTrial is not None and df_EPIC is not None and df_REVASC is not None:\n",
    "    print(f'df_secuTrial size: {df_secuTrial.shape}, df_REVASC: {df_REVASC.shape}, df_EPIC size: {df_EPIC.shape}')\n",
    "else:\n",
    "    print(\"Failed to load one or both dataframes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge all sT years into one file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO: read and merge all sT export files together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge all EPIC files into one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_single_file(file_path, merge_column, merged_df, prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Merges a single file into the main DataFrame with optional prefixing of columns.\n",
    "    Handles tab-delimited CSV files.\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str or Path): Path to file.\n",
    "        merge_column (str): Column name to merge on.\n",
    "        merged_df (pd.DataFrame): The main DataFrame to merge into.\n",
    "        prefix (str): Optional prefix to add to column names for this file.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Updated merged DataFrame.\n",
    "    \"\"\"\n",
    "    # Determine file extension\n",
    "    file_extension = file_path.suffix.lower()\n",
    "    \n",
    "    # Try to detect encoding for CSV files\n",
    "    detected_encoding = None\n",
    "    if file_extension == \".csv\":\n",
    "        detected_encoding = detect_encoding(file_path)\n",
    "        print(f\"Detected encoding for {file_path.name}: {detected_encoding}\")\n",
    "    \n",
    "    # Read file based on its extension\n",
    "    try:\n",
    "        if file_extension == \".xlsx\":\n",
    "            df = pd.read_excel(file_path, engine='openpyxl')\n",
    "        elif file_extension == \".xls\":\n",
    "            df = pd.read_excel(file_path, engine='xlrd')\n",
    "        elif file_extension == \".csv\":\n",
    "            # Try with the detected encoding and tab delimiter\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, encoding=detected_encoding, sep='\\t')\n",
    "            except:\n",
    "                # If that fails, try common encodings\n",
    "                encodings_to_try = ['latin1', 'iso-8859-1', 'cp1252', 'utf-8-sig']\n",
    "                for encoding in encodings_to_try:\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path, encoding=encoding, sep='\\t')\n",
    "                        print(f\"Successfully read {file_path.name} with encoding: {encoding}\")\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(f\"Failed with encoding {encoding}: {e}\")\n",
    "                else:\n",
    "                    # Last resort: try with different delimiters\n",
    "                    delimiters = [',', ';', '|']\n",
    "                    for delim in delimiters:\n",
    "                        try:\n",
    "                            df = pd.read_csv(file_path, encoding='latin1', sep=delim)\n",
    "                            print(f\"Successfully read {file_path.name} with delimiter: '{delim}'\")\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            print(f\"Failed with delimiter '{delim}': {e}\")\n",
    "                    else:\n",
    "                        raise ValueError(f\"Could not read file with any encoding or delimiter\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")\n",
    "        return merged_df  # Return the existing DataFrame without merging\n",
    "    \n",
    "    # Check if merge column exists\n",
    "    if merge_column not in df.columns:\n",
    "        print(f\"Warning: Merge column '{merge_column}' not found in {file_path.name}\")\n",
    "        print(f\"Available columns: {df.columns.tolist()}\")\n",
    "        return merged_df\n",
    "        \n",
    "    # Print number of columns in the current file\n",
    "    print(f\"File: {file_path.name} | Columns: {len(df.columns)} , Rows: {df.shape[0]}\")\n",
    "    \n",
    "    # Add prefix to columns except the merge column\n",
    "    df.rename(columns={col: f\"{prefix}{col}\" for col in df.columns if col != merge_column}, inplace=True)\n",
    "    \n",
    "    # Merge the DataFrame into the main DataFrame\n",
    "    if merged_df.empty:\n",
    "        return df\n",
    "    else:\n",
    "        # Check if merge column exists in both DataFrames\n",
    "        if merge_column in merged_df.columns:\n",
    "            return merged_df.merge(df, on=merge_column, how=\"outer\")\n",
    "        else:\n",
    "            print(f\"Warning: Merge column '{merge_column}' not found in merged DataFrame\")\n",
    "            print(f\"Merged DataFrame columns: {merged_df.columns.tolist()}\")\n",
    "            return merged_df\n",
    "\n",
    "def merge_excel_files(directory, merge_column):\n",
    "    \"\"\"\n",
    "    Merges all EPIC files in a directory based on a specific column, in a defined order.\n",
    "    \n",
    "    Parameters:\n",
    "        directory (str or Path): Directory containing files.\n",
    "        merge_column (str): Column name to use for merging files.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame\n",
    "    \"\"\"\n",
    "    # Define merge order and column prefixes\n",
    "    merge_order = {\n",
    "        \"encounters\": \"enct.\",\n",
    "        \"flowsheet\": \"flow.\",\n",
    "        \"imaging\": \"img.\",\n",
    "        \"lab\": \"lab.\",\n",
    "        \"medication\": \"med.\",\n",
    "        \"monitor\": \"mon.\"\n",
    "    }\n",
    "    \n",
    "    directory = Path(directory)\n",
    "    if not directory.exists():\n",
    "        raise FileNotFoundError(f\"Directory {directory} does not exist.\")\n",
    "    \n",
    "    merged_df = pd.DataFrame()\n",
    "    \n",
    "    # Merge files in the defined order\n",
    "    for keyword, prefix in merge_order.items():\n",
    "        files_found = False\n",
    "        for file in directory.glob(f\"*{keyword}*\"):\n",
    "            if file.suffix.lower() in [\".xlsx\", \".xls\", \".csv\"]:  # Check for valid file extensions\n",
    "                files_found = True\n",
    "                merged_df = merge_single_file(file, merge_column, merged_df, prefix)\n",
    "        \n",
    "        if not files_found and keyword != \"medication\":  # Special handling for medication(s)\n",
    "            # Try with similar names (e.g., check for \"medications\" if \"medication\" not found)\n",
    "            for file in directory.glob(f\"*{keyword}s*\"):\n",
    "                if file.suffix.lower() in [\".xlsx\", \".xls\", \".csv\"]:\n",
    "                    files_found = True\n",
    "                    merged_df = merge_single_file(file, merge_column, merged_df, prefix)\n",
    "            \n",
    "            if not files_found:\n",
    "                print(f\"No files matching '{keyword}' found in {directory}\")\n",
    "    \n",
    "    # Process any remaining files\n",
    "    for file in directory.glob(\"*\"):\n",
    "        if file.suffix.lower() in [\".xlsx\", \".xls\", \".csv\"]:\n",
    "            if not any(keyword in file.stem.lower() for keyword in merge_order):\n",
    "                merged_df = merge_single_file(file, merge_column, merged_df)\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files found in directory:\n",
      "  - medications.csv\n",
      "  - lab_results.csv\n",
      "  - merged_epic_data.csv\n",
      "  - flowsheet.csv\n",
      "  - encounters.csv\n",
      "  - monitoring.csv\n",
      "  - imaging.csv\n",
      "Detected encoding for encounters.csv: ISO-8859-1\n",
      "File: encounters.csv | Columns: 16 , Rows: 2288\n",
      "Detected encoding for flowsheet.csv: ascii\n",
      "File: flowsheet.csv | Columns: 31 , Rows: 2288\n",
      "Detected encoding for imaging.csv: ascii\n",
      "File: imaging.csv | Columns: 15 , Rows: 2288\n",
      "Detected encoding for lab_results.csv: ascii\n",
      "File: lab_results.csv | Columns: 14 , Rows: 2288\n",
      "Detected encoding for medications.csv: ISO-8859-1\n",
      "File: medications.csv | Columns: 23 , Rows: 2288\n",
      "Detected encoding for monitoring.csv: ISO-8859-1\n",
      "File: monitoring.csv | Columns: 13 , Rows: 2288\n",
      "Detected encoding for merged_epic_data.csv: utf-8\n",
      "Warning: Merge column 'PAT_ENC_CSN_ID' not found in merged_epic_data.csv\n",
      "Available columns: ['PAT_ENC_CSN_ID,enct.name_last,enct.name_first,enct.birth_date,enct.zip,enct.non_swiss,enct.arrival_date,enct.arrival_time,enct.sex,enct.transport,enct.living_pre,enct.height,enct.weight,enct.discharge_destinat,enct.discharge_date,enct.discharge_time,flow.FID,flow.CONTACT_DATE,flow.nih_admission,flow.gcs_admission,flow.bp_syst,flow.bp_diast,flow.firstangio_result,flow.ivt_start_date,flow.ivt_start_time,flow.rtpa_dose,flow.iat_stentintracran,flow.iat_stentextracran,flow.mca,flow.aca,flow.pca,flow.vertebrobasilar,flow.stroke_pre,flow.tia_pre,flow.ich_pre,flow.hypertension,flow.diabetes,flow.hyperlipidemia,flow.smoking,flow.atrialfib,flow.chd,flow.prostheticvalves,flow.lowoutput,flow.pad,flow.decompression,flow.decompression_date,img.FID,img.INCLUSION_STROKE_DOCUMENTATION,img.INCLUSION_STROKE_FLAG,img.firstimage_type,img.firstimage_time,img.firstangio_type,img.iat_mech,img.follow_mra,img.follow_cta,img.follow_ultrasound,img.follow_dsa,img.follow_tte,img.follow_tee,img.follow_holter,lab.FID,lab.INCLUSION_STROKE_DOCUMENTATION,lab.INCLUSION_STROKE_FLAG,lab.inr,lab.glucose,lab.admis_crp,lab.creatinine,lab.cholesterol_total,lab.cholesterol_ldl,lab.admis_haem,lab.admis_lecuco,lab.admis_platelets,lab.level_doac,med.INCLUSION_STROKE_DOCUMENTATION,med.INCLUSION_STROKE_FLAG,med.aspirin_pre,med.clopidogrel_pre,med.prasugrel_pre,med.ticagrelor_pre,med.dipyridamole_pre,med.vka_pre,med.rivaroxaban_pre,med.dabigatran_pre,med.apixaban_pre,med.edoxaban_pre,med.parenteralanticg_pre,med.antihypertensive_pre,med.antilipid_pre,med.hormone_pre,med.treat_antiplatelet,med.treat_anticoagulant,med.treat_ivt,med.statins_pre,med.antihyper_pre,med.other_pre,mon.FID,mon.INCLUSION_STROKE_DOCUMENTATION,mon.INCLUSION_STROKE_FLAG,mon.FLAG_USER,mon.FLAG_DATETIME,mon.ARZTBERICHT_STATIONAR_AUTHOR,mon.ADMISSION_DATE,mon.ED_ARRIVAL,mon.LYSE_DTTM,mon.DOOR_TO_NEEDLE_MINUTES,mon.GROIN_PUNCTURE,mon.DOOR_TO_GROIN']\n",
      "Merged DataFrame shape: (2288, 107)\n",
      "\n",
      "Sample of merged data (first 5 rows):\n",
      "   PAT_ENC_CSN_ID enct.name_last enct.name_first enct.birth_date enct.zip  \\\n",
      "0       102060077         Stucki          Thomas      28.04.1936     3112   \n",
      "1       102061151     Krähenbühl            Kurt      05.10.1940     3018   \n",
      "2       102061489         Rothen            Hans      07.04.1947     3503   \n",
      "3       102064008             Ak          Mehmet      01.04.1958     3422   \n",
      "4       102065045          Marti           Fritz      09.10.1929     3073   \n",
      "\n",
      "   enct.non_swiss enct.arrival_date enct.arrival_time  enct.sex  \\\n",
      "0               0          3/2/2024             10:04         1   \n",
      "1               0          3/2/2024             12:41         1   \n",
      "2               0          3/2/2024             13:17         1   \n",
      "3               0          3/3/2024              6:30         1   \n",
      "4               0          3/3/2024             11:49         1   \n",
      "\n",
      "   enct.transport  ...  mon.INCLUSION_STROKE_FLAG  mon.FLAG_USER  \\\n",
      "0               1  ...                         No            NaN   \n",
      "1               1  ...                         No            NaN   \n",
      "2               3  ...                         No            NaN   \n",
      "3               1  ...                         No            NaN   \n",
      "4               3  ...                         No            NaN   \n",
      "\n",
      "   mon.FLAG_DATETIME                   mon.ARZTBERICHT_STATIONAR_AUTHOR  \\\n",
      "0                NaN  SÖKELAND, GRETA CHARLOTTE / BRINER, MYRIAM SANDRA   \n",
      "1                NaN  FREY, SÉBASTIEN MARIO / FRANCA DALLA VECCHIA, ...   \n",
      "2                NaN                                      BÜRGE, MAXINE   \n",
      "3                NaN                                MARISON, FIONA ELEN   \n",
      "4                NaN  BOHLEN, STEFAN / KELLER, NINO / FRANCA DALLA V...   \n",
      "\n",
      "  mon.ADMISSION_DATE mon.ED_ARRIVAL  mon.LYSE_DTTM mon.DOOR_TO_NEEDLE_MINUTES  \\\n",
      "0            09:00.0        04:52.0            NaN                        NaN   \n",
      "1            49:00.0        41:34.0            NaN                        NaN   \n",
      "2            09:00.0        17:36.0            NaN                        NaN   \n",
      "3            32:00.0        30:54.0        50:00.0                       20.0   \n",
      "4            57:00.0        49:22.0            NaN                        NaN   \n",
      "\n",
      "   mon.GROIN_PUNCTURE  mon.DOOR_TO_GROIN  \n",
      "0                 NaN                NaN  \n",
      "1                 NaN                NaN  \n",
      "2                 NaN                NaN  \n",
      "3                 NaN                NaN  \n",
      "4                 NaN                NaN  \n",
      "\n",
      "[5 rows x 107 columns]\n",
      "\n",
      "Merged data saved to: /Users/yaskhanloo/Developer/bern-storke-center/EPIC-files/export-20250404/merged_epic_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Directory containing EPIC export files\n",
    "directory = epic_base_dir\n",
    "\n",
    "try:\n",
    "    # First, check what files are actually in the directory\n",
    "    print(\"Files found in directory:\")\n",
    "    all_files = list(Path(directory).glob(\"*\"))\n",
    "    for file in all_files:\n",
    "        print(f\"  - {file.name}\")\n",
    "    \n",
    "    # Then attempt the merge\n",
    "    df_EPIC_all = merge_excel_files(directory, merge_column=\"PAT_ENC_CSN_ID\")\n",
    "    print(f\"Merged DataFrame shape: {df_EPIC_all.shape}\")\n",
    "    \n",
    "    # Show a small sample of the merged data if successful\n",
    "    if not df_EPIC_all.empty:\n",
    "        print(\"\\nSample of merged data (first 5 rows):\")\n",
    "        print(df_EPIC_all.head())\n",
    "        \n",
    "        # Save the merged dataframe\n",
    "        output_path = Path(directory) / \"merged_epic_data.csv\"\n",
    "        df_EPIC_all.to_csv(output_path, index=False)\n",
    "        print(f\"\\nMerged data saved to: {output_path}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: Directory not found - {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Merging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REVASC merge with sT - single year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed columns in df_secuTrial: [nan]\n",
      "Unnamed columns in df_REVASC: [nan]\n",
      "df_secuTrial_w_REVAS size: (1795, 429)\n"
     ]
    }
   ],
   "source": [
    "# Check for unnamed columns in df_secuTrial\n",
    "unnamed_columns_secuTrial = [col for col in df_secuTrial.columns if not isinstance(col, str) or not col or col.startswith('Unnamed')]\n",
    "if unnamed_columns_secuTrial:\n",
    "    print(f'Unnamed columns in df_secuTrial: {unnamed_columns_secuTrial}')\n",
    "else:\n",
    "    print('No unnamed columns found in df_secuTrial.')\n",
    "\n",
    "# Check for unnamed columns in df_REVASC\n",
    "unnamed_columns_REVASC = [col for col in df_REVASC.columns if not isinstance(col, str) or not col or col.startswith('Unnamed')]\n",
    "if unnamed_columns_REVASC:\n",
    "    print(f'Unnamed columns in df_REVASC: {unnamed_columns_REVASC}')\n",
    "else:\n",
    "    print('No unnamed columns found in df_REVASC.')\n",
    "\n",
    "# Merge df_REVASC into df_secuTrial based on Case ID, adding suffix to shared columns\n",
    "df_secuTrial_w_REVAS = df_secuTrial.merge(\n",
    "    df_REVASC,\n",
    "    how='left',\n",
    "    left_on='Case ID',\n",
    "    right_on='CaseID',\n",
    "    suffixes=('', '.revas')  # No suffix for df_secuTrial, '.revas' for df_REVASC\n",
    ")\n",
    "\n",
    "df_secuTrial_w_REVAS.drop(columns=['CaseID'], inplace=True, errors='ignore')\n",
    "df_secuTrial_w_REVAS.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f'df_secuTrial_w_REVAS size: {df_secuTrial_w_REVAS.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add FID and SSR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_EPIC_all['FID'] = df_EPIC_all['img.FID'].fillna(0).astype(int)\n",
    "df_EPIC_all.insert(0, 'FID', df_EPIC_all.pop('FID'))\n",
    "\n",
    "df_secuTrial_w_REVAS['SSR'] = df_secuTrial_w_REVAS['Case ID'].str.extract(r'(\\d+)$').astype(int)\n",
    "df_secuTrial_w_REVAS.insert(1, 'SSR', df_secuTrial_w_REVAS.pop('SSR'))\n",
    "df_secuTrial_w_REVAS = df_secuTrial_w_REVAS.drop(columns=['nan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_log = pd.read_excel(base_dir / 'EPIC2sT-pipeline/Identification_log_SSR_2024_ohne PW.xlsx')\n",
    "\n",
    "# Set the first row as column names and drop it from the data\n",
    "id_log.columns = id_log.iloc[0]\n",
    "id_log = id_log.iloc[1:].reset_index(drop=True)  # Reset index for clarity\n",
    "\n",
    "# Rename columns for consistency\n",
    "id_log.rename(columns={'Fall-Nr.(FID)': 'FID', 'SSR Identification SSR-INS-000....': 'SSR'}, inplace=True)\n",
    "\n",
    "# Merge with df_EPIC_all on 'FID' and reorder columns\n",
    "df_EPIC_all = df_EPIC_all.merge(id_log[['FID', 'SSR']], on='FID', how='left')\n",
    "df_EPIC_all.insert(1, 'SSR', df_EPIC_all.pop('SSR'))  # Move 'SSR' to the second column\n",
    "\n",
    "# Merge with df_secuTrial_w_REVAS on 'SSR' and reorder columns\n",
    "df_secuTrial_w_REVAS = df_secuTrial_w_REVAS.merge(id_log[['SSR', 'FID']], on='SSR', how='left')\n",
    "df_secuTrial_w_REVAS.insert(0, 'FID', df_secuTrial_w_REVAS.pop('FID'))  # Move 'FID' to the first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['FID', 'SSR'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Find common column names\n",
    "common_columns = df_secuTrial_w_REVAS.columns.intersection(df_EPIC_all.columns)\n",
    "\n",
    "print(common_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find common values in 'FID' and 'SSR'\n",
    "common_values = df_secuTrial_w_REVAS[['FID', 'SSR']].merge(df_EPIC_all[['FID', 'SSR']], on=['FID', 'SSR'], how='inner')\n",
    "\n",
    "# Filter both DataFrames to keep only matching rows\n",
    "df_sT_common = df_secuTrial_w_REVAS.merge(common_values, on=['FID', 'SSR'], how='inner')\n",
    "df_ep_common = df_EPIC_all.merge(common_values, on=['FID', 'SSR'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1447, 430), (1447, 109))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sT_common.shape, df_ep_common.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file saved as /Users/yaskhanloo/Developer/bern-storke-center/EPIC-export-validation/validation-files/merged_lists.xlsx\n"
     ]
    }
   ],
   "source": [
    "output_dir = base_dir / 'EPIC-export-validation/validation-files'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "file_path = output_dir / \"merged_lists.xlsx\"\n",
    "with pd.ExcelWriter(file_path) as writer:\n",
    "    df_sT_common.to_excel(writer, sheet_name=\"secuTrial_list\", index=False)\n",
    "    df_ep_common.to_excel(writer, sheet_name=\"EPIC_list\", index=False)\n",
    "\n",
    "print(f\"Excel file saved as {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find rows that exist only in df1\n",
    "df_sT_only = df_secuTrial_w_REVAS.merge(df_EPIC_all[['FID', 'SSR']], on=['FID', 'SSR'], how='left', indicator=True).query('_merge == \"left_only\"').drop(columns=['_merge'])\n",
    "\n",
    "# Find rows that exist only in df2\n",
    "df_ep_only = df_EPIC_all.merge(df_secuTrial_w_REVAS[['FID', 'SSR']], on=['FID', 'SSR'], how='left', indicator=True).query('_merge == \"left_only\"').drop(columns=['_merge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((348, 430), (844, 109))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sT_only.shape, df_ep_only.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file saved as /Users/yaskhanloo/Developer/bern-storke-center/EPIC-export-validation/validation-files/missing_patients_2024.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Keep only the common columns in df1_only and df2_only\n",
    "df_sT_only_common_cols = df_sT_only[['FID', 'SSR', 'Last name', 'First name', 'DOB', 'Arrival at hospital']]\n",
    "df_ep_only_common_cols = df_ep_only[['FID', 'SSR','enct.name_last', 'enct.name_first', 'enct.birth_date', 'enct.arrival_date', \n",
    "                                   'mon.INCLUSION_STROKE_DOCUMENTATION', 'mon.INCLUSION_STROKE_FLAG']]\n",
    "\n",
    "# Save to an Excel file\n",
    "# Define output directory and ensure it exists\n",
    "output_dir = base_dir / 'EPIC-export-validation/validation-files'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "file_path = output_dir / \"missing_patients_2024.xlsx\"\n",
    "with pd.ExcelWriter(file_path) as writer:\n",
    "    df_sT_only_common_cols.to_excel(writer, sheet_name=\"secuTrial_list\", index=False)\n",
    "    df_ep_only_common_cols.to_excel(writer, sheet_name=\"EPIC_list\", index=False)\n",
    "\n",
    "print(f\"Excel file saved as {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### find the missing patients from EPIC and sT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column mappings\n",
    "df_sT_column_mapping = {\n",
    "    \"First name\": \"first_name\",\n",
    "    \"Last name\": \"last_name\",\n",
    "    \"DOB\": \"birth_date\",\n",
    "    \"Arrival at hospital\": \"arrival_date\"\n",
    "}\n",
    "\n",
    "df_ep_column_mapping = {\n",
    "    \"enct.name_first\": \"first_name\",\n",
    "    \"enct.name_last\": \"last_name\",\n",
    "    \"enct.birth_date\": \"birth_date\",\n",
    "    \"enct.arrival_date\": \"arrival_date\"\n",
    "}\n",
    "\n",
    "# Rename columns in both DataFrames to unify them\n",
    "df1_renamed = df_sT_only.rename(columns=df_sT_column_mapping)\n",
    "df2_renamed = df_ep_only.rename(columns=df_ep_column_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_renamed['birth_date'] = pd.to_datetime(df1_renamed['birth_date'], errors='coerce').dt.date\n",
    "df2_renamed['birth_date'] = pd.to_datetime(df2_renamed['birth_date'], errors='coerce').dt.date\n",
    "df1_renamed['arrival_date'] = pd.to_datetime(df1_renamed['arrival_date'], errors='coerce').dt.date\n",
    "df2_renamed['arrival_date'] = pd.to_datetime(df2_renamed['arrival_date'], errors='coerce').dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure columns exist before checking data types\n",
    "common_columns = [\"first_name\", \"last_name\", \"birth_date\"]\n",
    "\n",
    "# Function to check and match data types between two DataFrames\n",
    "def match_column_dtypes(df1, df2, columns):\n",
    "    for col in columns:\n",
    "        if col in df1.columns and col in df2.columns:\n",
    "            if df1[col].dtype != df2[col].dtype:\n",
    "                # Convert to a common type (string for names, datetime for dates)\n",
    "                if df1[col].dtype == \"object\" or df2[col].dtype == \"object\":\n",
    "                    df1[col] = df1[col].astype(str)\n",
    "                    df2[col] = df2[col].astype(str)\n",
    "                elif \"datetime\" in str(df1[col].dtype) or \"datetime\" in str(df2[col].dtype):\n",
    "                    df1[col] = pd.to_datetime(df1[col], errors='coerce')\n",
    "                    df2[col] = pd.to_datetime(df2[col], errors='coerce')\n",
    "\n",
    "# Match data types\n",
    "match_column_dtypes(df1_renamed, df2_renamed, common_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find common rows based on unified columns\n",
    "df_common_patients = df1_renamed.merge(df2_renamed, on=common_columns, how=\"inner\", suffixes=('_sT', '_ep'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 536)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_common_patients.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "SSR_sT",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "FID_sT",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "FID_ep",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "mon.INCLUSION_STROKE_DOCUMENTATION",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "mon.INCLUSION_STROKE_FLAG",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "arrival_date_sT",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "arrival_date_ep",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "first_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "last_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "birth_date",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "6af49d7e-af83-4d98-ad18-abda84c93d24",
       "rows": [
        [
         "0",
         "14411",
         "10534014",
         "10533127",
         "Yes",
         "Yes",
         "2024-09-09",
         "2024-09-09",
         "Katharina",
         "Schwendimann",
         "1950-04-04"
        ]
       ],
       "shape": {
        "columns": 10,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SSR_sT</th>\n",
       "      <th>FID_sT</th>\n",
       "      <th>FID_ep</th>\n",
       "      <th>mon.INCLUSION_STROKE_DOCUMENTATION</th>\n",
       "      <th>mon.INCLUSION_STROKE_FLAG</th>\n",
       "      <th>arrival_date_sT</th>\n",
       "      <th>arrival_date_ep</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>birth_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14411</td>\n",
       "      <td>10534014</td>\n",
       "      <td>10533127</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2024-09-09</td>\n",
       "      <td>2024-09-09</td>\n",
       "      <td>Katharina</td>\n",
       "      <td>Schwendimann</td>\n",
       "      <td>1950-04-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SSR_sT    FID_sT    FID_ep mon.INCLUSION_STROKE_DOCUMENTATION  \\\n",
       "0  14411  10534014  10533127                                Yes   \n",
       "\n",
       "  mon.INCLUSION_STROKE_FLAG arrival_date_sT arrival_date_ep first_name  \\\n",
       "0                       Yes      2024-09-09      2024-09-09  Katharina   \n",
       "\n",
       "      last_name  birth_date  \n",
       "0  Schwendimann  1950-04-04  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_common_patients[['SSR_sT', 'FID_sT', 'FID_ep', 'mon.INCLUSION_STROKE_DOCUMENTATION', 'mon.INCLUSION_STROKE_FLAG', 'arrival_date_sT', 'arrival_date_ep'] + common_columns].head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the selected columns to save\n",
    "selected_columns = ['SSR_sT', 'FID_sT', 'FID_ep', 'mon.INCLUSION_STROKE_DOCUMENTATION', \n",
    "                    'mon.INCLUSION_STROKE_FLAG', 'arrival_date_sT', 'arrival_date_ep'] + common_columns\n",
    "\n",
    "# Ensure df_common_patients exists and contains the required columns\n",
    "try:\n",
    "    # Select first 40 rows from the specified columns\n",
    "    df_to_save = df_common_patients[selected_columns].head(40)\n",
    "\n",
    "    # Define file path\n",
    "    file_path = output_dir / \"missing_patients_but exist.xlsx\"\n",
    "\n",
    "    # Save to Excel file\n",
    "    df_to_save.to_excel(file_path, index=False)\n",
    "\n",
    "    # Provide the download link\n",
    "    file_path\n",
    "\n",
    "except NameError:\n",
    "    print(\"Error: 'df_common_patients' is not defined. Please reload or redefine it.\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: Missing columns in 'df_common_patients': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix data types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read map file\n",
    "map_dir = base_dir / 'EPIC2sT-pipeline'\n",
    "map_file_name = 'map_epic2sT_code_V2_20250224.xlsx'\n",
    "\n",
    "map_file_path = map_dir / map_file_name\n",
    "\n",
    "# Load the column mapping Excel file\n",
    "df_mapping = pd.read_excel(map_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_mapping_dict, ep_type_conversion, ep_category_mappings, ep_datetime_formats = {}, {}, {}, {}\n",
    "sT_mapping_dict, sT_type_conversion, sT_category_mappings, sT_datetime_formats = {}, {}, {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(x):\n",
    "    if pd.isna(x) or str(x).strip().lower() in [\"\", \"null\", \"nan\", \"<na>\", \"nat\"]:\n",
    "        return np.nan\n",
    "    return x\n",
    "\n",
    "def convert_to_bool(x):\n",
    "    if pd.isna(x) or str(x).strip().lower() in [\"\", \"null\", \"nan\", \"<na>\", \"nat\"]:\n",
    "        return np.nan\n",
    "    return str(x).strip().lower() in [\"true\", \"yes\", \"1\"]\n",
    "\n",
    "def safe_datetime_conversion(s, col_name=None, source=None):\n",
    "    \"\"\"\n",
    "    Converts a column to datetime safely, using specific formats if available.\n",
    "    \"\"\"\n",
    "    # Apply specific formatting if applicable\n",
    "    if source == \"sT\" and col_name in sT_datetime_formats:\n",
    "        date_format = sT_datetime_formats[col_name]\n",
    "    elif source == \"ep\" and col_name in ep_datetime_formats:\n",
    "        date_format = ep_datetime_formats[col_name]\n",
    "    else:\n",
    "        date_format = None  # Use default parsing\n",
    "    \n",
    "    return pd.to_datetime(s.astype(str).str.strip(), format=date_format, errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_numeric_conversion(series, dtype):\n",
    "    \"\"\"Convert series to numeric type safely.\"\"\"\n",
    "    series = series.map(handle_missing_values)\n",
    "    if dtype == \"int\":\n",
    "        return pd.to_numeric(series, errors=\"coerce\").astype(\"Int64\")\n",
    "    elif dtype == \"float\":\n",
    "        return pd.to_numeric(series, errors=\"coerce\").astype(float)\n",
    "    elif dtype == \"float-2\":\n",
    "        return pd.to_numeric(series, errors=\"coerce\").round(2)\n",
    "    return series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sT data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_secutrial_mapping(row):\n",
    "    source_file = row[\"sT_exportFileName\"]\n",
    "    var_name = row[\"sT_varColumnName\"]\n",
    "    var_dtype = row[\"sT_varType\"]\n",
    "    var_map = row.get(\"sT_varMap\", None)  \n",
    "\n",
    "    # Determine column suffix\n",
    "    suffix = \".revas\" if isinstance(source_file, str) and \"REVASC\" in source_file else \"\"\n",
    "\n",
    "    # Ensure var_name is valid before concatenation\n",
    "    if pd.notna(var_name):\n",
    "        full_var_name = str(var_name).strip() + suffix\n",
    "        sT_mapping_dict[full_var_name] = var_name.strip()\n",
    "\n",
    "        # Define type conversion functions\n",
    "        if var_dtype == \"int\":\n",
    "            sT_type_conversion[full_var_name] = lambda s: pd.to_numeric(\n",
    "                s.map(handle_missing_values), errors=\"coerce\"\n",
    "            ).fillna(np.nan).astype(\"Int64\")\n",
    "\n",
    "        elif var_dtype == \"float\":\n",
    "            sT_type_conversion[full_var_name] = lambda s: pd.to_numeric(\n",
    "                s.map(handle_missing_values), errors=\"coerce\"\n",
    "            ).astype(float)\n",
    "\n",
    "        elif var_dtype == \"float-2\":\n",
    "            sT_type_conversion[full_var_name] = lambda s: pd.to_numeric(\n",
    "                s.map(handle_missing_values), errors=\"coerce\"\n",
    "            ).round(2)\n",
    "\n",
    "        elif var_dtype == \"bool\":\n",
    "            sT_type_conversion[full_var_name] = lambda s: s.map(convert_to_bool)\n",
    "\n",
    "        elif \"datetime\" in str(var_dtype):\n",
    "            format_map = {\n",
    "                \"dd.mm.yyyy\": \"%d.%m.%Y\",\n",
    "                \"yyyy-mm-dd\": \"%Y-%m-%d\",\n",
    "                \"yyyymmdd\": \"%Y%m%d\",\n",
    "                \"hh:mm:ss\": \"%H:%M:%S\",\n",
    "                \"hh:mm\": \"%H:%M\"\n",
    "            }\n",
    "\n",
    "            datetime_format = format_map.get(var_map, \"%Y%m%d\")\n",
    "            sT_datetime_formats[full_var_name] = datetime_format\n",
    "            sT_type_conversion[full_var_name] = safe_datetime_conversion\n",
    "\n",
    "        elif var_dtype == \"str\":\n",
    "            sT_type_conversion[full_var_name] = lambda s: s.astype(str).fillna(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_secuTrial_w_REVAS_2 = df_secuTrial_w_REVAS.rename(columns=sT_mapping_dict)\n",
    "for col, converter in sT_type_conversion.items():\n",
    "    if col in df_secuTrial_w_REVAS_2.columns:\n",
    "        df_secuTrial_w_REVAS_2[col] = converter(df_secuTrial_w_REVAS_2[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EPIC data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in df_mapping.iterrows():\n",
    "    source_file = row[\"EPIC_exportFileName\"]\n",
    "    var_name = row[\"EPIC_varColumnName\"]\n",
    "    var_dtype = row[\"EPIC_varType\"]\n",
    "    secutrial_var = row.get(\"sT_varColumnName\", None)\n",
    "\n",
    "    # Prefix mapping\n",
    "    prefix_map = {\n",
    "        \"encounter\": \"enct.\",\n",
    "        \"flowsheet\": \"flow.\",\n",
    "        \"imaging\": \"img.\",\n",
    "        \"lab\": \"lab.\",\n",
    "        \"medication\": \"med.\",\n",
    "        \"monitor\": \"mon.\"\n",
    "    }\n",
    "    prefix = next((v for k, v in prefix_map.items() if isinstance(source_file, str) and k.lower() in source_file.lower()), \"\")\n",
    "\n",
    "    if pd.notna(var_name):\n",
    "        full_var_name = prefix + str(var_name)\n",
    "        ep_mapping_dict[full_var_name] = full_var_name\n",
    "\n",
    "        if var_dtype == \"int\":\n",
    "            ep_type_conversion[full_var_name] = lambda s: pd.to_numeric(\n",
    "                s.map(handle_missing_values), errors=\"coerce\"\n",
    "            ).fillna(pd.NA).astype(\"Int64\")  # Fixed conversion to support missing values\n",
    "\n",
    "        elif var_dtype == \"float\":\n",
    "            ep_type_conversion[full_var_name] = lambda s: pd.to_numeric(\n",
    "                s.map(handle_missing_values), errors=\"coerce\"\n",
    "            ).astype(float)\n",
    "\n",
    "        elif var_dtype == \"bool\":\n",
    "            ep_type_conversion[full_var_name] = lambda s: s.map(convert_to_bool)\n",
    "\n",
    "        elif \"datetime\" in str(var_dtype):\n",
    "            datetime_format = sT_datetime_formats.get(secutrial_var, \"%Y%m%d\")\n",
    "            ep_datetime_formats[full_var_name] = datetime_format\n",
    "            ep_type_conversion[full_var_name] = lambda s: safe_datetime_conversion(s, full_var_name, \"ep\")  # ✅ Now properly applied\n",
    "\n",
    "        elif var_dtype == \"str\":\n",
    "            ep_type_conversion[full_var_name] = lambda s: s.astype(str).fillna(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_EPIC_all_2 = df_EPIC_all.rename(columns=ep_mapping_dict)\n",
    "\n",
    "for col, dtype in ep_type_conversion.items():\n",
    "    if col in df_EPIC_all_2.columns:\n",
    "        if dtype == \"bool\":\n",
    "            df_EPIC_all_2[col] = df_EPIC_all_2[col].map(convert_to_bool)\n",
    "        elif dtype == \"str\":\n",
    "            df_EPIC_all_2[col] = df_EPIC_all_2[col].astype(\"string\").fillna(\"\")\n",
    "        elif \"datetime\" in str(dtype):\n",
    "            df_EPIC_all_2[col] = safe_datetime_conversion(df_EPIC_all_2[col], col, \"ep\")\n",
    "        else:\n",
    "            df_EPIC_all_2[col] = safe_numeric_conversion(df_EPIC_all_2[col], dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map Key!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define reusable mappings\n",
    "yes_no_mapping = {0: 'no', 1: 'yes', False: 'no', True: 'yes'}\n",
    "bilateral_mapping = {0: 'no', 1: '', 2: 'right', 3: 'left', 4: 'bilateral'}\n",
    "prosthetic_valves_mapping = {0: 'None', 1: 'Biological', 2: 'Mechanical'}\n",
    "image_type_mapping = {1: 'CT', 2: 'MRI', 3: 'CT (external)', 4: 'MRI (external)'}\n",
    "transport_map = {1: 'Ambulance', 2: 'Helicopter', 3: 'Other (taxi,self,relatives,friends...)'}\n",
    "discharge_dest_map = {\n",
    "    1: 'Home', \n",
    "    3: 'Rehabilitation Hospital', \n",
    "    2: 'Other acute care hospital', \n",
    "    4: 'Nursing home, palliative care center, or other medical facility'\n",
    "}\n",
    "\n",
    "# Define common mappings for multiple columns\n",
    "yes_no_columns = [\n",
    "    'flow.iat_stentintracran', \n",
    "    'flow.iat_stentextracran', \n",
    "    'flow.stroke_pre', \n",
    "    'flow.tia_pre', \n",
    "    'flow.ich_pre',\n",
    "    'flow.hypertension', \n",
    "    'flow.diabetes', \n",
    "    'flow.hyperlipidemia', \n",
    "    'flow.smoking', \n",
    "    'flow.atrialfib', \n",
    "    'flow.chd',\n",
    "    'flow.lowoutput', \n",
    "    'flow.pad', \n",
    "    'flow.decompression', \n",
    "    'img.iat_mech', \n",
    "    'img.follow_mra', \n",
    "    'img.follow_cta',\n",
    "    'img.follow_ultrasound', \n",
    "    'img.follow_dsa', \n",
    "    'img.follow_tte', \n",
    "    'img.follow_tee', \n",
    "    'img.follow_holter',\n",
    "    'med.aspirin_pre', \n",
    "    'med.clopidogrel_pre', \n",
    "    'med.prasugrel_pre', \n",
    "    'med.ticagrelor_pre', \n",
    "    'med.dipyridamole_pre',\n",
    "    'med.vka_pre', \n",
    "    'med.rivaroxaban_pre', \n",
    "    'med.dabigatran_pre', \n",
    "    'med.apixaban_pre', \n",
    "    'med.edoxaban_pre',\n",
    "    'med.parenteralanticg_pre', \n",
    "    'med.antihypertensive_pre', \n",
    "    'med.antilipid_pre', \n",
    "    'med.hormone_pre',\n",
    "    'med.treat_antiplatelet', \n",
    "    'med.treat_anticoagulant', \n",
    "    'med.treat_ivt'\n",
    "]\n",
    "\n",
    "bilateral_columns = ['flow.mca', 'flow.aca', 'flow.pca', 'flow.vertebrobasilar']\n",
    "\n",
    "# Define value mappings for specific columns\n",
    "value_mappings = {\n",
    "    'enct.non_swiss': {True: 'yes'},\n",
    "    'enct.sex': {1: 'Male', 2: 'Female'},\n",
    "    'enct.transport': transport_map,\n",
    "    'enct.discharge_destinat': discharge_dest_map,  # Double-check mapping values\n",
    "    'flow.firstangio_result': {2: 'no', 3: 'yes'},  # Double-check mapping values\n",
    "    'flow.prostheticvalves': prosthetic_valves_mapping,\n",
    "    'img.firstimage_type': image_type_mapping,\n",
    "}\n",
    "\n",
    "# Apply yes_no_mapping and bilateral_mapping to multiple columns dynamically\n",
    "value_mappings.update({col: yes_no_mapping for col in yes_no_columns})\n",
    "value_mappings.update({col: bilateral_mapping for col in bilateral_columns})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare ep and sT dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main functioning one\n",
    "def compare_epic_secuTrial(epic_df, secuTrial_df, mapping_df, value_mappings=None):\n",
    "    \"\"\"\n",
    "    Compares values and data types between EPIC and secuTrial DataFrames using a mapping file,\n",
    "    but only for rows where FID and SSR match in both datasets.\n",
    "\n",
    "    Args:\n",
    "        epic_df (DataFrame): The EPIC dataset.\n",
    "        secuTrial_df (DataFrame): The SecuTrial dataset.\n",
    "        mapping_df (DataFrame): The mapping Excel file.\n",
    "        value_mappings (dict, optional): Dictionary for value conversions in EPIC.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with mismatched results in values and data types.\n",
    "        dict: Dictionary containing percentage statistics.\n",
    "    \"\"\"\n",
    "    if value_mappings is None:\n",
    "        value_mappings = {}\n",
    "        \n",
    "    epic_df.replace(-9999, pd.NA, inplace=True)\n",
    "    secuTrial_df.replace(-9999, pd.NA, inplace=True)\n",
    "\n",
    "    # Ensure necessary columns exist before comparison\n",
    "    if \"FID\" not in epic_df.columns or \"SSR\" not in epic_df.columns:\n",
    "        raise ValueError(\"EPIC DataFrame must contain 'FID' and 'SSR' columns.\")\n",
    "    if \"FID\" not in secuTrial_df.columns or \"SSR\" not in secuTrial_df.columns:\n",
    "        raise ValueError(\"SecuTrial DataFrame must contain 'FID' and 'SSR' columns.\")\n",
    "\n",
    "    # Create a set of (FID, SSR) pairs that exist in both DataFrames\n",
    "    matching_keys = set(epic_df[['FID', 'SSR']].apply(tuple, axis=1)) & set(secuTrial_df[['FID', 'SSR']].apply(tuple, axis=1))\n",
    "\n",
    "    # Store mismatched results\n",
    "    mismatched_results = []\n",
    "    match_count = 0\n",
    "    secu_missing_count = 0\n",
    "    epic_missing_count = 0\n",
    "    mismatch_count = 0\n",
    "    total_comparisons = 0\n",
    "    \n",
    "    # Apply value mappings to EPIC data before comparison and print changes\n",
    "    for col, mapping in value_mappings.items():\n",
    "        if col in epic_df.columns:\n",
    "            #print(f\"\\n🔹 Applying mapping to column: {col}\")\n",
    "            #print(\"Before Mapping:\")\n",
    "            #print(epic_df[col].value_counts(dropna=False))  # Show initial value distribution\n",
    "        \n",
    "            # Apply mapping\n",
    "            epic_df[col] = epic_df[col].map(mapping).fillna(epic_df[col])  \n",
    "            epic_df[col] = epic_df[col].astype(str)  # Ensure string for comparison\n",
    "\n",
    "            #print(\"\\nAfter Mapping:\")\n",
    "            #print(epic_df[col].value_counts(dropna=False))  # Show transformed value distribution\n",
    "    \n",
    "    # Iterate through the mapping file to compare columns\n",
    "    for _, row in mapping_df.iterrows():\n",
    "        epic_column_name = row.get('EPIC_varColumnName', None)\n",
    "        secuTrial_column_name = row.get('sT_varColumnName', None)\n",
    "        epic_dtype = row.get('EPIC_varType', None)\n",
    "        secuTrial_dtype = row.get('sT_varType', None)\n",
    "        column_source = row.get('EPIC_exportFileName', None)\n",
    "        secu_source = row.get('sT_exportFileName', None)\n",
    "\n",
    "        if not isinstance(epic_column_name, str) or not isinstance(secuTrial_column_name, str):\n",
    "            continue  # Skip if column names are missing\n",
    "\n",
    "        # Determine EPIC column prefix\n",
    "        prefix = \"\"\n",
    "        if isinstance(column_source, str):\n",
    "            if \"encounter\" in column_source:\n",
    "                prefix = \"enct.\"\n",
    "            elif \"flowsheet\" in column_source:\n",
    "                prefix = \"flow.\"\n",
    "            elif \"imaging\" in column_source:\n",
    "                prefix = \"img.\"\n",
    "            elif \"lab\" in column_source:\n",
    "                prefix = \"lab.\"\n",
    "            elif \"medication\" in column_source:\n",
    "                prefix = \"med.\"\n",
    "            elif \"monitor\" in column_source:\n",
    "                prefix = \"mon.\"\n",
    "\n",
    "        # Determine SecuTrial column suffix\n",
    "        suffix = \"\"\n",
    "        if isinstance(secu_source, str) and \"REVASC\" in secu_source:\n",
    "            suffix = \".revas\"\n",
    "\n",
    "        # Construct fully qualified column names\n",
    "        epic_col = f\"{prefix}{epic_column_name}\"  # EPIC column with prefix\n",
    "        secu_col = f\"{secuTrial_column_name}{suffix}\"  # SecuTrial column with suffix\n",
    "\n",
    "        # Count missing variables\n",
    "        if epic_col not in epic_df.columns and secu_col in secuTrial_df.columns:\n",
    "            epic_missing_count += 1\n",
    "            continue\n",
    "        elif secu_col not in secuTrial_df.columns and epic_col in epic_df.columns:\n",
    "            secu_missing_count += 1\n",
    "            continue\n",
    "        elif epic_col not in epic_df.columns and secu_col not in secuTrial_df.columns:\n",
    "            continue  # Skip comparison if column is missing in both\n",
    "\n",
    "        total_comparisons += 1\n",
    "\n",
    "        # Compare values for rows with matching (FID, SSR)\n",
    "        for fid, ssr in matching_keys:\n",
    "            epic_value = epic_df.loc[(epic_df[\"FID\"] == fid) & (epic_df[\"SSR\"] == ssr), epic_col].values\n",
    "            secu_value = secuTrial_df.loc[(secuTrial_df[\"FID\"] == fid) & (secuTrial_df[\"SSR\"] == ssr), secu_col].values\n",
    "\n",
    "            if len(epic_value) == 0 or len(secu_value) == 0:\n",
    "                continue  # Skip if value is missing in either DataFrame\n",
    "\n",
    "            # Convert both values to string before comparing\n",
    "            epic_value_str = str(epic_value[0])\n",
    "            secu_value_str = str(secu_value[0])\n",
    "\n",
    "            # Check if SecuTrial value is NaN or NaT\n",
    "            if pd.isna(secu_value[0]):\n",
    "                secu_missing_count += 1\n",
    "                continue  # Skip further comparison for this case\n",
    "\n",
    "            if epic_value_str == secu_value_str:\n",
    "                match_count += 1\n",
    "            else:  # Store mismatched results\n",
    "                mismatch_count += 1\n",
    "                mismatched_results.append({\n",
    "                    'FID': fid,\n",
    "                    'SSR': ssr,\n",
    "                    'EPIC Column': epic_col,\n",
    "                    'SecuTrial Column': secu_col,\n",
    "                    'EPIC Value': epic_value_str,\n",
    "                    'SecuTrial Value': secu_value_str,\n",
    "                    'EPIC Data Type': str(epic_df[epic_col].dtype),\n",
    "                    'SecuTrial Data Type': str(secuTrial_df[secu_col].dtype)\n",
    "                })\n",
    "\n",
    "\n",
    "    # Calculate percentages\n",
    "    total_vars = match_count + secu_missing_count + epic_missing_count + mismatch_count\n",
    "    percentage_stats = {\n",
    "        \"Matching Variables (%)\": round((match_count / total_vars) * 100, 2) if total_vars else 0,\n",
    "        \"Variables Missing in EPIC (%)\": round((epic_missing_count / total_vars) * 100, 2) if total_vars else 0,\n",
    "        \"Variables Missing in SecuTrial (%)\": round((secu_missing_count / total_vars) * 100, 2) if total_vars else 0,\n",
    "        \"Mismatched Variables (%)\": round((mismatch_count / total_vars) * 100, 2) if total_vars else 0\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(mismatched_results), percentage_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude code - 20250227\n",
    "\n",
    "def compare_epic_secuTrial(epic_df, secuTrial_df, mapping_df, value_mappings=None):\n",
    "    \"\"\"\n",
    "    Compares values and data types between EPIC and secuTrial DataFrames using a mapping file,\n",
    "    accounting for data type differences properly.\n",
    "\n",
    "    Args:\n",
    "        epic_df (DataFrame): The EPIC dataset.\n",
    "        secuTrial_df (DataFrame): The SecuTrial dataset.\n",
    "        mapping_df (DataFrame): The mapping Excel file with data type information.\n",
    "        value_mappings (dict, optional): Dictionary for value conversions in EPIC.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with mismatched results in values and data types.\n",
    "        dict: Dictionary containing percentage statistics.\n",
    "    \"\"\"\n",
    "    \n",
    "    if value_mappings is None:\n",
    "        value_mappings = {}\n",
    "        \n",
    "    # Create working copies to avoid modifying original dataframes\n",
    "    epic_df_copy = epic_df.copy()\n",
    "    secuTrial_df_copy = secuTrial_df.copy()\n",
    "    \n",
    "    # Replace missing value indicators\n",
    "    epic_df_copy.replace(-9999, pd.NA, inplace=True)\n",
    "    secuTrial_df_copy.replace(-9999, pd.NA, inplace=True)\n",
    "\n",
    "    # Ensure necessary columns exist before comparison\n",
    "    if \"FID\" not in epic_df_copy.columns or \"SSR\" not in epic_df_copy.columns:\n",
    "        raise ValueError(\"EPIC DataFrame must contain 'FID' and 'SSR' columns.\")\n",
    "    if \"FID\" not in secuTrial_df_copy.columns or \"SSR\" not in secuTrial_df_copy.columns:\n",
    "        raise ValueError(\"SecuTrial DataFrame must contain 'FID' and 'SSR' columns.\")\n",
    "\n",
    "    # Create a set of (FID, SSR) pairs that exist in both DataFrames\n",
    "    matching_keys = set(epic_df_copy[['FID', 'SSR']].apply(tuple, axis=1)) & set(secuTrial_df_copy[['FID', 'SSR']].apply(tuple, axis=1))\n",
    "\n",
    "    # Store mismatched results\n",
    "    mismatched_results = []\n",
    "    match_count = 0\n",
    "    secu_missing_count = 0\n",
    "    epic_missing_count = 0\n",
    "    mismatch_count = 0\n",
    "    total_comparisons = 0\n",
    "    \n",
    "    # Helper function to convert values to the correct type\n",
    "    def convert_to_type(value, target_type):\n",
    "        \"\"\"Convert value to specified type\"\"\"\n",
    "        if pd.isna(value):\n",
    "            return value\n",
    "            \n",
    "        # Handle various data types\n",
    "        if not isinstance(target_type, str):\n",
    "            return value  # If no type specified, return as is\n",
    "            \n",
    "        if target_type.lower() in ['int', 'integer', 'int64', 'int32']:\n",
    "            try:\n",
    "                return int(float(value)) if value != '' else pd.NA\n",
    "            except (ValueError, TypeError):\n",
    "                return pd.NA\n",
    "        elif target_type.lower() in ['float', 'double', 'numeric', 'float64', 'float32']:\n",
    "            try:\n",
    "                return float(value) if value != '' else pd.NA\n",
    "            except (ValueError, TypeError):\n",
    "                return pd.NA\n",
    "        elif target_type.lower() in ['date', 'datetime', 'timestamp']:\n",
    "            try:\n",
    "                return pd.to_datetime(value) if value else pd.NA\n",
    "            except (ValueError, TypeError):\n",
    "                return pd.NA\n",
    "        elif target_type.lower() in ['bool', 'boolean']:\n",
    "            if isinstance(value, bool):\n",
    "                return value\n",
    "            elif isinstance(value, (int, float)):\n",
    "                return bool(value)\n",
    "            elif isinstance(value, str):\n",
    "                return value.lower() in ['true', 'yes', 'y', '1', 't']\n",
    "            else:\n",
    "                return pd.NA\n",
    "        else:\n",
    "            # Default to string for text, categorical, etc.\n",
    "            return str(value) if value is not None else pd.NA\n",
    "    \n",
    "    # Helper function to check if values are equivalent\n",
    "    def equivalent_values(val1, val2, epic_type, secu_type):\n",
    "        \"\"\"Compare values with type awareness\"\"\"\n",
    "        # Handle NaN values consistently\n",
    "        if pd.isna(val1) and pd.isna(val2):\n",
    "            return True\n",
    "        elif pd.isna(val1) or pd.isna(val2):\n",
    "            return False\n",
    "            \n",
    "        # Special handling for numeric types\n",
    "        if epic_type.lower() in ['int', 'integer', 'float', 'double', 'numeric', 'int64', 'int32', 'float64', 'float32'] and \\\n",
    "           secu_type.lower() in ['int', 'integer', 'float', 'double', 'numeric', 'int64', 'int32', 'float64', 'float32']:\n",
    "            try:\n",
    "                return abs(float(val1) - float(val2)) < 1e-6\n",
    "            except (ValueError, TypeError):\n",
    "                return False\n",
    "        \n",
    "        # Date comparison\n",
    "        elif epic_type.lower() in ['date', 'datetime', 'timestamp'] and \\\n",
    "             secu_type.lower() in ['date', 'datetime', 'timestamp']:\n",
    "            try:\n",
    "                dt1 = pd.to_datetime(val1)\n",
    "                dt2 = pd.to_datetime(val2)\n",
    "                return dt1 == dt2\n",
    "            except (ValueError, TypeError):\n",
    "                return False\n",
    "                \n",
    "        # Boolean comparison\n",
    "        elif epic_type.lower() in ['bool', 'boolean'] and \\\n",
    "             secu_type.lower() in ['bool', 'boolean']:\n",
    "            bool_truthy = ['true', 'yes', 'y', '1', 't', 'True', 'TRUE', 1, True]\n",
    "            bool_falsy = ['false', 'no', 'n', '0', 'f', 'False', 'FALSE', 0, False]\n",
    "            \n",
    "            val1_bool = val1 in bool_truthy\n",
    "            val2_bool = val2 in bool_truthy\n",
    "            return val1_bool == val2_bool\n",
    "            \n",
    "        # String comparison (case insensitive)\n",
    "        elif isinstance(val1, str) and isinstance(val2, str):\n",
    "            return val1.strip().lower() == val2.strip().lower()\n",
    "            \n",
    "        # Default comparison\n",
    "        else:\n",
    "            return str(val1) == str(val2)\n",
    "    \n",
    "    # Apply value mappings to EPIC data before comparison\n",
    "    modified_columns = set()\n",
    "    for col, mapping in value_mappings.items():\n",
    "        if col in epic_df_copy.columns:\n",
    "            epic_df_copy[col] = epic_df_copy[col].map(lambda x: mapping.get(x, x))\n",
    "            modified_columns.add(col)\n",
    "    \n",
    "    # Iterate through the mapping file to compare columns\n",
    "    for _, row in mapping_df.iterrows():\n",
    "        epic_column_name = row.get('EPIC_varColumnName', None)\n",
    "        secuTrial_column_name = row.get('sT_varColumnName', None)\n",
    "        epic_dtype = row.get('EPIC_varType', 'text')  # Default to text if not specified\n",
    "        secuTrial_dtype = row.get('sT_varType', 'text')  # Default to text if not specified\n",
    "        column_source = row.get('EPIC_exportFileName', None)\n",
    "        secu_source = row.get('sT_exportFileName', None)\n",
    "\n",
    "        if not isinstance(epic_column_name, str) or not isinstance(secuTrial_column_name, str):\n",
    "            continue  # Skip if column names are missing\n",
    "\n",
    "        # Determine EPIC column prefix\n",
    "        prefix = \"\"\n",
    "        if isinstance(column_source, str):\n",
    "            if \"encounter\" in column_source:\n",
    "                prefix = \"enct.\"\n",
    "            elif \"flowsheet\" in column_source:\n",
    "                prefix = \"flow.\"\n",
    "            elif \"imaging\" in column_source:\n",
    "                prefix = \"img.\"\n",
    "            elif \"lab\" in column_source:\n",
    "                prefix = \"lab.\"\n",
    "            elif \"medication\" in column_source:\n",
    "                prefix = \"med.\"\n",
    "            elif \"monitor\" in column_source:\n",
    "                prefix = \"mon.\"\n",
    "\n",
    "        # Determine SecuTrial column suffix\n",
    "        suffix = \"\"\n",
    "        if isinstance(secu_source, str) and \"REVASC\" in secu_source:\n",
    "            suffix = \".revas\"\n",
    "\n",
    "        # Construct fully qualified column names\n",
    "        epic_col = f\"{prefix}{epic_column_name}\"  # EPIC column with prefix\n",
    "        secu_col = f\"{secuTrial_column_name}{suffix}\"  # SecuTrial column with suffix\n",
    "\n",
    "        # Check if columns exist in respective DataFrames\n",
    "        if epic_col not in epic_df_copy.columns and secu_col in secuTrial_df_copy.columns:\n",
    "            epic_missing_count += 1\n",
    "            continue\n",
    "        elif secu_col not in secuTrial_df_copy.columns and epic_col in epic_df_copy.columns:\n",
    "            secu_missing_count += 1\n",
    "            continue\n",
    "        elif epic_col not in epic_df_copy.columns and secu_col not in secuTrial_df_copy.columns:\n",
    "            continue  # Skip comparison if column is missing in both\n",
    "\n",
    "        total_comparisons += 1\n",
    "\n",
    "        # Compare values for rows with matching (FID, SSR)\n",
    "        for fid, ssr in matching_keys:\n",
    "            epic_row = epic_df_copy.loc[(epic_df_copy[\"FID\"] == fid) & (epic_df_copy[\"SSR\"] == ssr)]\n",
    "            secu_row = secuTrial_df_copy.loc[(secuTrial_df_copy[\"FID\"] == fid) & (secuTrial_df_copy[\"SSR\"] == ssr)]\n",
    "            \n",
    "            if epic_row.empty or secu_row.empty:\n",
    "                continue  # Skip if no matching row found\n",
    "                \n",
    "            epic_value = epic_row[epic_col].iloc[0] if epic_col in epic_row else pd.NA\n",
    "            secu_value = secu_row[secu_col].iloc[0] if secu_col in secu_row else pd.NA\n",
    "            \n",
    "            # Skip comparison if both values are missing\n",
    "            if pd.isna(epic_value) and pd.isna(secu_value):\n",
    "                match_count += 1\n",
    "                continue\n",
    "                \n",
    "            # Skip and count as missing if SecuTrial value is NaN\n",
    "            if pd.isna(secu_value) and not pd.isna(epic_value):\n",
    "                secu_missing_count += 1\n",
    "                continue\n",
    "                \n",
    "            # Skip and count as missing if EPIC value is NaN\n",
    "            if pd.isna(epic_value) and not pd.isna(secu_value):\n",
    "                epic_missing_count += 1\n",
    "                continue\n",
    "\n",
    "            # Convert values according to their intended data types\n",
    "            epic_value_converted = convert_to_type(epic_value, epic_dtype)\n",
    "            secu_value_converted = convert_to_type(secu_value, secuTrial_dtype)\n",
    "            \n",
    "            # Compare values with type awareness\n",
    "            if equivalent_values(epic_value_converted, secu_value_converted, epic_dtype, secuTrial_dtype):\n",
    "                match_count += 1\n",
    "            else:\n",
    "                mismatch_count += 1\n",
    "                mismatched_results.append({\n",
    "                    'FID': fid,\n",
    "                    'SSR': ssr,\n",
    "                    'EPIC Column': epic_col,\n",
    "                    'SecuTrial Column': secu_col,\n",
    "                    'EPIC Value': str(epic_value),\n",
    "                    'SecuTrial Value': str(secu_value),\n",
    "                    'EPIC Expected Type': epic_dtype,\n",
    "                    'SecuTrial Expected Type': secuTrial_dtype,\n",
    "                    'EPIC Actual Type': type(epic_value).__name__,\n",
    "                    'SecuTrial Actual Type': type(secu_value).__name__\n",
    "                })\n",
    "\n",
    "    # Calculate percentages\n",
    "    total_compared = match_count + mismatch_count + secu_missing_count + epic_missing_count\n",
    "    percentage_stats = {\n",
    "        \"Matching Variables (%)\": round((match_count / total_compared) * 100, 2) if total_compared else 0,\n",
    "        \"Variables Missing in EPIC (%)\": round((epic_missing_count / total_compared) * 100, 2) if total_compared else 0,\n",
    "        \"Variables Missing in SecuTrial (%)\": round((secu_missing_count / total_compared) * 100, 2) if total_compared else 0,\n",
    "        \"Mismatched Variables (%)\": round((mismatch_count / total_compared) * 100, 2) if total_compared else 0,\n",
    "        \"Total Comparisons\": total_compared,\n",
    "        \"Matches\": match_count,\n",
    "        \"EPIC Missing\": epic_missing_count,\n",
    "        \"SecuTrial Missing\": secu_missing_count,\n",
    "        \"Mismatches\": mismatch_count\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(mismatched_results), percentage_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude code 2\n",
    "def compare_epic_secuTrial(epic_df, secuTrial_df, mapping_df, value_mappings=None):\n",
    "    \"\"\"\n",
    "    Compares values and data types between EPIC and secuTrial DataFrames using a mapping file,\n",
    "    accounting for data type differences properly.\n",
    "\n",
    "    Args:\n",
    "        epic_df (DataFrame): The EPIC dataset.\n",
    "        secuTrial_df (DataFrame): The SecuTrial dataset.\n",
    "        mapping_df (DataFrame): The mapping Excel file with data type information.\n",
    "        value_mappings (dict, optional): Dictionary for value conversions in EPIC.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with mismatched results in values and data types.\n",
    "        dict: Dictionary containing percentage statistics.\n",
    "    \"\"\"\n",
    "    \n",
    "    if value_mappings is None:\n",
    "        value_mappings = {}\n",
    "        \n",
    "    # Create working copies to avoid modifying original dataframes\n",
    "    epic_df_copy = epic_df.copy()\n",
    "    secuTrial_df_copy = secuTrial_df.copy()\n",
    "    \n",
    "    # Replace missing value indicators\n",
    "    epic_df_copy.replace(-9999, pd.NA, inplace=True)\n",
    "    secuTrial_df_copy.replace(-9999, pd.NA, inplace=True)\n",
    "\n",
    "    # Ensure necessary columns exist before comparison\n",
    "    if \"FID\" not in epic_df_copy.columns or \"SSR\" not in epic_df_copy.columns:\n",
    "        raise ValueError(\"EPIC DataFrame must contain 'FID' and 'SSR' columns.\")\n",
    "    if \"FID\" not in secuTrial_df_copy.columns or \"SSR\" not in secuTrial_df_copy.columns:\n",
    "        raise ValueError(\"SecuTrial DataFrame must contain 'FID' and 'SSR' columns.\")\n",
    "\n",
    "    # Create a set of (FID, SSR) pairs that exist in both DataFrames\n",
    "    matching_keys = set(epic_df_copy[['FID', 'SSR']].apply(tuple, axis=1)) & set(secuTrial_df_copy[['FID', 'SSR']].apply(tuple, axis=1))\n",
    "\n",
    "    # Store mismatched results\n",
    "    mismatched_results = []\n",
    "    match_count = 0\n",
    "    secu_missing_count = 0\n",
    "    epic_missing_count = 0\n",
    "    mismatch_count = 0\n",
    "    total_comparisons = 0\n",
    "    \n",
    "    # Helper function to convert values to the correct type\n",
    "    def convert_to_type(value, target_type):\n",
    "        \"\"\"Convert value to specified type\"\"\"\n",
    "        if pd.isna(value):\n",
    "            return value\n",
    "            \n",
    "        # Handle various data types\n",
    "        if not isinstance(target_type, str):\n",
    "            return value  # If no type specified, return as is\n",
    "            \n",
    "        if target_type.lower() in ['int', 'integer', 'int64', 'int32']:\n",
    "            try:\n",
    "                return int(float(value)) if value != '' else pd.NA\n",
    "            except (ValueError, TypeError):\n",
    "                return pd.NA\n",
    "        elif target_type.lower() in ['float', 'double', 'numeric', 'float64', 'float32']:\n",
    "            try:\n",
    "                return float(value) if value != '' else pd.NA\n",
    "            except (ValueError, TypeError):\n",
    "                return pd.NA\n",
    "        elif target_type.lower() in ['date', 'datetime', 'timestamp']:\n",
    "            try:\n",
    "                return pd.to_datetime(value) if value else pd.NA\n",
    "            except (ValueError, TypeError):\n",
    "                return pd.NA\n",
    "        elif target_type.lower() in ['bool', 'boolean']:\n",
    "            if isinstance(value, bool):\n",
    "                return value\n",
    "            elif isinstance(value, (int, float)):\n",
    "                return bool(value)\n",
    "            elif isinstance(value, str):\n",
    "                return value.lower() in ['true', 'yes', 'y', '1', 't']\n",
    "            else:\n",
    "                return pd.NA\n",
    "        else:\n",
    "            # Default to string for text, categorical, etc.\n",
    "            return str(value) if value is not None else pd.NA\n",
    "    \n",
    "    # Helper function to check if values are equivalent\n",
    "    def equivalent_values(val1, val2):\n",
    "        \"\"\"Compare values directly\"\"\"\n",
    "        # Handle NaN values consistently\n",
    "        if pd.isna(val1) and pd.isna(val2):\n",
    "            return True\n",
    "        elif pd.isna(val1) or pd.isna(val2):\n",
    "            return False\n",
    "            \n",
    "        # Special handling for numeric types\n",
    "        if isinstance(val1, (int, float)) and isinstance(val2, (int, float)):\n",
    "            try:\n",
    "                return abs(float(val1) - float(val2)) < 1e-6\n",
    "            except (ValueError, TypeError):\n",
    "                return False\n",
    "        \n",
    "        # Date comparison\n",
    "        elif isinstance(val1, pd.Timestamp) and isinstance(val2, pd.Timestamp):\n",
    "            return val1 == val2\n",
    "                \n",
    "        # Boolean comparison\n",
    "        elif isinstance(val1, bool) and isinstance(val2, bool):\n",
    "            return val1 == val2\n",
    "            \n",
    "        # String comparison (case insensitive)\n",
    "        elif isinstance(val1, str) and isinstance(val2, str):\n",
    "            return val1.strip().lower() == val2.strip().lower()\n",
    "            \n",
    "        # Default comparison\n",
    "        else:\n",
    "            return str(val1) == str(val2)\n",
    "    \n",
    "    # Build a column mapping dictionary for easier lookups\n",
    "    column_mappings = {}\n",
    "    column_types = {}\n",
    "    \n",
    "    for _, row in mapping_df.iterrows():\n",
    "        epic_column_name = row.get('EPIC_varColumnName', None)\n",
    "        secuTrial_column_name = row.get('sT_varColumnName', None)\n",
    "        epic_dtype = row.get('EPIC_varType', 'text')  # Default to text if not specified\n",
    "        secuTrial_dtype = row.get('sT_varType', 'text')  # Default to text if not specified\n",
    "        column_source = row.get('EPIC_exportFileName', None)\n",
    "        secu_source = row.get('sT_exportFileName', None)\n",
    "\n",
    "        if not isinstance(epic_column_name, str) or not isinstance(secuTrial_column_name, str):\n",
    "            continue  # Skip if column names are missing\n",
    "\n",
    "        # Determine EPIC column prefix\n",
    "        prefix = \"\"\n",
    "        if isinstance(column_source, str):\n",
    "            if \"encounter\" in column_source:\n",
    "                prefix = \"enct.\"\n",
    "            elif \"flowsheet\" in column_source:\n",
    "                prefix = \"flow.\"\n",
    "            elif \"imaging\" in column_source:\n",
    "                prefix = \"img.\"\n",
    "            elif \"lab\" in column_source:\n",
    "                prefix = \"lab.\"\n",
    "            elif \"medication\" in column_source:\n",
    "                prefix = \"med.\"\n",
    "            elif \"monitor\" in column_source:\n",
    "                prefix = \"mon.\"\n",
    "\n",
    "        # Determine SecuTrial column suffix\n",
    "        suffix = \"\"\n",
    "        if isinstance(secu_source, str) and \"REVASC\" in secu_source:\n",
    "            suffix = \".revas\"\n",
    "\n",
    "        # Construct fully qualified column names\n",
    "        epic_col = f\"{prefix}{epic_column_name}\"  # EPIC column with prefix\n",
    "        secu_col = f\"{secuTrial_column_name}{suffix}\"  # SecuTrial column with suffix\n",
    "        \n",
    "        # Store the mapping\n",
    "        column_mappings[epic_col] = secu_col\n",
    "        column_types[epic_col] = {'epic_type': epic_dtype, 'secu_type': secuTrial_dtype}\n",
    "    \n",
    "    # First, apply value mappings to EPIC data\n",
    "    for col, mapping in value_mappings.items():\n",
    "        if col in epic_df_copy.columns:\n",
    "            epic_df_copy[col] = epic_df_copy[col].map(lambda x: mapping.get(x, x))\n",
    "    \n",
    "    # Next, convert EPIC data types to match secuTrial types\n",
    "    for epic_col, secu_col in column_mappings.items():\n",
    "        if epic_col in epic_df_copy.columns and secu_col in secuTrial_df_copy.columns:\n",
    "            target_type = column_types[epic_col]['secu_type']\n",
    "            epic_df_copy[epic_col] = epic_df_copy[epic_col].apply(lambda x: convert_to_type(x, target_type))\n",
    "    \n",
    "    # Now compare the values\n",
    "    for epic_col, secu_col in column_mappings.items():\n",
    "        # Check if columns exist in respective DataFrames\n",
    "        if epic_col not in epic_df_copy.columns and secu_col in secuTrial_df_copy.columns:\n",
    "            epic_missing_count += 1\n",
    "            continue\n",
    "        elif secu_col not in secuTrial_df_copy.columns and epic_col in epic_df_copy.columns:\n",
    "            secu_missing_count += 1\n",
    "            continue\n",
    "        elif epic_col not in epic_df_copy.columns and secu_col not in secuTrial_df_copy.columns:\n",
    "            continue  # Skip comparison if column is missing in both\n",
    "\n",
    "        total_comparisons += 1\n",
    "        \n",
    "        # Get data types for this column pair\n",
    "        epic_dtype = column_types[epic_col]['epic_type']\n",
    "        secuTrial_dtype = column_types[epic_col]['secu_type']\n",
    "\n",
    "        # Compare values for rows with matching (FID, SSR)\n",
    "        for fid, ssr in matching_keys:\n",
    "            epic_row = epic_df_copy.loc[(epic_df_copy[\"FID\"] == fid) & (epic_df_copy[\"SSR\"] == ssr)]\n",
    "            secu_row = secuTrial_df_copy.loc[(secuTrial_df_copy[\"FID\"] == fid) & (secuTrial_df_copy[\"SSR\"] == ssr)]\n",
    "            \n",
    "            if epic_row.empty or secu_row.empty:\n",
    "                continue  # Skip if no matching row found\n",
    "                \n",
    "            epic_value = epic_row[epic_col].iloc[0] if epic_col in epic_row else pd.NA\n",
    "            secu_value = secu_row[secu_col].iloc[0] if secu_col in secu_row else pd.NA\n",
    "            \n",
    "            # Skip comparison if both values are missing\n",
    "            if pd.isna(epic_value) and pd.isna(secu_value):\n",
    "                match_count += 1\n",
    "                continue\n",
    "                \n",
    "            # Skip and count as missing if SecuTrial value is NaN\n",
    "            if pd.isna(secu_value) and not pd.isna(epic_value):\n",
    "                secu_missing_count += 1\n",
    "                continue\n",
    "                \n",
    "            # Skip and count as missing if EPIC value is NaN\n",
    "            if pd.isna(epic_value) and not pd.isna(secu_value):\n",
    "                epic_missing_count += 1\n",
    "                continue\n",
    "\n",
    "            # Compare values directly (no need to convert again)\n",
    "            if equivalent_values(epic_value, secu_value):\n",
    "                match_count += 1\n",
    "            else:\n",
    "                mismatch_count += 1\n",
    "                mismatched_results.append({\n",
    "                    'FID': fid,\n",
    "                    'SSR': ssr,\n",
    "                    'EPIC Column': epic_col,\n",
    "                    'SecuTrial Column': secu_col,\n",
    "                    'EPIC Value': str(epic_value),\n",
    "                    'SecuTrial Value': str(secu_value),\n",
    "                    'EPIC Expected Type': epic_dtype,\n",
    "                    'SecuTrial Expected Type': secuTrial_dtype,\n",
    "                    'EPIC Actual Type': type(epic_value).__name__,\n",
    "                    'SecuTrial Actual Type': type(secu_value).__name__\n",
    "                })\n",
    "\n",
    "    # Calculate percentages\n",
    "    total_compared = match_count + mismatch_count + secu_missing_count + epic_missing_count\n",
    "    percentage_stats = {\n",
    "        \"Matching Variables (%)\": round((match_count / total_compared) * 100, 2) if total_compared else 0,\n",
    "        \"Variables Missing in EPIC (%)\": round((epic_missing_count / total_compared) * 100, 2) if total_compared else 0,\n",
    "        \"Variables Missing in SecuTrial (%)\": round((secu_missing_count / total_compared) * 100, 2) if total_compared else 0,\n",
    "        \"Mismatched Variables (%)\": round((mismatch_count / total_compared) * 100, 2) if total_compared else 0,\n",
    "        \"Total Comparisons\": total_compared,\n",
    "        \"Matches\": match_count,\n",
    "        \"EPIC Missing\": epic_missing_count,\n",
    "        \"SecuTrial Missing\": secu_missing_count,\n",
    "        \"Mismatches\": mismatch_count\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(mismatched_results), percentage_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude code 3\n",
    "def compare_epic_secuTrial(epic_df, secuTrial_df, mapping_df, value_mappings=None):\n",
    "    \"\"\"\n",
    "    Compares values and data types between EPIC and secuTrial DataFrames using a mapping file,\n",
    "    accounting for data type differences properly.\n",
    "\n",
    "    Args:\n",
    "        epic_df (DataFrame): The EPIC dataset.\n",
    "        secuTrial_df (DataFrame): The SecuTrial dataset.\n",
    "        mapping_df (DataFrame): The mapping Excel file with data type information.\n",
    "        value_mappings (dict, optional): Dictionary for value conversions in EPIC.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with mismatched results in values and data types.\n",
    "        dict: Dictionary containing percentage statistics.\n",
    "    \"\"\"\n",
    "    \n",
    "    import pandas as pd\n",
    "    import re\n",
    "    \n",
    "    if value_mappings is None:\n",
    "        value_mappings = {}\n",
    "        \n",
    "    # Create working copies to avoid modifying original dataframes\n",
    "    epic_df_copy = epic_df.copy()\n",
    "    secuTrial_df_copy = secuTrial_df.copy()\n",
    "    \n",
    "    # Replace missing value indicators\n",
    "    epic_df_copy.replace(-9999, pd.NA, inplace=True)\n",
    "    secuTrial_df_copy.replace(-9999, pd.NA, inplace=True)\n",
    "\n",
    "    # Ensure necessary columns exist before comparison\n",
    "    if \"FID\" not in epic_df_copy.columns or \"SSR\" not in epic_df_copy.columns:\n",
    "        raise ValueError(\"EPIC DataFrame must contain 'FID' and 'SSR' columns.\")\n",
    "    if \"FID\" not in secuTrial_df_copy.columns or \"SSR\" not in secuTrial_df_copy.columns:\n",
    "        raise ValueError(\"SecuTrial DataFrame must contain 'FID' and 'SSR' columns.\")\n",
    "\n",
    "    # Create a set of (FID, SSR) pairs that exist in both DataFrames\n",
    "    matching_keys = set(epic_df_copy[['FID', 'SSR']].apply(tuple, axis=1)) & set(secuTrial_df_copy[['FID', 'SSR']].apply(tuple, axis=1))\n",
    "\n",
    "    # Store mismatched results\n",
    "    mismatched_results = []\n",
    "    match_count = 0\n",
    "    secu_missing_count = 0\n",
    "    epic_missing_count = 0\n",
    "    mismatch_count = 0\n",
    "    total_comparisons = 0\n",
    "    \n",
    "    # Helper function to convert values to the correct type\n",
    "    def convert_to_type(value, target_type):\n",
    "        \"\"\"Convert value to specified type with specific formatting\"\"\"\n",
    "        if pd.isna(value):\n",
    "            return value\n",
    "            \n",
    "        # Handle various data types\n",
    "        if not isinstance(target_type, str):\n",
    "            return value  # If no type specified, return as is\n",
    "            \n",
    "        # Check for float with decimal specification (e.g., float-1, float-2)\n",
    "        float_match = re.match(r'float-(\\d+)', target_type.lower())\n",
    "        if float_match:\n",
    "            try:\n",
    "                decimal_places = int(float_match.group(1))\n",
    "                if value == '':\n",
    "                    return pd.NA\n",
    "                float_val = float(value)\n",
    "                return round(float_val, decimal_places)\n",
    "            except (ValueError, TypeError):\n",
    "                return pd.NA\n",
    "        \n",
    "        if target_type.lower() in ['int', 'integer', 'int64', 'int32']:\n",
    "            try:\n",
    "                return int(float(value)) if value != '' else pd.NA\n",
    "            except (ValueError, TypeError):\n",
    "                return pd.NA\n",
    "        elif target_type.lower() in ['float', 'double', 'numeric', 'float64', 'float32']:\n",
    "            try:\n",
    "                return float(value) if value != '' else pd.NA\n",
    "            except (ValueError, TypeError):\n",
    "                return pd.NA\n",
    "        elif target_type.lower() in ['date', 'datetime', 'timestamp']:\n",
    "            try:\n",
    "                if value == '':\n",
    "                    return pd.NA\n",
    "                # Convert to datetime and then to yyyymmdd hh:mm format\n",
    "                dt = pd.to_datetime(value)\n",
    "                return dt.strftime('%Y%m%d %H:%M')\n",
    "            except (ValueError, TypeError, AttributeError):\n",
    "                return pd.NA\n",
    "        elif target_type.lower() in ['bool', 'boolean']:\n",
    "            if isinstance(value, bool):\n",
    "                return value\n",
    "            elif isinstance(value, (int, float)):\n",
    "                return bool(value)\n",
    "            elif isinstance(value, str):\n",
    "                return value.lower() in ['true', 'yes', 'y', '1', 't']\n",
    "            else:\n",
    "                return pd.NA\n",
    "        else:\n",
    "            # Default to string for text, categorical, etc.\n",
    "            return str(value) if value is not None else pd.NA\n",
    "    \n",
    "    # Helper function to check if values are equivalent\n",
    "    def equivalent_values(val1, val2, target_type):\n",
    "        \"\"\"Compare values with formatted type awareness\"\"\"\n",
    "        # Handle NaN values consistently\n",
    "        if pd.isna(val1) and pd.isna(val2):\n",
    "            return True\n",
    "        elif pd.isna(val1) or pd.isna(val2):\n",
    "            return False\n",
    "            \n",
    "        # Check for float with decimal specification (e.g., float-1, float-2)\n",
    "        float_match = re.match(r'float-(\\d+)', target_type.lower()) if isinstance(target_type, str) else None\n",
    "        if float_match:\n",
    "            try:\n",
    "                decimal_places = int(float_match.group(1))\n",
    "                val1_rounded = round(float(val1), decimal_places)\n",
    "                val2_rounded = round(float(val2), decimal_places)\n",
    "                return val1_rounded == val2_rounded\n",
    "            except (ValueError, TypeError):\n",
    "                return False\n",
    "        \n",
    "        # Special handling for numeric types\n",
    "        if isinstance(val1, (int, float)) and isinstance(val2, (int, float)):\n",
    "            try:\n",
    "                return abs(float(val1) - float(val2)) < 1e-6\n",
    "            except (ValueError, TypeError):\n",
    "                return False\n",
    "        \n",
    "        # Date comparison (already in string format)\n",
    "        if isinstance(target_type, str) and target_type.lower() in ['date', 'datetime', 'timestamp']:\n",
    "            return val1 == val2\n",
    "                \n",
    "        # Boolean comparison\n",
    "        elif isinstance(val1, bool) and isinstance(val2, bool):\n",
    "            return val1 == val2\n",
    "            \n",
    "        # String comparison (case insensitive)\n",
    "        elif isinstance(val1, str) and isinstance(val2, str):\n",
    "            return val1.strip().lower() == val2.strip().lower()\n",
    "            \n",
    "        # Default comparison\n",
    "        else:\n",
    "            return str(val1) == str(val2)\n",
    "    \n",
    "    # Build a column mapping dictionary for easier lookups\n",
    "    column_mappings = {}\n",
    "    column_types = {}\n",
    "    \n",
    "    for _, row in mapping_df.iterrows():\n",
    "        epic_column_name = row.get('EPIC_varColumnName', None)\n",
    "        secuTrial_column_name = row.get('sT_varColumnName', None)\n",
    "        epic_dtype = row.get('EPIC_varType', 'text')  # Default to text if not specified\n",
    "        secuTrial_dtype = row.get('sT_varType', 'text')  # Default to text if not specified\n",
    "        column_source = row.get('EPIC_exportFileName', None)\n",
    "        secu_source = row.get('sT_exportFileName', None)\n",
    "\n",
    "        if not isinstance(epic_column_name, str) or not isinstance(secuTrial_column_name, str):\n",
    "            continue  # Skip if column names are missing\n",
    "\n",
    "        # Determine EPIC column prefix\n",
    "        prefix = \"\"\n",
    "        if isinstance(column_source, str):\n",
    "            if \"encounter\" in column_source:\n",
    "                prefix = \"enct.\"\n",
    "            elif \"flowsheet\" in column_source:\n",
    "                prefix = \"flow.\"\n",
    "            elif \"imaging\" in column_source:\n",
    "                prefix = \"img.\"\n",
    "            elif \"lab\" in column_source:\n",
    "                prefix = \"lab.\"\n",
    "            elif \"medication\" in column_source:\n",
    "                prefix = \"med.\"\n",
    "            elif \"monitor\" in column_source:\n",
    "                prefix = \"mon.\"\n",
    "\n",
    "        # Determine SecuTrial column suffix\n",
    "        suffix = \"\"\n",
    "        if isinstance(secu_source, str) and \"REVASC\" in secu_source:\n",
    "            suffix = \".revas\"\n",
    "\n",
    "        # Construct fully qualified column names\n",
    "        epic_col = f\"{prefix}{epic_column_name}\"  # EPIC column with prefix\n",
    "        secu_col = f\"{secuTrial_column_name}{suffix}\"  # SecuTrial column with suffix\n",
    "        \n",
    "        # If secuTrial type is int, override EPIC type to also be int\n",
    "        if secuTrial_dtype.lower() in ['int', 'integer', 'int64', 'int32']:\n",
    "            epic_dtype = 'int'\n",
    "        # If secuTrial type is float or float-n, override EPIC type\n",
    "        elif secuTrial_dtype.lower() in ['float', 'double', 'numeric', 'float64', 'float32'] or re.match(r'float-\\d+', secuTrial_dtype.lower()):\n",
    "            epic_dtype = secuTrial_dtype\n",
    "        \n",
    "        # Store the mapping\n",
    "        column_mappings[epic_col] = secu_col\n",
    "        column_types[epic_col] = {'epic_type': epic_dtype, 'secu_type': secuTrial_dtype}\n",
    "    \n",
    "    # First, apply value mappings to EPIC data\n",
    "    for col, mapping in value_mappings.items():\n",
    "        if col in epic_df_copy.columns:\n",
    "            epic_df_copy[col] = epic_df_copy[col].map(lambda x: mapping.get(x, x))\n",
    "    \n",
    "    # Next, convert data types in both dataframes\n",
    "    for epic_col, secu_col in column_mappings.items():\n",
    "        # For EPIC dataframe\n",
    "        if epic_col in epic_df_copy.columns:\n",
    "            target_type = column_types[epic_col]['epic_type']\n",
    "            epic_df_copy[epic_col] = epic_df_copy[epic_col].apply(lambda x: convert_to_type(x, target_type))\n",
    "        \n",
    "        # For secuTrial dataframe\n",
    "        if secu_col in secuTrial_df_copy.columns:\n",
    "            target_type = column_types[epic_col]['secu_type']\n",
    "            secuTrial_df_copy[secu_col] = secuTrial_df_copy[secu_col].apply(lambda x: convert_to_type(x, target_type))\n",
    "    \n",
    "    # Now compare the values\n",
    "    for epic_col, secu_col in column_mappings.items():\n",
    "        # Check if columns exist in respective DataFrames\n",
    "        if epic_col not in epic_df_copy.columns and secu_col in secuTrial_df_copy.columns:\n",
    "            epic_missing_count += 1\n",
    "            continue\n",
    "        elif secu_col not in secuTrial_df_copy.columns and epic_col in epic_df_copy.columns:\n",
    "            secu_missing_count += 1\n",
    "            continue\n",
    "        elif epic_col not in epic_df_copy.columns and secu_col not in secuTrial_df_copy.columns:\n",
    "            continue  # Skip comparison if column is missing in both\n",
    "\n",
    "        total_comparisons += 1\n",
    "        \n",
    "        # Get target type for this column\n",
    "        target_type = column_types[epic_col]['secu_type']  # Use secuTrial type as the target\n",
    "\n",
    "        # Compare values for rows with matching (FID, SSR)\n",
    "        for fid, ssr in matching_keys:\n",
    "            epic_row = epic_df_copy.loc[(epic_df_copy[\"FID\"] == fid) & (epic_df_copy[\"SSR\"] == ssr)]\n",
    "            secu_row = secuTrial_df_copy.loc[(secuTrial_df_copy[\"FID\"] == fid) & (secuTrial_df_copy[\"SSR\"] == ssr)]\n",
    "            \n",
    "            if epic_row.empty or secu_row.empty:\n",
    "                continue  # Skip if no matching row found\n",
    "                \n",
    "            epic_value = epic_row[epic_col].iloc[0] if epic_col in epic_row.columns else pd.NA\n",
    "            secu_value = secu_row[secu_col].iloc[0] if secu_col in secu_row.columns else pd.NA\n",
    "            \n",
    "            # Skip comparison if both values are missing\n",
    "            if pd.isna(epic_value) and pd.isna(secu_value):\n",
    "                match_count += 1\n",
    "                continue\n",
    "                \n",
    "            # Skip and count as missing if SecuTrial value is NaN\n",
    "            if pd.isna(secu_value) and not pd.isna(epic_value):\n",
    "                secu_missing_count += 1\n",
    "                continue\n",
    "                \n",
    "            # Skip and count as missing if EPIC value is NaN\n",
    "            if pd.isna(epic_value) and not pd.isna(secu_value):\n",
    "                epic_missing_count += 1\n",
    "                continue\n",
    "\n",
    "            # Compare values using the target type\n",
    "            if equivalent_values(epic_value, secu_value, target_type):\n",
    "                match_count += 1\n",
    "            else:\n",
    "                mismatch_count += 1\n",
    "                mismatched_results.append({\n",
    "                    'FID': fid,\n",
    "                    'SSR': ssr,\n",
    "                    'EPIC Column': epic_col,\n",
    "                    'SecuTrial Column': secu_col,\n",
    "                    'EPIC Value': str(epic_value),\n",
    "                    'SecuTrial Value': str(secu_value),\n",
    "                    'EPIC Expected Type': column_types[epic_col]['epic_type'],\n",
    "                    'SecuTrial Expected Type': target_type,\n",
    "                    'EPIC Actual Type': type(epic_value).__name__,\n",
    "                    'SecuTrial Actual Type': type(secu_value).__name__\n",
    "                })\n",
    "\n",
    "    # Calculate percentages\n",
    "    total_compared = match_count + mismatch_count + secu_missing_count + epic_missing_count\n",
    "    percentage_stats = {\n",
    "        \"Matching Variables (%)\": round((match_count / total_compared) * 100, 2) if total_compared else 0,\n",
    "        \"Variables Missing in EPIC (%)\": round((epic_missing_count / total_compared) * 100, 2) if total_compared else 0,\n",
    "        \"Variables Missing in SecuTrial (%)\": round((secu_missing_count / total_compared) * 100, 2) if total_compared else 0,\n",
    "        \"Mismatched Variables (%)\": round((mismatch_count / total_compared) * 100, 2) if total_compared else 0,\n",
    "        \"Total Comparisons\": total_compared,\n",
    "        \"Matches\": match_count,\n",
    "        \"EPIC Missing\": epic_missing_count,\n",
    "        \"SecuTrial Missing\": secu_missing_count,\n",
    "        \"Mismatches\": mismatch_count\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(mismatched_results), percentage_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude code 4\n",
    "def compare_epic_secuTrial(epic_df, secuTrial_df, mapping_df, value_mappings=None):\n",
    "    \"\"\"\n",
    "    Compares values and data types between EPIC and secuTrial DataFrames using a mapping file,\n",
    "    accounting for data type differences properly.\n",
    "\n",
    "    Args:\n",
    "        epic_df (DataFrame): The EPIC dataset.\n",
    "        secuTrial_df (DataFrame): The SecuTrial dataset.\n",
    "        mapping_df (DataFrame): The mapping Excel file with data type information.\n",
    "        value_mappings (dict, optional): Dictionary for value conversions in EPIC.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with mismatched results in values and data types.\n",
    "        dict: Dictionary containing percentage statistics.\n",
    "    \"\"\"\n",
    "    \n",
    "    import pandas as pd\n",
    "    import re\n",
    "    \n",
    "    if value_mappings is None:\n",
    "        value_mappings = {}\n",
    "        \n",
    "    # Create working copies to avoid modifying original dataframes\n",
    "    epic_df_copy = epic_df.copy()\n",
    "    secuTrial_df_copy = secuTrial_df.copy()\n",
    "    \n",
    "    # Replace missing value indicators\n",
    "    epic_df_copy.replace(-9999, pd.NA, inplace=True)\n",
    "    secuTrial_df_copy.replace(-9999, pd.NA, inplace=True)\n",
    "\n",
    "    # Ensure necessary columns exist before comparison\n",
    "    if \"FID\" not in epic_df_copy.columns or \"SSR\" not in epic_df_copy.columns:\n",
    "        raise ValueError(\"EPIC DataFrame must contain 'FID' and 'SSR' columns.\")\n",
    "    if \"FID\" not in secuTrial_df_copy.columns or \"SSR\" not in secuTrial_df_copy.columns:\n",
    "        raise ValueError(\"SecuTrial DataFrame must contain 'FID' and 'SSR' columns.\")\n",
    "\n",
    "    # Create a set of (FID, SSR) pairs that exist in both DataFrames\n",
    "    matching_keys = set(epic_df_copy[['FID', 'SSR']].apply(tuple, axis=1)) & set(secuTrial_df_copy[['FID', 'SSR']].apply(tuple, axis=1))\n",
    "\n",
    "    # Store mismatched results\n",
    "    mismatched_results = []\n",
    "    match_count = 0\n",
    "    secu_missing_count = 0\n",
    "    epic_missing_count = 0\n",
    "    mismatch_count = 0\n",
    "    total_comparisons = 0\n",
    "    \n",
    "    # Helper function to standardize boolean values\n",
    "    def standardize_boolean(value):\n",
    "        if pd.isna(value):\n",
    "            return pd.NA\n",
    "        \n",
    "        if isinstance(value, bool):\n",
    "            return \"yes\" if value else \"no\"\n",
    "        elif isinstance(value, (int, float)):\n",
    "            return \"yes\" if value else \"no\"\n",
    "        elif isinstance(value, str):\n",
    "            if value.lower() in ['true', 'yes', 'y', '1', 't']:\n",
    "                return \"yes\"\n",
    "            elif value.lower() in ['false', 'no', 'n', '0', 'f']:\n",
    "                return \"no\"\n",
    "        \n",
    "        return str(value)\n",
    "    \n",
    "    # Helper function to convert values to the correct type\n",
    "    def convert_to_type(value, target_type):\n",
    "        \"\"\"Convert value to specified type with specific formatting\"\"\"\n",
    "        if pd.isna(value):\n",
    "            return pd.NA\n",
    "            \n",
    "        # Handle various data types\n",
    "        if not isinstance(target_type, str):\n",
    "            return value  # If no type specified, return as is\n",
    "            \n",
    "        # Check for float with decimal specification (e.g., float-1, float-2)\n",
    "        float_match = re.match(r'float-(\\d+)', target_type.lower())\n",
    "        if float_match:\n",
    "            try:\n",
    "                decimal_places = int(float_match.group(1))\n",
    "                if value == '':\n",
    "                    return pd.NA\n",
    "                float_val = float(value)\n",
    "                return round(float_val, decimal_places)\n",
    "            except (ValueError, TypeError):\n",
    "                return pd.NA\n",
    "        \n",
    "        if target_type.lower() in ['int', 'integer', 'int64', 'int32']:\n",
    "            try:\n",
    "                return int(float(value)) if value != '' else pd.NA\n",
    "            except (ValueError, TypeError):\n",
    "                return pd.NA\n",
    "        elif target_type.lower() in ['float', 'double', 'numeric', 'float64', 'float32']:\n",
    "            try:\n",
    "                return float(value) if value != '' else pd.NA\n",
    "            except (ValueError, TypeError):\n",
    "                return pd.NA\n",
    "        elif target_type.lower() in ['date', 'datetime', 'timestamp']:\n",
    "            try:\n",
    "                if value == '':\n",
    "                    return pd.NA\n",
    "                # Convert to datetime and then to yyyymmdd hh:mm format\n",
    "                dt = pd.to_datetime(value)\n",
    "                return dt.strftime('%Y%m%d %H:%M')\n",
    "            except (ValueError, TypeError, AttributeError):\n",
    "                return pd.NA\n",
    "        elif target_type.lower() in ['bool', 'boolean']:\n",
    "            return standardize_boolean(value)\n",
    "        else:\n",
    "            # Default to string for text, categorical, etc.\n",
    "            return str(value) if value is not None and value != '' else pd.NA\n",
    "    \n",
    "    # Helper function to check if values are equivalent\n",
    "    def equivalent_values(val1, val2, target_type):\n",
    "        \"\"\"Compare values with formatted type awareness\"\"\"\n",
    "        # Handle NaN values consistently\n",
    "        if pd.isna(val1) and pd.isna(val2):\n",
    "            return True\n",
    "        elif pd.isna(val1) or pd.isna(val2):\n",
    "            return False\n",
    "            \n",
    "        # Check for float with decimal specification (e.g., float-1, float-2)\n",
    "        float_match = re.match(r'float-(\\d+)', target_type.lower()) if isinstance(target_type, str) else None\n",
    "        if float_match:\n",
    "            try:\n",
    "                decimal_places = int(float_match.group(1))\n",
    "                val1_rounded = round(float(val1), decimal_places)\n",
    "                val2_rounded = round(float(val2), decimal_places)\n",
    "                return val1_rounded == val2_rounded\n",
    "            except (ValueError, TypeError):\n",
    "                return False\n",
    "        \n",
    "        # Boolean comparison (standardized to yes/no)\n",
    "        if isinstance(target_type, str) and target_type.lower() in ['bool', 'boolean']:\n",
    "            val1_std = standardize_boolean(val1)\n",
    "            val2_std = standardize_boolean(val2)\n",
    "            return val1_std == val2_std\n",
    "        \n",
    "        # Special handling for numeric types\n",
    "        if isinstance(val1, (int, float)) and isinstance(val2, (int, float)):\n",
    "            try:\n",
    "                return abs(float(val1) - float(val2)) < 1e-6\n",
    "            except (ValueError, TypeError):\n",
    "                return False\n",
    "        \n",
    "        # Date comparison (already in string format)\n",
    "        if isinstance(target_type, str) and target_type.lower() in ['date', 'datetime', 'timestamp']:\n",
    "            return val1 == val2\n",
    "            \n",
    "        # String comparison (case insensitive)\n",
    "        elif isinstance(val1, str) and isinstance(val2, str):\n",
    "            return val1.strip().lower() == val2.strip().lower()\n",
    "            \n",
    "        # Default comparison\n",
    "        else:\n",
    "            return str(val1) == str(val2)\n",
    "    \n",
    "    # Build a column mapping dictionary for easier lookups\n",
    "    column_mappings = {}\n",
    "    column_types = {}\n",
    "    \n",
    "    for _, row in mapping_df.iterrows():\n",
    "        epic_column_name = row.get('EPIC_varColumnName', None)\n",
    "        secuTrial_column_name = row.get('sT_varColumnName', None)\n",
    "        epic_dtype = row.get('EPIC_varType', 'text')  # Default to text if not specified\n",
    "        secuTrial_dtype = row.get('sT_varType', 'text')  # Default to text if not specified\n",
    "        column_source = row.get('EPIC_exportFileName', None)\n",
    "        secu_source = row.get('sT_exportFileName', None)\n",
    "\n",
    "        if not isinstance(epic_column_name, str) or not isinstance(secuTrial_column_name, str):\n",
    "            continue  # Skip if column names are missing\n",
    "\n",
    "        # Determine EPIC column prefix\n",
    "        prefix = \"\"\n",
    "        if isinstance(column_source, str):\n",
    "            if \"encounter\" in column_source:\n",
    "                prefix = \"enct.\"\n",
    "            elif \"flowsheet\" in column_source:\n",
    "                prefix = \"flow.\"\n",
    "            elif \"imaging\" in column_source:\n",
    "                prefix = \"img.\"\n",
    "            elif \"lab\" in column_source:\n",
    "                prefix = \"lab.\"\n",
    "            elif \"medication\" in column_source:\n",
    "                prefix = \"med.\"\n",
    "            elif \"monitor\" in column_source:\n",
    "                prefix = \"mon.\"\n",
    "\n",
    "        # Determine SecuTrial column suffix\n",
    "        suffix = \"\"\n",
    "        if isinstance(secu_source, str) and \"REVASC\" in secu_source:\n",
    "            suffix = \".revas\"\n",
    "\n",
    "        # Construct fully qualified column names\n",
    "        epic_col = f\"{prefix}{epic_column_name}\"  # EPIC column with prefix\n",
    "        secu_col = f\"{secuTrial_column_name}{suffix}\"  # SecuTrial column with suffix\n",
    "        \n",
    "        # If secuTrial type is int, override EPIC type to also be int\n",
    "        if secuTrial_dtype.lower() in ['int', 'integer', 'int64', 'int32']:\n",
    "            epic_dtype = 'int'\n",
    "        # If secuTrial type is float or float-n, override EPIC type\n",
    "        elif secuTrial_dtype.lower() in ['float', 'double', 'numeric', 'float64', 'float32'] or re.match(r'float-\\d+', secuTrial_dtype.lower()):\n",
    "            epic_dtype = secuTrial_dtype\n",
    "        \n",
    "        # Store the mapping\n",
    "        column_mappings[epic_col] = secu_col\n",
    "        column_types[epic_col] = {'epic_type': epic_dtype, 'secu_type': secuTrial_dtype}\n",
    "    \n",
    "    # First, apply value mappings to EPIC data\n",
    "    for col, mapping in value_mappings.items():\n",
    "        if col in epic_df_copy.columns:\n",
    "            epic_df_copy[col] = epic_df_copy[col].map(lambda x: mapping.get(x, x))\n",
    "    \n",
    "    # Next, convert data types in both dataframes\n",
    "    for epic_col, secu_col in column_mappings.items():\n",
    "        # For EPIC dataframe\n",
    "        if epic_col in epic_df_copy.columns:\n",
    "            target_type = column_types[epic_col]['epic_type']\n",
    "            epic_df_copy[epic_col] = epic_df_copy[epic_col].apply(lambda x: convert_to_type(x, target_type))\n",
    "        \n",
    "        # For secuTrial dataframe\n",
    "        if secu_col in secuTrial_df_copy.columns:\n",
    "            target_type = column_types[epic_col]['secu_type']\n",
    "            secuTrial_df_copy[secu_col] = secuTrial_df_copy[secu_col].apply(lambda x: convert_to_type(x, target_type))\n",
    "    \n",
    "    # Now compare the values\n",
    "    for epic_col, secu_col in column_mappings.items():\n",
    "        # Check if columns exist in respective DataFrames\n",
    "        if epic_col not in epic_df_copy.columns and secu_col in secuTrial_df_copy.columns:\n",
    "            epic_missing_count += 1\n",
    "            continue\n",
    "        elif secu_col not in secuTrial_df_copy.columns and epic_col in epic_df_copy.columns:\n",
    "            secu_missing_count += 1\n",
    "            continue\n",
    "        elif epic_col not in epic_df_copy.columns and secu_col not in secuTrial_df_copy.columns:\n",
    "            continue  # Skip comparison if column is missing in both\n",
    "\n",
    "        total_comparisons += 1\n",
    "        \n",
    "        # Get target type for this column\n",
    "        target_type = column_types[epic_col]['secu_type']  # Use secuTrial type as the target\n",
    "\n",
    "        # Compare values for rows with matching (FID, SSR)\n",
    "        for fid, ssr in matching_keys:\n",
    "            epic_row = epic_df_copy.loc[(epic_df_copy[\"FID\"] == fid) & (epic_df_copy[\"SSR\"] == ssr)]\n",
    "            secu_row = secuTrial_df_copy.loc[(secuTrial_df_copy[\"FID\"] == fid) & (secuTrial_df_copy[\"SSR\"] == ssr)]\n",
    "            \n",
    "            if epic_row.empty or secu_row.empty:\n",
    "                continue  # Skip if no matching row found\n",
    "                \n",
    "            epic_value = epic_row[epic_col].iloc[0] if epic_col in epic_row.columns else pd.NA\n",
    "            secu_value = secu_row[secu_col].iloc[0] if secu_col in secu_row.columns else pd.NA\n",
    "            \n",
    "            # Both values are NaN/missing - count as match\n",
    "            if pd.isna(epic_value) and pd.isna(secu_value):\n",
    "                match_count += 1\n",
    "                continue\n",
    "                \n",
    "            # Only secuTrial value is NaN/missing\n",
    "            if pd.isna(secu_value) and not pd.isna(epic_value):\n",
    "                secu_missing_count += 1\n",
    "                continue\n",
    "                \n",
    "            # Only EPIC value is NaN/missing\n",
    "            if pd.isna(epic_value) and not pd.isna(secu_value):\n",
    "                epic_missing_count += 1\n",
    "                continue\n",
    "\n",
    "            # Compare values using the target type\n",
    "            if equivalent_values(epic_value, secu_value, target_type):\n",
    "                match_count += 1\n",
    "            else:\n",
    "                mismatch_count += 1\n",
    "                mismatched_results.append({\n",
    "                    'FID': fid,\n",
    "                    'SSR': ssr,\n",
    "                    'EPIC Column': epic_col,\n",
    "                    'SecuTrial Column': secu_col,\n",
    "                    'EPIC Value': str(epic_value),\n",
    "                    'SecuTrial Value': str(secu_value),\n",
    "                    'EPIC Expected Type': column_types[epic_col]['epic_type'],\n",
    "                    'SecuTrial Expected Type': target_type,\n",
    "                    'EPIC Actual Type': type(epic_value).__name__,\n",
    "                    'SecuTrial Actual Type': type(secu_value).__name__\n",
    "                })\n",
    "\n",
    "    # Calculate percentages\n",
    "    total_compared = match_count + mismatch_count + secu_missing_count + epic_missing_count\n",
    "    percentage_stats = {\n",
    "        \"Matching Variables (%)\": round((match_count / total_compared) * 100, 2) if total_compared else 0,\n",
    "        \"Variables Missing in EPIC (%)\": round((epic_missing_count / total_compared) * 100, 2) if total_compared else 0,\n",
    "        \"Variables Missing in SecuTrial (%)\": round((secu_missing_count / total_compared) * 100, 2) if total_compared else 0,\n",
    "        \"Mismatched Variables (%)\": round((mismatch_count / total_compared) * 100, 2) if total_compared else 0,\n",
    "        \"Total Comparisons\": total_compared,\n",
    "        \"Matches\": match_count,\n",
    "        \"EPIC Missing\": epic_missing_count,\n",
    "        \"SecuTrial Missing\": secu_missing_count,\n",
    "        \"Mismatches\": mismatch_count\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(mismatched_results), percentage_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_epic_secuTrial(epic_df, secuTrial_df, mapping_df, value_mappings=None):\n",
    "    \"\"\"\n",
    "    Compares values and data types between EPIC and secuTrial DataFrames using a mapping file,\n",
    "    accounting for data type differences properly. Includes monthly breakdown of statistics.\n",
    "\n",
    "    Args:\n",
    "        epic_df (DataFrame): The EPIC dataset.\n",
    "        secuTrial_df (DataFrame): The SecuTrial dataset.\n",
    "        mapping_df (DataFrame): The mapping Excel file with data type information.\n",
    "        value_mappings (dict, optional): Dictionary for value conversions in EPIC.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with mismatched results in values and data types.\n",
    "        dict: Dictionary containing percentage statistics.\n",
    "        dict: Dictionary containing monthly statistics.\n",
    "    \"\"\"\n",
    "    \n",
    "    import pandas as pd\n",
    "    import re\n",
    "    from datetime import datetime\n",
    "    \n",
    "    if value_mappings is None:\n",
    "        value_mappings = {}\n",
    "        \n",
    "    # Create working copies to avoid modifying original dataframes\n",
    "    epic_df_copy = epic_df.copy()\n",
    "    secuTrial_df_copy = secuTrial_df.copy()\n",
    "    \n",
    "    # Replace missing value indicators\n",
    "    epic_df_copy.replace(-9999, pd.NA, inplace=True)\n",
    "    secuTrial_df_copy.replace(-9999, pd.NA, inplace=True)\n",
    "\n",
    "    # Ensure necessary columns exist before comparison\n",
    "    if \"FID\" not in epic_df_copy.columns or \"SSR\" not in epic_df_copy.columns:\n",
    "        raise ValueError(\"EPIC DataFrame must contain 'FID' and 'SSR' columns.\")\n",
    "    if \"FID\" not in secuTrial_df_copy.columns or \"SSR\" not in secuTrial_df_copy.columns:\n",
    "        raise ValueError(\"SecuTrial DataFrame must contain 'FID' and 'SSR' columns.\")\n",
    "        \n",
    "    # Modify line ~40-43 in the function\n",
    "    if \"enct.arrival_date\" not in epic_df_copy.columns:\n",
    "        raise ValueError(\"EPIC DataFrame must contain 'enct.arrival_date' column for monthly breakdown.\")\n",
    "    if \"Arrival at hospital\" not in secuTrial_df_copy.columns:\n",
    "        raise ValueError(\"SecuTrial DataFrame must contain 'Arrival at hospital' column for monthly breakdown.\")\n",
    "\n",
    "    # And then use these columns for date conversion ~46-47\n",
    "    epic_df_copy['DATE'] = pd.to_datetime(epic_df_copy['enct.arrival_date'], errors='coerce')\n",
    "    secuTrial_df_copy['DATE'] = pd.to_datetime(secuTrial_df_copy['Arrival at hospital'], errors='coerce')\n",
    "\n",
    "    # Create a set of (FID, SSR) pairs that exist in both DataFrames\n",
    "    matching_keys = set(epic_df_copy[['FID', 'SSR']].apply(tuple, axis=1)) & set(secuTrial_df_copy[['FID', 'SSR']].apply(tuple, axis=1))\n",
    "\n",
    "    # Store mismatched results\n",
    "    mismatched_results = []\n",
    "    match_count = 0\n",
    "    secu_missing_count = 0\n",
    "    epic_missing_count = 0\n",
    "    mismatch_count = 0\n",
    "    total_comparisons = 0\n",
    "    \n",
    "    # Setup monthly statistics tracking\n",
    "    months = {4: 'April', 5: 'May', 6: 'June', 7: 'July', 8: 'August', \n",
    "              9: 'September', 10: 'October', 11: 'November', 12: 'December'}\n",
    "    \n",
    "    monthly_stats = {month_name: {'match_count': 0, 'secu_missing_count': 0, \n",
    "                                'epic_missing_count': 0, 'mismatch_count': 0, \n",
    "                                'total_compared': 0} \n",
    "                   for month_name in months.values()}\n",
    "    \n",
    "    # Helper function to standardize boolean values\n",
    "    def standardize_boolean(value):\n",
    "        if pd.isna(value):\n",
    "            return pd.NA\n",
    "        \n",
    "        if isinstance(value, bool):\n",
    "            return \"yes\" if value else \"no\"\n",
    "        elif isinstance(value, (int, float)):\n",
    "            return \"yes\" if value else \"no\"\n",
    "        elif isinstance(value, str):\n",
    "            if value.lower() in ['true', 'yes', 'y', '1', 't']:\n",
    "                return \"yes\"\n",
    "            elif value.lower() in ['false', 'no', 'n', '0', 'f']:\n",
    "                return \"no\"\n",
    "        \n",
    "        return str(value)\n",
    "    \n",
    "    # Helper function to convert values to the correct type\n",
    "    def convert_to_type(value, target_type):\n",
    "        \"\"\"Convert value to specified type with specific formatting\"\"\"\n",
    "        if pd.isna(value):\n",
    "            return pd.NA\n",
    "            \n",
    "        # Handle various data types\n",
    "        if not isinstance(target_type, str):\n",
    "            return value  # If no type specified, return as is\n",
    "            \n",
    "        # Check for float with decimal specification (e.g., float-1, float-2)\n",
    "        float_match = re.match(r'float-(\\d+)', target_type.lower())\n",
    "        if float_match:\n",
    "            try:\n",
    "                decimal_places = int(float_match.group(1))\n",
    "                if value == '':\n",
    "                    return pd.NA\n",
    "                float_val = float(value)\n",
    "                return round(float_val, decimal_places)\n",
    "            except (ValueError, TypeError):\n",
    "                return pd.NA\n",
    "        \n",
    "        if target_type.lower() in ['int', 'integer', 'int64', 'int32']:\n",
    "            try:\n",
    "                return int(float(value)) if value != '' else pd.NA\n",
    "            except (ValueError, TypeError):\n",
    "                return pd.NA\n",
    "        elif target_type.lower() in ['float', 'double', 'numeric', 'float64', 'float32']:\n",
    "            try:\n",
    "                return float(value) if value != '' else pd.NA\n",
    "            except (ValueError, TypeError):\n",
    "                return pd.NA\n",
    "        elif target_type.lower() in ['date', 'datetime', 'timestamp']:\n",
    "            try:\n",
    "                if value == '':\n",
    "                    return pd.NA\n",
    "                # Convert to datetime and then to yyyymmdd hh:mm format\n",
    "                dt = pd.to_datetime(value)\n",
    "                return dt.strftime('%Y%m%d %H:%M')\n",
    "            except (ValueError, TypeError, AttributeError):\n",
    "                return pd.NA\n",
    "        elif target_type.lower() in ['bool', 'boolean']:\n",
    "            return standardize_boolean(value)\n",
    "        else:\n",
    "            # Default to string for text, categorical, etc.\n",
    "            return str(value) if value is not None and value != '' else pd.NA\n",
    "    \n",
    "    # Helper function to check if values are equivalent\n",
    "    def equivalent_values(val1, val2, target_type):\n",
    "        \"\"\"Compare values with formatted type awareness\"\"\"\n",
    "        # Handle NaN values consistently\n",
    "        if pd.isna(val1) and pd.isna(val2):\n",
    "            return True\n",
    "        elif pd.isna(val1) or pd.isna(val2):\n",
    "            return False\n",
    "            \n",
    "        # Check for float with decimal specification (e.g., float-1, float-2)\n",
    "        float_match = re.match(r'float-(\\d+)', target_type.lower()) if isinstance(target_type, str) else None\n",
    "        if float_match:\n",
    "            try:\n",
    "                decimal_places = int(float_match.group(1))\n",
    "                val1_rounded = round(float(val1), decimal_places)\n",
    "                val2_rounded = round(float(val2), decimal_places)\n",
    "                return val1_rounded == val2_rounded\n",
    "            except (ValueError, TypeError):\n",
    "                return False\n",
    "        \n",
    "        # Boolean comparison (standardized to yes/no)\n",
    "        if isinstance(target_type, str) and target_type.lower() in ['bool', 'boolean']:\n",
    "            val1_std = standardize_boolean(val1)\n",
    "            val2_std = standardize_boolean(val2)\n",
    "            return val1_std == val2_std\n",
    "        \n",
    "        # Special handling for numeric types\n",
    "        if isinstance(val1, (int, float)) and isinstance(val2, (int, float)):\n",
    "            try:\n",
    "                return abs(float(val1) - float(val2)) < 1e-6\n",
    "            except (ValueError, TypeError):\n",
    "                return False\n",
    "        \n",
    "        # Date comparison (already in string format)\n",
    "        if isinstance(target_type, str) and target_type.lower() in ['date', 'datetime', 'timestamp']:\n",
    "            return val1 == val2\n",
    "            \n",
    "        # String comparison (case insensitive)\n",
    "        elif isinstance(val1, str) and isinstance(val2, str):\n",
    "            return val1.strip().lower() == val2.strip().lower()\n",
    "            \n",
    "        # Default comparison\n",
    "        else:\n",
    "            return str(val1) == str(val2)\n",
    "    \n",
    "    # Build a column mapping dictionary for easier lookups\n",
    "    column_mappings = {}\n",
    "    column_types = {}\n",
    "    \n",
    "    for _, row in mapping_df.iterrows():\n",
    "        epic_column_name = row.get('EPIC_varColumnName', None)\n",
    "        secuTrial_column_name = row.get('sT_varColumnName', None)\n",
    "        epic_dtype = row.get('EPIC_varType', 'text')  # Default to text if not specified\n",
    "        secuTrial_dtype = row.get('sT_varType', 'text')  # Default to text if not specified\n",
    "        column_source = row.get('EPIC_exportFileName', None)\n",
    "        secu_source = row.get('sT_exportFileName', None)\n",
    "\n",
    "        if not isinstance(epic_column_name, str) or not isinstance(secuTrial_column_name, str):\n",
    "            continue  # Skip if column names are missing\n",
    "\n",
    "        # Determine EPIC column prefix\n",
    "        prefix = \"\"\n",
    "        if isinstance(column_source, str):\n",
    "            if \"encounter\" in column_source:\n",
    "                prefix = \"enct.\"\n",
    "            elif \"flowsheet\" in column_source:\n",
    "                prefix = \"flow.\"\n",
    "            elif \"imaging\" in column_source:\n",
    "                prefix = \"img.\"\n",
    "            elif \"lab\" in column_source:\n",
    "                prefix = \"lab.\"\n",
    "            elif \"medication\" in column_source:\n",
    "                prefix = \"med.\"\n",
    "            elif \"monitor\" in column_source:\n",
    "                prefix = \"mon.\"\n",
    "\n",
    "        # Determine SecuTrial column suffix\n",
    "        suffix = \"\"\n",
    "        if isinstance(secu_source, str) and \"REVASC\" in secu_source:\n",
    "            suffix = \".revas\"\n",
    "\n",
    "        # Construct fully qualified column names\n",
    "        epic_col = f\"{prefix}{epic_column_name}\"  # EPIC column with prefix\n",
    "        secu_col = f\"{secuTrial_column_name}{suffix}\"  # SecuTrial column with suffix\n",
    "        \n",
    "        # If secuTrial type is int, override EPIC type to also be int\n",
    "        if secuTrial_dtype.lower() in ['int', 'integer', 'int64', 'int32']:\n",
    "            epic_dtype = 'int'\n",
    "        # If secuTrial type is float or float-n, override EPIC type\n",
    "        elif secuTrial_dtype.lower() in ['float', 'double', 'numeric', 'float64', 'float32'] or re.match(r'float-\\d+', secuTrial_dtype.lower()):\n",
    "            epic_dtype = secuTrial_dtype\n",
    "        \n",
    "        # Store the mapping\n",
    "        column_mappings[epic_col] = secu_col\n",
    "        column_types[epic_col] = {'epic_type': epic_dtype, 'secu_type': secuTrial_dtype}\n",
    "    \n",
    "    # First, apply value mappings to EPIC data\n",
    "    for col, mapping in value_mappings.items():\n",
    "        if col in epic_df_copy.columns:\n",
    "            epic_df_copy[col] = epic_df_copy[col].map(lambda x: mapping.get(x, x))\n",
    "    \n",
    "    # Next, convert data types in both dataframes\n",
    "    for epic_col, secu_col in column_mappings.items():\n",
    "        # For EPIC dataframe\n",
    "        if epic_col in epic_df_copy.columns:\n",
    "            target_type = column_types[epic_col]['epic_type']\n",
    "            epic_df_copy[epic_col] = epic_df_copy[epic_col].apply(lambda x: convert_to_type(x, target_type))\n",
    "        \n",
    "        # For secuTrial dataframe\n",
    "        if secu_col in secuTrial_df_copy.columns:\n",
    "            target_type = column_types[epic_col]['secu_type']\n",
    "            secuTrial_df_copy[secu_col] = secuTrial_df_copy[secu_col].apply(lambda x: convert_to_type(x, target_type))\n",
    "    \n",
    "    # Now compare the values\n",
    "    for epic_col, secu_col in column_mappings.items():\n",
    "        # Check if columns exist in respective DataFrames\n",
    "        if epic_col not in epic_df_copy.columns and secu_col in secuTrial_df_copy.columns:\n",
    "            epic_missing_count += 1\n",
    "            continue\n",
    "        elif secu_col not in secuTrial_df_copy.columns and epic_col in epic_df_copy.columns:\n",
    "            secu_missing_count += 1\n",
    "            continue\n",
    "        elif epic_col not in epic_df_copy.columns and secu_col not in secuTrial_df_copy.columns:\n",
    "            continue  # Skip comparison if column is missing in both\n",
    "\n",
    "        total_comparisons += 1\n",
    "        \n",
    "        # Get target type for this column\n",
    "        target_type = column_types[epic_col]['secu_type']  # Use secuTrial type as the target\n",
    "\n",
    "        # Compare values for rows with matching (FID, SSR)\n",
    "        for fid, ssr in matching_keys:\n",
    "            epic_row = epic_df_copy.loc[(epic_df_copy[\"FID\"] == fid) & (epic_df_copy[\"SSR\"] == ssr)]\n",
    "            secu_row = secuTrial_df_copy.loc[(secuTrial_df_copy[\"FID\"] == fid) & (secuTrial_df_copy[\"SSR\"] == ssr)]\n",
    "            \n",
    "            if epic_row.empty or secu_row.empty:\n",
    "                continue  # Skip if no matching row found\n",
    "                \n",
    "            epic_value = epic_row[epic_col].iloc[0] if epic_col in epic_row.columns else pd.NA\n",
    "            secu_value = secu_row[secu_col].iloc[0] if secu_col in secu_row.columns else pd.NA\n",
    "            \n",
    "            # Get the month for this record (use epic date if available, else secu date)\n",
    "            record_date = None\n",
    "            if not epic_row.empty and 'DATE' in epic_row.columns and not pd.isna(epic_row['DATE'].iloc[0]):\n",
    "                record_date = epic_row['DATE'].iloc[0]\n",
    "            elif not secu_row.empty and 'DATE' in secu_row.columns and not pd.isna(secu_row['DATE'].iloc[0]):\n",
    "                record_date = secu_row['DATE'].iloc[0]\n",
    "                \n",
    "            # Skip if no valid date or not in April-December range\n",
    "            if record_date is None:\n",
    "                continue\n",
    "                \n",
    "            record_month = record_date.month\n",
    "            # Skip if not in our target month range (April-December)\n",
    "            if record_month < 4 or record_month > 12:\n",
    "                continue\n",
    "                \n",
    "            month_name = months[record_month]\n",
    "                \n",
    "            # Both values are NaN/missing - count as match\n",
    "            if pd.isna(epic_value) and pd.isna(secu_value):\n",
    "                match_count += 1\n",
    "                monthly_stats[month_name]['match_count'] += 1\n",
    "                monthly_stats[month_name]['total_compared'] += 1\n",
    "                continue\n",
    "                \n",
    "            # Only secuTrial value is NaN/missing\n",
    "            if pd.isna(secu_value) and not pd.isna(epic_value):\n",
    "                secu_missing_count += 1\n",
    "                monthly_stats[month_name]['secu_missing_count'] += 1\n",
    "                monthly_stats[month_name]['total_compared'] += 1\n",
    "                continue\n",
    "                \n",
    "            # Only EPIC value is NaN/missing\n",
    "            if pd.isna(epic_value) and not pd.isna(secu_value):\n",
    "                epic_missing_count += 1\n",
    "                monthly_stats[month_name]['epic_missing_count'] += 1\n",
    "                monthly_stats[month_name]['total_compared'] += 1\n",
    "                continue\n",
    "\n",
    "            # Compare values using the target type\n",
    "            if equivalent_values(epic_value, secu_value, target_type):\n",
    "                match_count += 1\n",
    "                monthly_stats[month_name]['match_count'] += 1\n",
    "            else:\n",
    "                mismatch_count += 1\n",
    "                monthly_stats[month_name]['mismatch_count'] += 1\n",
    "                mismatched_results.append({\n",
    "                    'FID': fid,\n",
    "                    'SSR': ssr,\n",
    "                    'Month': month_name,\n",
    "                    'DATE': record_date,\n",
    "                    'EPIC Column': epic_col,\n",
    "                    'SecuTrial Column': secu_col,\n",
    "                    'EPIC Value': str(epic_value),\n",
    "                    'SecuTrial Value': str(secu_value),\n",
    "                    'EPIC Expected Type': column_types[epic_col]['epic_type'],\n",
    "                    'SecuTrial Expected Type': target_type,\n",
    "                    'EPIC Actual Type': type(epic_value).__name__,\n",
    "                    'SecuTrial Actual Type': type(secu_value).__name__\n",
    "                })\n",
    "            \n",
    "            monthly_stats[month_name]['total_compared'] += 1\n",
    "\n",
    "    # Calculate percentages\n",
    "    total_compared = match_count + mismatch_count + secu_missing_count + epic_missing_count\n",
    "    percentage_stats = {\n",
    "        \"Matching Variables (%)\": round((match_count / total_compared) * 100, 2) if total_compared else 0,\n",
    "        \"Variables Missing in EPIC (%)\": round((epic_missing_count / total_compared) * 100, 2) if total_compared else 0,\n",
    "        \"Variables Missing in SecuTrial (%)\": round((secu_missing_count / total_compared) * 100, 2) if total_compared else 0,\n",
    "        \"Mismatched Variables (%)\": round((mismatch_count / total_compared) * 100, 2) if total_compared else 0,\n",
    "        \"Total Comparisons\": total_compared,\n",
    "        \"Matches\": match_count,\n",
    "        \"EPIC Missing\": epic_missing_count,\n",
    "        \"SecuTrial Missing\": secu_missing_count,\n",
    "        \"Mismatches\": mismatch_count\n",
    "    }\n",
    "    \n",
    "    # Calculate monthly percentages\n",
    "    monthly_percentage_stats = {}\n",
    "    for month, stats in monthly_stats.items():\n",
    "        total = stats['total_compared']\n",
    "        if total > 0:\n",
    "            monthly_percentage_stats[month] = {\n",
    "                \"Matching Variables (%)\": round((stats['match_count'] / total) * 100, 2),\n",
    "                \"Variables Missing in EPIC (%)\": round((stats['epic_missing_count'] / total) * 100, 2),\n",
    "                \"Variables Missing in SecuTrial (%)\": round((stats['secu_missing_count'] / total) * 100, 2),\n",
    "                \"Mismatched Variables (%)\": round((stats['mismatch_count'] / total) * 100, 2),\n",
    "                \"Total Comparisons\": total,\n",
    "                \"Matches\": stats['match_count'],\n",
    "                \"EPIC Missing\": stats['epic_missing_count'],\n",
    "                \"SecuTrial Missing\": stats['secu_missing_count'],\n",
    "                \"Mismatches\": stats['mismatch_count']\n",
    "            }\n",
    "        else:\n",
    "            monthly_percentage_stats[month] = {\n",
    "                \"Matching Variables (%)\": 0,\n",
    "                \"Variables Missing in EPIC (%)\": 0,\n",
    "                \"Variables Missing in SecuTrial (%)\": 0,\n",
    "                \"Mismatched Variables (%)\": 0,\n",
    "                \"Total Comparisons\": 0,\n",
    "                \"Matches\": 0,\n",
    "                \"EPIC Missing\": 0,\n",
    "                \"SecuTrial Missing\": 0,\n",
    "                \"Mismatches\": 0\n",
    "            }\n",
    "\n",
    "    return pd.DataFrame(mismatched_results), percentage_stats, monthly_percentage_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_epic_secuTrial(epic_df, secuTrial_df, mapping_df, value_mappings=None):\n",
    "    \"\"\"\n",
    "    Compares values and data types between EPIC and secuTrial DataFrames using a mapping file,\n",
    "    accounting for data type differences properly. Includes monthly breakdown of statistics\n",
    "    and variable-level statistics.\n",
    "\n",
    "    Args:\n",
    "        epic_df (DataFrame): The EPIC dataset.\n",
    "        secuTrial_df (DataFrame): The SecuTrial dataset.\n",
    "        mapping_df (DataFrame): The mapping Excel file with data type information.\n",
    "        value_mappings (dict, optional): Dictionary for value conversions in EPIC.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with mismatched results in values and data types.\n",
    "        dict: Dictionary containing percentage statistics.\n",
    "        dict: Dictionary containing monthly statistics.\n",
    "        dict: Dictionary containing variable-level statistics.\n",
    "    \"\"\"\n",
    "    \n",
    "    import pandas as pd\n",
    "    import re\n",
    "    from datetime import datetime\n",
    "    \n",
    "    if value_mappings is None:\n",
    "        value_mappings = {}\n",
    "        \n",
    "    # Create working copies to avoid modifying original dataframes\n",
    "    epic_df_copy = epic_df.copy()\n",
    "    secuTrial_df_copy = secuTrial_df.copy()\n",
    "    \n",
    "    # Replace missing value indicators\n",
    "    epic_df_copy.replace(-9999, pd.NA, inplace=True)\n",
    "    secuTrial_df_copy.replace(-9999, pd.NA, inplace=True)\n",
    "\n",
    "    # Ensure necessary columns exist before comparison\n",
    "    if \"FID\" not in epic_df_copy.columns or \"SSR\" not in epic_df_copy.columns:\n",
    "        raise ValueError(\"EPIC DataFrame must contain 'FID' and 'SSR' columns.\")\n",
    "    if \"FID\" not in secuTrial_df_copy.columns or \"SSR\" not in secuTrial_df_copy.columns:\n",
    "        raise ValueError(\"SecuTrial DataFrame must contain 'FID' and 'SSR' columns.\")\n",
    "        \n",
    "    # Modify line ~40-43 in the function\n",
    "    if \"enct.arrival_date\" not in epic_df_copy.columns:\n",
    "        raise ValueError(\"EPIC DataFrame must contain 'enct.arrival_date' column for monthly breakdown.\")\n",
    "    if \"Arrival at hospital\" not in secuTrial_df_copy.columns:\n",
    "        raise ValueError(\"SecuTrial DataFrame must contain 'Arrival at hospital' column for monthly breakdown.\")\n",
    "\n",
    "    # And then use these columns for date conversion ~46-47\n",
    "    epic_df_copy['DATE'] = pd.to_datetime(epic_df_copy['enct.arrival_date'], errors='coerce')\n",
    "    secuTrial_df_copy['DATE'] = pd.to_datetime(secuTrial_df_copy['Arrival at hospital'], errors='coerce')\n",
    "\n",
    "    # Create a set of (FID, SSR) pairs that exist in both DataFrames\n",
    "    matching_keys = set(epic_df_copy[['FID', 'SSR']].apply(tuple, axis=1)) & set(secuTrial_df_copy[['FID', 'SSR']].apply(tuple, axis=1))\n",
    "\n",
    "    # Store mismatched results\n",
    "    mismatched_results = []\n",
    "    match_count = 0\n",
    "    secu_missing_count = 0\n",
    "    epic_missing_count = 0\n",
    "    mismatch_count = 0\n",
    "    total_comparisons = 0\n",
    "    \n",
    "    # Setup monthly statistics tracking\n",
    "    months = {4: 'April', 5: 'May', 6: 'June', 7: 'July', 8: 'August', \n",
    "              9: 'September', 10: 'October', 11: 'November', 12: 'December'}\n",
    "    \n",
    "    monthly_stats = {month_name: {'match_count': 0, 'secu_missing_count': 0, \n",
    "                                'epic_missing_count': 0, 'mismatch_count': 0, \n",
    "                                'total_compared': 0} \n",
    "                   for month_name in months.values()}\n",
    "    \n",
    "    # Create a dictionary to store variable-level statistics\n",
    "    variable_stats = {}\n",
    "    \n",
    "    # Helper function to standardize boolean values\n",
    "    def standardize_boolean(value):\n",
    "        if pd.isna(value):\n",
    "            return pd.NA\n",
    "        \n",
    "        if isinstance(value, bool):\n",
    "            return \"yes\" if value else \"no\"\n",
    "        elif isinstance(value, (int, float)):\n",
    "            return \"yes\" if value else \"no\"\n",
    "        elif isinstance(value, str):\n",
    "            if value.lower() in ['true', 'yes', 'y', '1', 't']:\n",
    "                return \"yes\"\n",
    "            elif value.lower() in ['false', 'no', 'n', '0', 'f']:\n",
    "                return \"no\"\n",
    "        \n",
    "        return str(value)\n",
    "    \n",
    "    # Helper function to convert values to the correct type\n",
    "    def convert_to_type(value, target_type):\n",
    "        \"\"\"Convert value to specified type with specific formatting\"\"\"\n",
    "        if pd.isna(value):\n",
    "            return pd.NA\n",
    "            \n",
    "        # Handle various data types\n",
    "        if not isinstance(target_type, str):\n",
    "            return value  # If no type specified, return as is\n",
    "            \n",
    "        # Check for float with decimal specification (e.g., float-1, float-2)\n",
    "        float_match = re.match(r'float-(\\d+)', target_type.lower())\n",
    "        if float_match:\n",
    "            try:\n",
    "                decimal_places = int(float_match.group(1))\n",
    "                if value == '':\n",
    "                    return pd.NA\n",
    "                float_val = float(value)\n",
    "                return round(float_val, decimal_places)\n",
    "            except (ValueError, TypeError):\n",
    "                return pd.NA\n",
    "        \n",
    "        if target_type.lower() in ['int', 'integer', 'int64', 'int32']:\n",
    "            try:\n",
    "                return int(float(value)) if value != '' else pd.NA\n",
    "            except (ValueError, TypeError):\n",
    "                return pd.NA\n",
    "        elif target_type.lower() in ['float', 'double', 'numeric', 'float64', 'float32']:\n",
    "            try:\n",
    "                return float(value) if value != '' else pd.NA\n",
    "            except (ValueError, TypeError):\n",
    "                return pd.NA\n",
    "        elif target_type.lower() in ['date', 'datetime', 'timestamp']:\n",
    "            try:\n",
    "                if value == '':\n",
    "                    return pd.NA\n",
    "                # Convert to datetime and then to yyyymmdd hh:mm format\n",
    "                dt = pd.to_datetime(value)\n",
    "                return dt.strftime('%Y%m%d %H:%M')\n",
    "            except (ValueError, TypeError, AttributeError):\n",
    "                return pd.NA\n",
    "        elif target_type.lower() in ['bool', 'boolean']:\n",
    "            return standardize_boolean(value)\n",
    "        else:\n",
    "            # Default to string for text, categorical, etc.\n",
    "            return str(value) if value is not None and value != '' else pd.NA\n",
    "    \n",
    "    # Helper function to check if values are equivalent\n",
    "    def equivalent_values(val1, val2, target_type):\n",
    "        \"\"\"Compare values with formatted type awareness\"\"\"\n",
    "        # Handle NaN values consistently\n",
    "        if pd.isna(val1) and pd.isna(val2):\n",
    "            return True\n",
    "        elif pd.isna(val1) or pd.isna(val2):\n",
    "            return False\n",
    "            \n",
    "        # Check for float with decimal specification (e.g., float-1, float-2)\n",
    "        float_match = re.match(r'float-(\\d+)', target_type.lower()) if isinstance(target_type, str) else None\n",
    "        if float_match:\n",
    "            try:\n",
    "                decimal_places = int(float_match.group(1))\n",
    "                val1_rounded = round(float(val1), decimal_places)\n",
    "                val2_rounded = round(float(val2), decimal_places)\n",
    "                return val1_rounded == val2_rounded\n",
    "            except (ValueError, TypeError):\n",
    "                return False\n",
    "        \n",
    "        # Boolean comparison (standardized to yes/no)\n",
    "        if isinstance(target_type, str) and target_type.lower() in ['bool', 'boolean']:\n",
    "            val1_std = standardize_boolean(val1)\n",
    "            val2_std = standardize_boolean(val2)\n",
    "            return val1_std == val2_std\n",
    "        \n",
    "        # Special handling for numeric types\n",
    "        if isinstance(val1, (int, float)) and isinstance(val2, (int, float)):\n",
    "            try:\n",
    "                return abs(float(val1) - float(val2)) < 1e-6\n",
    "            except (ValueError, TypeError):\n",
    "                return False\n",
    "        \n",
    "        # Date comparison (already in string format)\n",
    "        if isinstance(target_type, str) and target_type.lower() in ['date', 'datetime', 'timestamp']:\n",
    "            return val1 == val2\n",
    "            \n",
    "        # String comparison (case insensitive)\n",
    "        elif isinstance(val1, str) and isinstance(val2, str):\n",
    "            return val1.strip().lower() == val2.strip().lower()\n",
    "            \n",
    "        # Default comparison\n",
    "        else:\n",
    "            return str(val1) == str(val2)\n",
    "    \n",
    "    # Build a column mapping dictionary for easier lookups\n",
    "    column_mappings = {}\n",
    "    column_types = {}\n",
    "    \n",
    "    for _, row in mapping_df.iterrows():\n",
    "        epic_column_name = row.get('EPIC_varColumnName', None)\n",
    "        secuTrial_column_name = row.get('sT_varColumnName', None)\n",
    "        epic_dtype = row.get('EPIC_varType', 'text')  # Default to text if not specified\n",
    "        secuTrial_dtype = row.get('sT_varType', 'text')  # Default to text if not specified\n",
    "        column_source = row.get('EPIC_exportFileName', None)\n",
    "        secu_source = row.get('sT_exportFileName', None)\n",
    "\n",
    "        if not isinstance(epic_column_name, str) or not isinstance(secuTrial_column_name, str):\n",
    "            continue  # Skip if column names are missing\n",
    "\n",
    "        # Determine EPIC column prefix\n",
    "        prefix = \"\"\n",
    "        if isinstance(column_source, str):\n",
    "            if \"encounter\" in column_source:\n",
    "                prefix = \"enct.\"\n",
    "            elif \"flowsheet\" in column_source:\n",
    "                prefix = \"flow.\"\n",
    "            elif \"imaging\" in column_source:\n",
    "                prefix = \"img.\"\n",
    "            elif \"lab\" in column_source:\n",
    "                prefix = \"lab.\"\n",
    "            elif \"medication\" in column_source:\n",
    "                prefix = \"med.\"\n",
    "            elif \"monitor\" in column_source:\n",
    "                prefix = \"mon.\"\n",
    "\n",
    "        # Determine SecuTrial column suffix\n",
    "        suffix = \"\"\n",
    "        if isinstance(secu_source, str) and \"REVASC\" in secu_source:\n",
    "            suffix = \".revas\"\n",
    "\n",
    "        # Construct fully qualified column names\n",
    "        epic_col = f\"{prefix}{epic_column_name}\"  # EPIC column with prefix\n",
    "        secu_col = f\"{secuTrial_column_name}{suffix}\"  # SecuTrial column with suffix\n",
    "        \n",
    "        # If secuTrial type is int, override EPIC type to also be int\n",
    "        if secuTrial_dtype.lower() in ['int', 'integer', 'int64', 'int32']:\n",
    "            epic_dtype = 'int'\n",
    "        # If secuTrial type is float or float-n, override EPIC type\n",
    "        elif secuTrial_dtype.lower() in ['float', 'double', 'numeric', 'float64', 'float32'] or re.match(r'float-\\d+', secuTrial_dtype.lower()):\n",
    "            epic_dtype = secuTrial_dtype\n",
    "        \n",
    "        # Store the mapping\n",
    "        column_mappings[epic_col] = secu_col\n",
    "        column_types[epic_col] = {'epic_type': epic_dtype, 'secu_type': secuTrial_dtype}\n",
    "        \n",
    "        # Initialize variable stats for this column pair\n",
    "        variable_stats[f\"{epic_col} <-> {secu_col}\"] = {\n",
    "            'match_count': 0,\n",
    "            'epic_missing_count': 0,\n",
    "            'secu_missing_count': 0,\n",
    "            'mismatch_count': 0,\n",
    "            'total_compared': 0,\n",
    "            'epic_type': epic_dtype,\n",
    "            'secu_type': secuTrial_dtype\n",
    "        }\n",
    "    \n",
    "    # First, apply value mappings to EPIC data\n",
    "    for col, mapping in value_mappings.items():\n",
    "        if col in epic_df_copy.columns:\n",
    "            epic_df_copy[col] = epic_df_copy[col].map(lambda x: mapping.get(x, x))\n",
    "    \n",
    "    # Next, convert data types in both dataframes\n",
    "    for epic_col, secu_col in column_mappings.items():\n",
    "        # For EPIC dataframe\n",
    "        if epic_col in epic_df_copy.columns:\n",
    "            target_type = column_types[epic_col]['epic_type']\n",
    "            epic_df_copy[epic_col] = epic_df_copy[epic_col].apply(lambda x: convert_to_type(x, target_type))\n",
    "        \n",
    "        # For secuTrial dataframe\n",
    "        if secu_col in secuTrial_df_copy.columns:\n",
    "            target_type = column_types[epic_col]['secu_type']\n",
    "            secuTrial_df_copy[secu_col] = secuTrial_df_copy[secu_col].apply(lambda x: convert_to_type(x, target_type))\n",
    "    \n",
    "    # Now compare the values\n",
    "    for epic_col, secu_col in column_mappings.items():\n",
    "        var_key = f\"{epic_col} <-> {secu_col}\"\n",
    "        \n",
    "        # Check if columns exist in respective DataFrames\n",
    "        if epic_col not in epic_df_copy.columns and secu_col in secuTrial_df_copy.columns:\n",
    "            epic_missing_count += 1\n",
    "            variable_stats[var_key]['epic_missing_count'] += 1\n",
    "            variable_stats[var_key]['total_compared'] += 1\n",
    "            continue\n",
    "        elif secu_col not in secuTrial_df_copy.columns and epic_col in epic_df_copy.columns:\n",
    "            secu_missing_count += 1\n",
    "            variable_stats[var_key]['secu_missing_count'] += 1\n",
    "            variable_stats[var_key]['total_compared'] += 1\n",
    "            continue\n",
    "        elif epic_col not in epic_df_copy.columns and secu_col not in secuTrial_df_copy.columns:\n",
    "            continue  # Skip comparison if column is missing in both\n",
    "\n",
    "        total_comparisons += 1\n",
    "        \n",
    "        # Get target type for this column\n",
    "        target_type = column_types[epic_col]['secu_type']  # Use secuTrial type as the target\n",
    "\n",
    "        # Compare values for rows with matching (FID, SSR)\n",
    "        for fid, ssr in matching_keys:\n",
    "            epic_row = epic_df_copy.loc[(epic_df_copy[\"FID\"] == fid) & (epic_df_copy[\"SSR\"] == ssr)]\n",
    "            secu_row = secuTrial_df_copy.loc[(secuTrial_df_copy[\"FID\"] == fid) & (secuTrial_df_copy[\"SSR\"] == ssr)]\n",
    "            \n",
    "            if epic_row.empty or secu_row.empty:\n",
    "                continue  # Skip if no matching row found\n",
    "                \n",
    "            epic_value = epic_row[epic_col].iloc[0] if epic_col in epic_row.columns else pd.NA\n",
    "            secu_value = secu_row[secu_col].iloc[0] if secu_col in secu_row.columns else pd.NA\n",
    "            \n",
    "            # Get the month for this record (use epic date if available, else secu date)\n",
    "            record_date = None\n",
    "            if not epic_row.empty and 'DATE' in epic_row.columns and not pd.isna(epic_row['DATE'].iloc[0]):\n",
    "                record_date = epic_row['DATE'].iloc[0]\n",
    "            elif not secu_row.empty and 'DATE' in secu_row.columns and not pd.isna(secu_row['DATE'].iloc[0]):\n",
    "                record_date = secu_row['DATE'].iloc[0]\n",
    "                \n",
    "            # Skip if no valid date or not in April-December range\n",
    "            if record_date is None:\n",
    "                continue\n",
    "                \n",
    "            record_month = record_date.month\n",
    "            # Skip if not in our target month range (April-December)\n",
    "            if record_month < 4 or record_month > 12:\n",
    "                continue\n",
    "                \n",
    "            month_name = months[record_month]\n",
    "                \n",
    "            # Both values are NaN/missing - count as match\n",
    "            if pd.isna(epic_value) and pd.isna(secu_value):\n",
    "                match_count += 1\n",
    "                monthly_stats[month_name]['match_count'] += 1\n",
    "                monthly_stats[month_name]['total_compared'] += 1\n",
    "                variable_stats[var_key]['match_count'] += 1\n",
    "                variable_stats[var_key]['total_compared'] += 1\n",
    "                continue\n",
    "                \n",
    "            # Only secuTrial value is NaN/missing\n",
    "            if pd.isna(secu_value) and not pd.isna(epic_value):\n",
    "                secu_missing_count += 1\n",
    "                monthly_stats[month_name]['secu_missing_count'] += 1\n",
    "                monthly_stats[month_name]['total_compared'] += 1\n",
    "                variable_stats[var_key]['secu_missing_count'] += 1\n",
    "                variable_stats[var_key]['total_compared'] += 1\n",
    "                continue\n",
    "                \n",
    "            # Only EPIC value is NaN/missing\n",
    "            if pd.isna(epic_value) and not pd.isna(secu_value):\n",
    "                epic_missing_count += 1\n",
    "                monthly_stats[month_name]['epic_missing_count'] += 1\n",
    "                monthly_stats[month_name]['total_compared'] += 1\n",
    "                variable_stats[var_key]['epic_missing_count'] += 1\n",
    "                variable_stats[var_key]['total_compared'] += 1\n",
    "                continue\n",
    "\n",
    "            # Compare values using the target type\n",
    "            if equivalent_values(epic_value, secu_value, target_type):\n",
    "                match_count += 1\n",
    "                monthly_stats[month_name]['match_count'] += 1\n",
    "                variable_stats[var_key]['match_count'] += 1\n",
    "            else:\n",
    "                mismatch_count += 1\n",
    "                monthly_stats[month_name]['mismatch_count'] += 1\n",
    "                variable_stats[var_key]['mismatch_count'] += 1\n",
    "                mismatched_results.append({\n",
    "                    'FID': fid,\n",
    "                    'SSR': ssr,\n",
    "                    'Month': month_name,\n",
    "                    'DATE': record_date,\n",
    "                    'EPIC Column': epic_col,\n",
    "                    'SecuTrial Column': secu_col,\n",
    "                    'EPIC Value': str(epic_value),\n",
    "                    'SecuTrial Value': str(secu_value),\n",
    "                    'EPIC Expected Type': column_types[epic_col]['epic_type'],\n",
    "                    'SecuTrial Expected Type': target_type,\n",
    "                    'EPIC Actual Type': type(epic_value).__name__,\n",
    "                    'SecuTrial Actual Type': type(secu_value).__name__\n",
    "                })\n",
    "            \n",
    "            monthly_stats[month_name]['total_compared'] += 1\n",
    "            variable_stats[var_key]['total_compared'] += 1\n",
    "\n",
    "    # Calculate percentages\n",
    "    total_compared = match_count + mismatch_count + secu_missing_count + epic_missing_count\n",
    "    percentage_stats = {\n",
    "        \"Matching Variables (%)\": round((match_count / total_compared) * 100, 2) if total_compared else 0,\n",
    "        \"Variables Missing in EPIC (%)\": round((epic_missing_count / total_compared) * 100, 2) if total_compared else 0,\n",
    "        \"Variables Missing in SecuTrial (%)\": round((secu_missing_count / total_compared) * 100, 2) if total_compared else 0,\n",
    "        \"Mismatched Variables (%)\": round((mismatch_count / total_compared) * 100, 2) if total_compared else 0,\n",
    "        \"Total Comparisons\": total_compared,\n",
    "        \"Matches\": match_count,\n",
    "        \"EPIC Missing\": epic_missing_count,\n",
    "        \"SecuTrial Missing\": secu_missing_count,\n",
    "        \"Mismatches\": mismatch_count\n",
    "    }\n",
    "    \n",
    "    # Calculate monthly percentages\n",
    "    monthly_percentage_stats = {}\n",
    "    for month, stats in monthly_stats.items():\n",
    "        total = stats['total_compared']\n",
    "        if total > 0:\n",
    "            monthly_percentage_stats[month] = {\n",
    "                \"Matching Variables (%)\": round((stats['match_count'] / total) * 100, 2),\n",
    "                \"Variables Missing in EPIC (%)\": round((stats['epic_missing_count'] / total) * 100, 2),\n",
    "                \"Variables Missing in SecuTrial (%)\": round((stats['secu_missing_count'] / total) * 100, 2),\n",
    "                \"Mismatched Variables (%)\": round((stats['mismatch_count'] / total) * 100, 2),\n",
    "                \"Total Comparisons\": total,\n",
    "                \"Matches\": stats['match_count'],\n",
    "                \"EPIC Missing\": stats['epic_missing_count'],\n",
    "                \"SecuTrial Missing\": stats['secu_missing_count'],\n",
    "                \"Mismatches\": stats['mismatch_count']\n",
    "            }\n",
    "        else:\n",
    "            monthly_percentage_stats[month] = {\n",
    "                \"Matching Variables (%)\": 0,\n",
    "                \"Variables Missing in EPIC (%)\": 0,\n",
    "                \"Variables Missing in SecuTrial (%)\": 0,\n",
    "                \"Mismatched Variables (%)\": 0,\n",
    "                \"Total Comparisons\": 0,\n",
    "                \"Matches\": 0,\n",
    "                \"EPIC Missing\": 0,\n",
    "                \"SecuTrial Missing\": 0,\n",
    "                \"Mismatches\": 0\n",
    "            }\n",
    "    \n",
    "    # Calculate variable-level percentages\n",
    "    variable_percentage_stats = {}\n",
    "    for var_key, stats in variable_stats.items():\n",
    "        total = stats['total_compared']\n",
    "        if total > 0:\n",
    "            variable_percentage_stats[var_key] = {\n",
    "                \"Matching Values (%)\": round((stats['match_count'] / total) * 100, 2),\n",
    "                \"Values Missing in EPIC (%)\": round((stats['epic_missing_count'] / total) * 100, 2),\n",
    "                \"Values Missing in SecuTrial (%)\": round((stats['secu_missing_count'] / total) * 100, 2),\n",
    "                \"Mismatched Values (%)\": round((stats['mismatch_count'] / total) * 100, 2),\n",
    "                \"Total Comparisons\": total,\n",
    "                \"Matches\": stats['match_count'],\n",
    "                \"EPIC Missing\": stats['epic_missing_count'],\n",
    "                \"SecuTrial Missing\": stats['secu_missing_count'],\n",
    "                \"Mismatches\": stats['mismatch_count'],\n",
    "                \"EPIC Type\": stats['epic_type'],\n",
    "                \"SecuTrial Type\": stats['secu_type']\n",
    "            }\n",
    "        else:\n",
    "            variable_percentage_stats[var_key] = {\n",
    "                \"Matching Values (%)\": 0,\n",
    "                \"Values Missing in EPIC (%)\": 0,\n",
    "                \"Values Missing in SecuTrial (%)\": 0,\n",
    "                \"Mismatched Values (%)\": 0,\n",
    "                \"Total Comparisons\": 0,\n",
    "                \"Matches\": 0,\n",
    "                \"EPIC Missing\": 0,\n",
    "                \"SecuTrial Missing\": 0,\n",
    "                \"Mismatches\": 0,\n",
    "                \"EPIC Type\": stats['epic_type'],\n",
    "                \"SecuTrial Type\": stats['secu_type']\n",
    "            }\n",
    "\n",
    "    return pd.DataFrame(mismatched_results), percentage_stats, monthly_percentage_stats, variable_percentage_stats\n",
    "\n",
    "# Function to get top problematic variables\n",
    "def get_top_problematic_variables(variable_stats, sort_by='mismatch_percent', top_n=10):\n",
    "    \"\"\"\n",
    "    Identify the most problematic variables based on specified criteria\n",
    "    \n",
    "    Args:\n",
    "        variable_stats (dict): Dictionary containing variable-level statistics\n",
    "        sort_by (str): Criteria to sort by: 'mismatch_percent', 'missing_epic_percent', \n",
    "                       'missing_secuTrial_percent', or 'total_problems'\n",
    "        top_n (int): Number of variables to return\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Top problematic variables sorted by the specified criteria\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Create a DataFrame from the variable stats\n",
    "    var_df = pd.DataFrame()\n",
    "    \n",
    "    for var_name, stats in variable_stats.items():\n",
    "        if stats['Total Comparisons'] > 0:  # Only include variables with actual comparisons\n",
    "            row = {\n",
    "                'Variable': var_name,\n",
    "                'Total Comparisons': stats['Total Comparisons'],\n",
    "                'Match Count': stats['Matches'],\n",
    "                'Match Percent': stats['Matching Values (%)'],\n",
    "                'Mismatch Count': stats['Mismatches'],\n",
    "                'Mismatch Percent': stats['Mismatched Values (%)'],\n",
    "                'EPIC Missing Count': stats['EPIC Missing'],\n",
    "                'EPIC Missing Percent': stats['Values Missing in EPIC (%)'],\n",
    "                'SecuTrial Missing Count': stats['SecuTrial Missing'],\n",
    "                'SecuTrial Missing Percent': stats['Values Missing in SecuTrial (%)'],\n",
    "                'EPIC Type': stats['EPIC Type'],\n",
    "                'SecuTrial Type': stats['SecuTrial Type'],\n",
    "                'Total Problems': stats['Mismatches'] + stats['EPIC Missing'] + stats['SecuTrial Missing'],\n",
    "                'Total Problem Percent': (100 - stats['Matching Values (%)'])\n",
    "            }\n",
    "            var_df = pd.concat([var_df, pd.DataFrame([row])], ignore_index=True)\n",
    "    \n",
    "    # Sort based on criteria\n",
    "    if sort_by == 'mismatch_percent':\n",
    "        var_df = var_df.sort_values(by='Mismatch Percent', ascending=False)\n",
    "    elif sort_by == 'missing_epic_percent':\n",
    "        var_df = var_df.sort_values(by='EPIC Missing Percent', ascending=False)\n",
    "    elif sort_by == 'missing_secuTrial_percent':\n",
    "        var_df = var_df.sort_values(by='SecuTrial Missing Percent', ascending=False)\n",
    "    elif sort_by == 'total_problems':\n",
    "        var_df = var_df.sort_values(by='Total Problem Percent', ascending=False)\n",
    "    else:\n",
    "        var_df = var_df.sort_values(by='Total Problem Percent', ascending=False)\n",
    "    \n",
    "    return var_df.head(top_n)\n",
    "\n",
    "# Function to generate detailed report\n",
    "def generate_comparison_report(mismatched_results, overall_stats, monthly_stats, variable_stats):\n",
    "    \"\"\"\n",
    "    Generates a comprehensive report from the comparison results\n",
    "    \n",
    "    Args:\n",
    "        mismatched_results (DataFrame): DataFrame with details of mismatched values\n",
    "        overall_stats (dict): Dictionary with overall statistics\n",
    "        monthly_stats (dict): Dictionary with monthly breakdown of statistics\n",
    "        variable_stats (dict): Dictionary with variable-level statistics\n",
    "        \n",
    "    Returns:\n",
    "        str: Markdown formatted report\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from io import StringIO\n",
    "    \n",
    "    report = StringIO()\n",
    "    \n",
    "    # Overall Summary\n",
    "    report.write(\"# EPIC-SecuTrial Data Comparison Summary\\n\\n\")\n",
    "    report.write(\"## Overall Statistics\\n\\n\")\n",
    "    report.write(f\"* Total Comparisons: {overall_stats['Total Comparisons']}\\n\")\n",
    "    report.write(f\"* Matching Variables: {overall_stats['Matches']} ({overall_stats['Matching Variables (%)']}%)\\n\")\n",
    "    report.write(f\"* Mismatched Variables: {overall_stats['Mismatches']} ({overall_stats['Mismatched Variables (%)']}%)\\n\")\n",
    "    report.write(f\"* Variables Missing in EPIC: {overall_stats['EPIC Missing']} ({overall_stats['Variables Missing in EPIC (%)']}%)\\n\")\n",
    "    report.write(f\"* Variables Missing in SecuTrial: {overall_stats['SecuTrial Missing']} ({overall_stats['Variables Missing in SecuTrial (%)']}%)\\n\\n\")\n",
    "    \n",
    "    # Monthly Breakdown\n",
    "    report.write(\"## Monthly Statistics\\n\\n\")\n",
    "    monthly_df = pd.DataFrame(columns=['Month', 'Total Comparisons', 'Matching (%)', 'Mismatched (%)', \n",
    "                                       'EPIC Missing (%)', 'SecuTrial Missing (%)'])\n",
    "    \n",
    "    for month, stats in monthly_stats.items():\n",
    "        monthly_df = pd.concat([monthly_df, pd.DataFrame([{\n",
    "            'Month': month,\n",
    "            'Total Comparisons': stats['Total Comparisons'],\n",
    "            'Matching (%)': stats['Matching Variables (%)'],\n",
    "            'Mismatched (%)': stats['Mismatched Variables (%)'],\n",
    "            'EPIC Missing (%)': stats['Variables Missing in EPIC (%)'],\n",
    "            'SecuTrial Missing (%)': stats['Variables Missing in SecuTrial (%)']\n",
    "        }])], ignore_index=True)\n",
    "    \n",
    "    report.write(monthly_df.to_markdown(index=False))\n",
    "    report.write(\"\\n\\n\")\n",
    "    \n",
    "    # Top Problematic Variables\n",
    "    report.write(\"## Top 10 Problematic Variables\\n\\n\")\n",
    "    top_vars = get_top_problematic_variables(variable_stats, sort_by='total_problems', top_n=10)\n",
    "    report.write(top_vars[['Variable', 'Total Comparisons', 'Match Percent', 'Mismatch Percent', \n",
    "                           'EPIC Missing Percent', 'SecuTrial Missing Percent', 'EPIC Type', 'SecuTrial Type']]\n",
    "                .to_markdown(index=False))\n",
    "    report.write(\"\\n\\n\")\n",
    "    \n",
    "    # Variables with Type Mismatches\n",
    "    report.write(\"## Variables with Type Mismatches\\n\\n\")\n",
    "    type_mismatches = []\n",
    "    for var_name, stats in variable_stats.items():\n",
    "        if stats['EPIC Type'] != stats['SecuTrial Type']:\n",
    "            type_mismatches.append({\n",
    "                'Variable': var_name,\n",
    "                'EPIC Type': stats['EPIC Type'],\n",
    "                'SecuTrial Type': stats['SecuTrial Type'],\n",
    "                'Mismatch Percent': stats['Mismatched Values (%)']\n",
    "            })\n",
    "    \n",
    "    if type_mismatches:\n",
    "        type_mismatch_df = pd.DataFrame(type_mismatches)\n",
    "        report.write(type_mismatch_df.sort_values(by='Mismatch Percent', ascending=False).to_markdown(index=False))\n",
    "    else:\n",
    "        report.write(\"No type mismatches found.\\n\")\n",
    "    \n",
    "    return report.getvalue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### comparison application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/7d7jg9x11fb6sn0rshxtgnj80000gn/T/ipykernel_18816/3811210246.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_EPIC_all_2['DATE'] = df_EPIC_all_2['enct.arrival_date']\n"
     ]
    }
   ],
   "source": [
    "# Add DATE column to EPIC dataframe\n",
    "df_EPIC_all_2['DATE'] = df_EPIC_all_2['enct.arrival_date'] \n",
    "\n",
    "# Add DATE column to secuTrial dataframe\n",
    "df_secuTrial_w_REVAS_2['DATE'] = df_secuTrial_w_REVAS_2['Arrival at hospital']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now proceed with comparison using the aligned data\n",
    "#mismatched_results, comparison_stats, monthly_percentage_stats = compare_epic_secuTrial(df_EPIC_all_2, df_secuTrial_w_REVAS_2, df_mapping, value_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/7d7jg9x11fb6sn0rshxtgnj80000gn/T/ipykernel_18816/2695550548.py:128: UserWarning: Parsing dates in %d.%m.%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  dt = pd.to_datetime(value)\n",
      "/var/folders/3_/7d7jg9x11fb6sn0rshxtgnj80000gn/T/ipykernel_18816/2695550548.py:533: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  monthly_df = pd.concat([monthly_df, pd.DataFrame([{\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# EPIC-SecuTrial Data Comparison Summary\n",
      "\n",
      "## Overall Statistics\n",
      "\n",
      "* Total Comparisons: 109448\n",
      "* Matching Variables: 59859 (54.69%)\n",
      "* Mismatched Variables: 4101 (3.75%)\n",
      "* Variables Missing in EPIC: 8362 (7.64%)\n",
      "* Variables Missing in SecuTrial: 37126 (33.92%)\n",
      "\n",
      "## Monthly Statistics\n",
      "\n",
      "| Month     |   Total Comparisons |   Matching (%) |   Mismatched (%) |   EPIC Missing (%) |   SecuTrial Missing (%) |\n",
      "|:----------|--------------------:|---------------:|-----------------:|-------------------:|------------------------:|\n",
      "| April     |               10000 |          47.92 |             4.58 |              21.07 |                   26.43 |\n",
      "| May       |               11760 |          62.86 |             7.99 |              18.78 |                   10.37 |\n",
      "| June      |                9760 |          67.72 |             4.53 |               3.56 |                   24.2  |\n",
      "| July      |               11840 |          64.77 |             6.24 |              13.29 |                   15.69 |\n",
      "| August    |               11920 |          67.53 |             4.77 |               4.03 |                   23.67 |\n",
      "| September |               12880 |          49.91 |             5.2  |              12.17 |                   32.72 |\n",
      "| October   |               14080 |          47.91 |             0.87 |               0.35 |                   50.87 |\n",
      "| November  |               14320 |          45.15 |             0.61 |               0.15 |                   54.08 |\n",
      "| December  |               12880 |          44.31 |             0.57 |               0    |                   55.12 |\n",
      "\n",
      "## Top 10 Problematic Variables\n",
      "\n",
      "| Variable                                              |   Total Comparisons |   Match Percent |   Mismatch Percent |   EPIC Missing Percent |   SecuTrial Missing Percent | EPIC Type   | SecuTrial Type   |\n",
      "|:------------------------------------------------------|--------------------:|----------------:|-------------------:|-----------------------:|----------------------------:|:------------|:-----------------|\n",
      "| img.firstangio_result <-> 1st vascular imaging result |                   1 |            0    |               0    |                 100    |                        0    | str         | str              |\n",
      "| enct.bmi <-> BMI                                      |                   1 |            0    |               0    |                 100    |                        0    | float       | float            |\n",
      "| med.iat_uk <-> IAT urokinase                          |                   1 |            0    |               0    |                 100    |                        0    | nan         | bool             |\n",
      "| med.iat_rtpa <-> IAT rtPA                             |                   1 |            0    |               0    |                 100    |                        0    | nan         | bool             |\n",
      "| med.inr <-> Vit. K ag INR                             |                   1 |            0    |               0    |                 100    |                        0    | float       | float            |\n",
      "| lab.admis_crp <-> CRP.revas                           |                   1 |            0    |               0    |                   0    |                      100    | int         | int              |\n",
      "| enct.age <-> Age (calc.)                              |                   1 |            0    |               0    |                 100    |                        0    | float       | float            |\n",
      "| lab.admis_platelets <-> Platelets.revas               |                   1 |            0    |               0    |                   0    |                      100    | int         | int              |\n",
      "| enct.non_swiss <-> Non-Swiss                          |                1368 |            1.02 |               0    |                   0    |                       98.98 | bool        | bool             |\n",
      "| flow.gcs_admission <-> GCS on admission               |                1368 |            5.85 |               2.27 |                   0.29 |                       91.59 | int         | int              |\n",
      "\n",
      "## Variables with Type Mismatches\n",
      "\n",
      "| Variable                                               | EPIC Type   | SecuTrial Type   |   Mismatch Percent |\n",
      "|:-------------------------------------------------------|:------------|:-----------------|-------------------:|\n",
      "| img.firstangio_type <-> 1st vascular imaging type      | int         | str              |              16.59 |\n",
      "| med.treat_ivt <-> IVT with rtPA                        | bool        | str              |               5.85 |\n",
      "| enct.sex <-> Sex                                       | int         | str              |               0    |\n",
      "| enct.transport <-> Transport                           | int         | str              |               0    |\n",
      "| enct.living_pre <-> Prestroke living situation         | int         | str              |               0    |\n",
      "| enct.discharge_destinat <-> Discharge destination      | int         | str              |               0    |\n",
      "| flow.firstangio_result <-> 1st vascular imaging result | int         | str              |               0    |\n",
      "| flow.mca <-> MCA                                       | int         | str              |               0    |\n",
      "| flow.aca <-> ACA                                       | int         | str              |               0    |\n",
      "| flow.pca <-> PCA                                       | int         | str              |               0    |\n",
      "| flow.vertebrobasilar <-> Vertebro-basilar              | int         | str              |               0    |\n",
      "| flow.decompression <-> Decompr. craniectomy            | str         | bool             |               0    |\n",
      "| img.firstimage_type <-> 1st brain imaging type         | int         | str              |               0    |\n",
      "| med.iat_rtpa <-> IAT rtPA                              | nan         | bool             |               0    |\n",
      "| med.iat_uk <-> IAT urokinase                           | nan         | bool             |               0    |\n"
     ]
    }
   ],
   "source": [
    "# Now proceed with comparison using the aligned data\n",
    "mismatched_results, comparison_stats, monthly_percentage_stats, variable_stats = compare_epic_secuTrial(df_EPIC_all_2, df_secuTrial_w_REVAS_2, df_mapping, value_mappings)\n",
    "\n",
    "# Optionally generate a comprehensive report\n",
    "report = generate_comparison_report(mismatched_results, comparison_stats, monthly_percentage_stats, variable_stats)\n",
    "print(report)  # Or save to a file with: with open('comparison_report.md', 'w') as f: f.write(report)\n",
    "\n",
    "# Export variable statistics to Excel\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame for variable statistics\n",
    "variable_df = pd.DataFrame()\n",
    "for var_name, stats in variable_stats.items():\n",
    "    row = {\n",
    "        'Variable': var_name,\n",
    "        'Total Comparisons': stats['Total Comparisons'],\n",
    "        'Matches': stats['Matches'],\n",
    "        'Match Percent': stats['Matching Values (%)'],\n",
    "        'Mismatches': stats['Mismatches'],\n",
    "        'Mismatch Percent': stats['Mismatched Values (%)'],\n",
    "        'EPIC Missing': stats['EPIC Missing'],\n",
    "        'EPIC Missing Percent': stats['Values Missing in EPIC (%)'],\n",
    "        'SecuTrial Missing': stats['SecuTrial Missing'],\n",
    "        'SecuTrial Missing Percent': stats['Values Missing in SecuTrial (%)'],\n",
    "        'EPIC Type': stats['EPIC Type'],\n",
    "        'SecuTrial Type': stats['SecuTrial Type']\n",
    "    }\n",
    "    variable_df = pd.concat([variable_df, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "# Sort by highest mismatch percentage\n",
    "variable_df = variable_df.sort_values(by='Mismatch Percent', ascending=False)\n",
    "\n",
    "# Export to Excel\n",
    "variable_df.to_excel('variable_statistics.xlsx', index=False)\n",
    "\n",
    "# You can also export the mismatched results for detailed examination\n",
    "mismatched_results.to_excel('mismatched_values.xlsx', index=False)\n",
    "\n",
    "# Export monthly statistics to Excel\n",
    "monthly_df = pd.DataFrame()\n",
    "for month, stats in monthly_percentage_stats.items():\n",
    "    row = {\n",
    "        'Month': month,\n",
    "        'Total Comparisons': stats['Total Comparisons'],\n",
    "        'Matches': stats['Matches'],\n",
    "        'Match Percent': stats['Matching Variables (%)'],\n",
    "        'Mismatches': stats['Mismatches'],\n",
    "        'Mismatch Percent': stats['Mismatched Variables (%)'],\n",
    "        'EPIC Missing': stats['EPIC Missing'],\n",
    "        'EPIC Missing Percent': stats['Variables Missing in EPIC (%)'],\n",
    "        'SecuTrial Missing': stats['SecuTrial Missing'], \n",
    "        'SecuTrial Missing Percent': stats['Variables Missing in SecuTrial (%)']\n",
    "    }\n",
    "    monthly_df = pd.concat([monthly_df, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "# Export monthly statistics\n",
    "with pd.ExcelWriter('comparison_statistics.xlsx') as writer:\n",
    "    variable_df.to_excel(writer, sheet_name='Variable Statistics', index=False)\n",
    "    monthly_df.to_excel(writer, sheet_name='Monthly Statistics', index=False)\n",
    "    pd.DataFrame([comparison_stats]).to_excel(writer, sheet_name='Overall Statistics', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert comparison statistics to a DataFrame and display\n",
    "comparison_stats_df = pd.DataFrame([comparison_stats])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Matching Variables (%)': 54.69,\n",
       " 'Variables Missing in EPIC (%)': 7.64,\n",
       " 'Variables Missing in SecuTrial (%)': 33.92,\n",
       " 'Mismatched Variables (%)': 3.75,\n",
       " 'Total Comparisons': 109448,\n",
       " 'Matches': 59859,\n",
       " 'EPIC Missing': 8362,\n",
       " 'SecuTrial Missing': 37126,\n",
       " 'Mismatches': 4101}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monthly statistics saved to: /Users/yaskhanloo/Developer/bern-storke-center/EPIC-export-validation/validation-files/monthly_validation_stats_20250414_095129.xlsx\n",
      "\n",
      "Monthly Statistics Summary:\n",
      "---------------------------\n",
      "\n",
      "April:\n",
      "  Matches: 4792 (47.92%)\n",
      "  Mismatches: 458 (4.58%)\n",
      "  Total comparisons: 10000\n",
      "\n",
      "May:\n",
      "  Matches: 7392 (62.86%)\n",
      "  Mismatches: 940 (7.99%)\n",
      "  Total comparisons: 11760\n",
      "\n",
      "June:\n",
      "  Matches: 6609 (67.72%)\n",
      "  Mismatches: 442 (4.53%)\n",
      "  Total comparisons: 9760\n",
      "\n",
      "July:\n",
      "  Matches: 7669 (64.77%)\n",
      "  Mismatches: 739 (6.24%)\n",
      "  Total comparisons: 11840\n",
      "\n",
      "August:\n",
      "  Matches: 8050 (67.53%)\n",
      "  Mismatches: 568 (4.77%)\n",
      "  Total comparisons: 11920\n",
      "\n",
      "September:\n",
      "  Matches: 6428 (49.91%)\n",
      "  Mismatches: 670 (5.2%)\n",
      "  Total comparisons: 12880\n",
      "\n",
      "October:\n",
      "  Matches: 6746 (47.91%)\n",
      "  Mismatches: 122 (0.87%)\n",
      "  Total comparisons: 14080\n",
      "\n",
      "November:\n",
      "  Matches: 6466 (45.15%)\n",
      "  Mismatches: 88 (0.61%)\n",
      "  Total comparisons: 14320\n",
      "\n",
      "December:\n",
      "  Matches: 5707 (44.31%)\n",
      "  Mismatches: 74 (0.57%)\n",
      "  Total comparisons: 12880\n"
     ]
    }
   ],
   "source": [
    "# Save the monthly statistics to Excel\n",
    "output_dir = base_dir / 'EPIC-export-validation/validation-files'\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "monthly_stats_path = output_dir / f\"monthly_validation_stats_{timestamp}.xlsx\"\n",
    "\n",
    "# Convert monthly stats to DataFrames\n",
    "monthly_stats_df = pd.DataFrame.from_dict(monthly_percentage_stats, orient='index')\n",
    "\n",
    "# Create a styled Excel writer\n",
    "with pd.ExcelWriter(monthly_stats_path, engine='openpyxl') as writer:\n",
    "    # Write monthly stats\n",
    "    monthly_stats_df.to_excel(writer, sheet_name=\"Monthly_Stats\")\n",
    "    \n",
    "    # Write overall stats\n",
    "    pd.DataFrame([comparison_stats]).to_excel(writer, sheet_name=\"Overall_Stats\", index=False)\n",
    "\n",
    "print(f\"Monthly statistics saved to: {monthly_stats_path}\")\n",
    "\n",
    "# Print summary of monthly stats\n",
    "print(\"\\nMonthly Statistics Summary:\")\n",
    "print(\"---------------------------\")\n",
    "for month, stats in monthly_percentage_stats.items():\n",
    "    print(f\"\\n{month}:\")\n",
    "    print(f\"  Matches: {stats['Matches']} ({stats['Matching Variables (%)']}%)\")\n",
    "    print(f\"  Mismatches: {stats['Mismatches']} ({stats['Mismatched Variables (%)']}%)\")\n",
    "    print(f\"  Total comparisons: {stats['Total Comparisons']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mismatch report generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restructure_mismatched_data(differences_df, epic_df):\n",
    "    \"\"\"\n",
    "    Restructures the mismatched data so that each row represents a single (FID, SSR),\n",
    "    and each discrepancy appears in separate columns.\n",
    "\n",
    "    Args:\n",
    "        differences_df (DataFrame): DataFrame containing mismatched values.\n",
    "        epic_df (DataFrame): The original EPIC DataFrame to determine column order.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A structured DataFrame where mismatches are arranged in a single row per patient.\n",
    "    \"\"\"\n",
    "    # Standardize column names to prevent mismatches\n",
    "    differences_df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "\n",
    "    required_columns = [\"FID\", \"SSR\", \"EPIC Column\", \"SecuTrial Column\", \"EPIC Value\", \"SecuTrial Value\"]\n",
    "    missing_columns = [col for col in required_columns if col not in differences_df.columns]\n",
    "\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns in differences_df: {missing_columns}\")\n",
    "\n",
    "    # Resolve duplicates by taking the first occurrence\n",
    "    duplicate_check = differences_df.duplicated(subset=[\"FID\", \"SSR\", \"EPIC Column\"], keep=False)\n",
    "    if duplicate_check.any():\n",
    "        print(f\"Warning: {duplicate_check.sum()} duplicate rows found. Resolving by taking the first occurrence.\")\n",
    "\n",
    "    differences_df = differences_df.groupby([\"FID\", \"SSR\", \"EPIC Column\"], as_index=False).first()\n",
    "\n",
    "    # Pivot the table to make each discrepancy a separate column\n",
    "    pivoted_df = differences_df.pivot(index=[\"FID\", \"SSR\"], \n",
    "                                      columns=\"EPIC Column\", \n",
    "                                      values=[\"SecuTrial Value\", \"EPIC Value\"])\n",
    "\n",
    "    # Flatten multi-level column names\n",
    "    pivoted_df.columns = [f\"{col[1]}_st\" if col[0] == \"SecuTrial Value\" else f\"{col[1]}_ep\" \n",
    "                          for col in pivoted_df.columns]\n",
    "\n",
    "    # Reset index to include FID and SSR as columns\n",
    "    pivoted_df.reset_index(inplace=True)\n",
    "\n",
    "    # Debug: Print column names after pivot\n",
    "    #print(\"Columns after pivot:\", pivoted_df.columns)\n",
    "\n",
    "    # Ensure column order follows the order in the original EPIC DataFrame\n",
    "    column_order = [\"FID\", \"SSR\"]\n",
    "\n",
    "    # Extract base column names from epic_df (without prefix/suffix)\n",
    "    base_columns = [col for col in epic_df.columns if not col.startswith((\"FID\", \"SSR\"))]\n",
    "\n",
    "    # Ensure `_st` (SecuTrial) columns appear first, then `_ep` (EPIC) columns\n",
    "    for col in base_columns:\n",
    "        if f\"{col}_st\" in pivoted_df.columns:\n",
    "            column_order.append(f\"{col}_st\")\n",
    "        if f\"{col}_ep\" in pivoted_df.columns:\n",
    "            column_order.append(f\"{col}_ep\")\n",
    "\n",
    "    # Debug: Check if any expected columns are missing\n",
    "    missing_expected_columns = [col for col in column_order if col not in pivoted_df.columns]\n",
    "    if missing_expected_columns:\n",
    "        print(f\"⚠️ Warning: Some expected columns are missing after pivot: {missing_expected_columns}\")\n",
    "\n",
    "    # Select only available columns and reorder\n",
    "    column_order = [col for col in column_order if col in pivoted_df.columns]\n",
    "    pivoted_df = pivoted_df[column_order]\n",
    "\n",
    "    # Fill NaN values with an empty string for better readability\n",
    "    pivoted_df.fillna(\"\", inplace=True)\n",
    "\n",
    "    return pivoted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "restructured_df = restructure_mismatched_data(mismatched_results, df_EPIC_all_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "FID",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "SSR",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "enct.name_last_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "enct.name_last_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "enct.name_first_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "enct.name_first_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "enct.birth_date_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "enct.birth_date_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "enct.zip_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "enct.zip_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "enct.arrival_date_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "enct.arrival_date_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "enct.arrival_time_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "enct.arrival_time_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "enct.height_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "enct.height_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "enct.weight_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "enct.weight_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "enct.discharge_date_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "enct.discharge_date_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "enct.discharge_time_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "enct.discharge_time_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "flow.nih_admission_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "flow.nih_admission_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "flow.gcs_admission_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "flow.gcs_admission_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "flow.bp_syst_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "flow.bp_syst_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "flow.bp_diast_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "flow.bp_diast_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "flow.ivt_start_date_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "flow.ivt_start_date_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "flow.ivt_start_time_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "flow.ivt_start_time_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "flow.rtpa_dose_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "flow.rtpa_dose_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "flow.stroke_pre_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "flow.stroke_pre_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "flow.tia_pre_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "flow.tia_pre_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "flow.ich_pre_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "flow.ich_pre_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "flow.hypertension_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "flow.hypertension_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "flow.diabetes_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "flow.diabetes_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "flow.hyperlipidemia_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "flow.hyperlipidemia_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "flow.smoking_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "flow.smoking_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "flow.atrialfib_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "flow.atrialfib_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "flow.chd_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "flow.chd_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "flow.lowoutput_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "flow.lowoutput_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "flow.pad_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "flow.pad_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "img.firstimage_time_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "img.firstimage_time_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "img.firstangio_type_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "img.firstangio_type_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "img.iat_mech_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "img.iat_mech_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "img.follow_mra_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "img.follow_mra_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "img.follow_cta_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "img.follow_cta_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "img.follow_ultrasound_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "img.follow_ultrasound_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "img.follow_dsa_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "img.follow_dsa_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "img.follow_tte_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "img.follow_tte_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "img.follow_tee_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "img.follow_tee_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "img.follow_holter_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "img.follow_holter_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "lab.inr_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "lab.inr_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "lab.glucose_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "lab.glucose_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "lab.creatinine_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "lab.creatinine_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "lab.cholesterol_total_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "lab.cholesterol_total_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "lab.cholesterol_ldl_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "lab.cholesterol_ldl_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "med.aspirin_pre_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "med.aspirin_pre_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "med.clopidogrel_pre_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "med.clopidogrel_pre_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "med.prasugrel_pre_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "med.prasugrel_pre_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "med.ticagrelor_pre_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "med.ticagrelor_pre_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "med.vka_pre_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "med.vka_pre_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "med.rivaroxaban_pre_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "med.rivaroxaban_pre_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "med.apixaban_pre_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "med.apixaban_pre_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "med.parenteralanticg_pre_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "med.parenteralanticg_pre_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "med.antihypertensive_pre_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "med.antihypertensive_pre_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "med.antilipid_pre_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "med.antilipid_pre_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "med.hormone_pre_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "med.hormone_pre_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "med.treat_antiplatelet_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "med.treat_antiplatelet_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "med.treat_anticoagulant_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "med.treat_anticoagulant_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "med.treat_ivt_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "med.treat_ivt_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "mon.GROIN_PUNCTURE_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "mon.GROIN_PUNCTURE_ep",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "mon.DOOR_TO_GROIN_st",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "mon.DOOR_TO_GROIN_ep",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "a0b067bf-43b4-4aa4-9e09-ebc3e212cb40",
       "rows": [
        [
         "0",
         "10105076",
         "13681",
         "",
         "",
         "",
         "",
         "19580401 00:00",
         "19580104 00:00",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         ""
        ],
        [
         "1",
         "10105089",
         "13672",
         "",
         "",
         "Katja",
         "Katia",
         "",
         "",
         "",
         "",
         "",
         "",
         "20250414 08:05",
         "20250414 08:04",
         "175",
         "170",
         "100",
         "99",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "yes",
         "no",
         "",
         "",
         "",
         "",
         "no",
         "yes",
         "started before admission",
         "no",
         "20250414 09:13",
         "20250414 15:00",
         "68",
         "71"
        ],
        [
         "2",
         "10105149",
         "13682",
         "",
         "",
         "",
         "",
         "19440721 00:00",
         "19460721 00:00",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         ""
        ],
        [
         "3",
         "10105259",
         "13673",
         "",
         "",
         "",
         "",
         "19300504 00:00",
         "19300405 00:00",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "17",
         "15",
         "",
         "",
         "",
         "",
         "",
         "",
         "20240401 00:00",
         "20240104 00:00",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "20240401 00:00",
         "20250414 15:07",
         "CT-angiography",
         "1",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "no",
         "yes",
         "no",
         "yes",
         "started before admission",
         "no",
         "20250414 16:36",
         "20250414 00:00",
         "",
         ""
        ],
        [
         "4",
         "10105411",
         "13674",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "20240401 00:00",
         "20250414 03:00",
         "MR-angiography",
         "2",
         "",
         "",
         "no",
         "yes",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "",
         "no",
         "yes",
         "no",
         "yes",
         "no",
         "yes",
         "",
         "",
         "",
         ""
        ]
       ],
       "shape": {
        "columns": 120,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FID</th>\n",
       "      <th>SSR</th>\n",
       "      <th>enct.name_last_st</th>\n",
       "      <th>enct.name_last_ep</th>\n",
       "      <th>enct.name_first_st</th>\n",
       "      <th>enct.name_first_ep</th>\n",
       "      <th>enct.birth_date_st</th>\n",
       "      <th>enct.birth_date_ep</th>\n",
       "      <th>enct.zip_st</th>\n",
       "      <th>enct.zip_ep</th>\n",
       "      <th>...</th>\n",
       "      <th>med.treat_antiplatelet_st</th>\n",
       "      <th>med.treat_antiplatelet_ep</th>\n",
       "      <th>med.treat_anticoagulant_st</th>\n",
       "      <th>med.treat_anticoagulant_ep</th>\n",
       "      <th>med.treat_ivt_st</th>\n",
       "      <th>med.treat_ivt_ep</th>\n",
       "      <th>mon.GROIN_PUNCTURE_st</th>\n",
       "      <th>mon.GROIN_PUNCTURE_ep</th>\n",
       "      <th>mon.DOOR_TO_GROIN_st</th>\n",
       "      <th>mon.DOOR_TO_GROIN_ep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10105076</td>\n",
       "      <td>13681</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>19580401 00:00</td>\n",
       "      <td>19580104 00:00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10105089</td>\n",
       "      <td>13672</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Katja</td>\n",
       "      <td>Katia</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>started before admission</td>\n",
       "      <td>no</td>\n",
       "      <td>20250414 09:13</td>\n",
       "      <td>20250414 15:00</td>\n",
       "      <td>68</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10105149</td>\n",
       "      <td>13682</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>19440721 00:00</td>\n",
       "      <td>19460721 00:00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10105259</td>\n",
       "      <td>13673</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>19300504 00:00</td>\n",
       "      <td>19300405 00:00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>started before admission</td>\n",
       "      <td>no</td>\n",
       "      <td>20250414 16:36</td>\n",
       "      <td>20250414 00:00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10105411</td>\n",
       "      <td>13674</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        FID    SSR enct.name_last_st enct.name_last_ep enct.name_first_st  \\\n",
       "0  10105076  13681                                                          \n",
       "1  10105089  13672                                                  Katja   \n",
       "2  10105149  13682                                                          \n",
       "3  10105259  13673                                                          \n",
       "4  10105411  13674                                                          \n",
       "\n",
       "  enct.name_first_ep enct.birth_date_st enct.birth_date_ep enct.zip_st  \\\n",
       "0                        19580401 00:00     19580104 00:00               \n",
       "1              Katia                                                     \n",
       "2                        19440721 00:00     19460721 00:00               \n",
       "3                        19300504 00:00     19300405 00:00               \n",
       "4                                                                        \n",
       "\n",
       "  enct.zip_ep  ... med.treat_antiplatelet_st med.treat_antiplatelet_ep  \\\n",
       "0              ...                                                       \n",
       "1              ...                                                       \n",
       "2              ...                                                       \n",
       "3              ...                        no                       yes   \n",
       "4              ...                        no                       yes   \n",
       "\n",
       "  med.treat_anticoagulant_st med.treat_anticoagulant_ep  \\\n",
       "0                                                         \n",
       "1                         no                        yes   \n",
       "2                                                         \n",
       "3                         no                        yes   \n",
       "4                         no                        yes   \n",
       "\n",
       "           med.treat_ivt_st med.treat_ivt_ep mon.GROIN_PUNCTURE_st  \\\n",
       "0                                                                    \n",
       "1  started before admission               no        20250414 09:13   \n",
       "2                                                                    \n",
       "3  started before admission               no        20250414 16:36   \n",
       "4                        no              yes                         \n",
       "\n",
       "  mon.GROIN_PUNCTURE_ep mon.DOOR_TO_GROIN_st mon.DOOR_TO_GROIN_ep  \n",
       "0                                                                  \n",
       "1        20250414 15:00                   68                   71  \n",
       "2                                                                  \n",
       "3        20250414 00:00                                            \n",
       "4                                                                  \n",
       "\n",
       "[5 rows x 120 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restructured_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save new EPIC and sT dataframe as Excel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved successfully at: /Users/yaskhanloo/Developer/bern-storke-center/EPIC-export-validation/validation-files/df_EPIC_all_20250414_095129.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Define output directory and ensure it exists\n",
    "output_dir = base_dir / 'EPIC-export-validation/validation-files'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Generate timestamped file name and path\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file_path = output_dir / f\"df_EPIC_all_{timestamp}.xlsx\"\n",
    "\n",
    "# Save the DataFrame\n",
    "try:\n",
    "    df_EPIC_all.to_excel(output_file_path, index=False)\n",
    "    print(f\"File saved successfully at: {output_file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved successfully at: /Users/yaskhanloo/Developer/bern-storke-center/EPIC-export-validation/validation-files/df_secuTrial_w_REVAS_20250414_095131.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Generate the output file path\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file_path = output_dir / f\"df_secuTrial_w_REVAS_{timestamp}.xlsx\"\n",
    "\n",
    "# Save the DataFrame\n",
    "try:\n",
    "    df_secuTrial_w_REVAS.to_excel(output_file_path, index=False)\n",
    "    print(f\"File saved successfully at: {output_file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved successfully at: /Users/yaskhanloo/Developer/bern-storke-center/EPIC-export-validation/validation-files/report_mismatched_values_20250414_095135.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Generate the output file path\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file_path = output_dir / f\"report_mismatched_values_{timestamp}.xlsx\"\n",
    "\n",
    "# Save the DataFrame\n",
    "try:\n",
    "    restructured_df.to_excel(output_file_path, index=False)\n",
    "    print(f\"File saved successfully at: {output_file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving file: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
