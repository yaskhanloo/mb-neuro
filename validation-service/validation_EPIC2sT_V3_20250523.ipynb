{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automozed EPIC validation process against secuTrial data entries\n",
    "\n",
    "created by: Yasaman Safarkhanlo on 2024.10.07\n",
    "\n",
    "last modified: file name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import chardet\n",
    "import logging\n",
    "import re\n",
    "import io\n",
    "from typing import Dict, Any, Optional, Tuple, List, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging():\n",
    "    \"\"\"Configure logging for the application, works both locally and in Docker\"\"\"\n",
    "    # Detect environment: if running in Docker, use /app/data/logs; else, use ./logs\n",
    "    base_dir = os.getenv('BASE_DIR', '.')  # Docker should set BASE_DIR=/app/data\n",
    "    log_dir = Path(base_dir) / \"logs\"\n",
    "    log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_file = log_dir / f\"validation_service_{timestamp}.log\"\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return logging.getLogger('epic-validation')\n",
    "\n",
    "logger = setup_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_modify_secuTrial_export(df):\n",
    "    \"\"\"\n",
    "    Process secuTrial export dataframe by removing metadata rows and setting proper headers.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return (df.iloc[6:]\n",
    "                 .pipe(lambda x: x.set_axis(x.iloc[0], axis=1))\n",
    "                 .iloc[1:]\n",
    "                 .reset_index(drop=True)\n",
    "                 .dropna(how='all'))\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing secuTrial export: {e}\")\n",
    "        return df\n",
    "\n",
    "def safe_read_file(file_path, custom_reader=None):\n",
    "    \"\"\"\n",
    "    Safely reads a file (Excel or CSV), with an option for a custom reader function.\n",
    "    \"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    file_extension = file_path.suffix.lower()\n",
    "\n",
    "    try:\n",
    "        if file_extension in [\".xlsx\", \".xls\"]:\n",
    "            if custom_reader:\n",
    "                df = pd.read_excel(file_path, engine='openpyxl' if file_extension == \".xlsx\" else 'xlrd', header=None)\n",
    "            else:\n",
    "                df = pd.read_excel(file_path, engine='openpyxl' if file_extension == \".xlsx\" else 'xlrd')\n",
    "        elif file_extension == \".csv\":\n",
    "            encodings = ['utf-8', 'latin1', 'iso-8859-1', 'cp1252']\n",
    "            df = None\n",
    "            for encoding in encodings:\n",
    "                try:\n",
    "                    df = pd.read_csv(file_path, encoding=encoding)\n",
    "                    break\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "            if df is None:\n",
    "                raise ValueError(\"Could not read CSV with any encoding\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
    "        \n",
    "        result = custom_reader(df) if custom_reader else df\n",
    "\n",
    "        if result is None or result.empty:\n",
    "            logger.warning(f\"{file_path.name} is empty after processing.\")\n",
    "            return None\n",
    "\n",
    "        return result\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"File not found at {file_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading file at {file_path}: {e}\")\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 14:25:43,438 - epic-validation - INFO - Latest secuTrial export found: /Users/yaskhanloo/Developer/bern-storke-center/sT-files/export-20250520\n",
      "2025-05-23 14:25:43,439 - epic-validation - INFO - Latest EPIC export found: /Users/yaskhanloo/Developer/bern-storke-center/EPIC-files/export-20250516\n",
      "/Users/yaskhanloo/Library/Python/3.9/lib/python/site-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
      "/Users/yaskhanloo/Library/Python/3.9/lib/python/site-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
      "2025-05-23 14:25:59,443 - epic-validation - INFO - Data loaded successfully: secuTrial=(1803, 174), REVASC=(4980, 256), EPIC=(2543, 18)\n"
     ]
    }
   ],
   "source": [
    "base_dir = Path(\"/Users/yaskhanloo/Developer/bern-storke-center\")\n",
    "\n",
    "# Dynamically find the latest export folders\n",
    "latest_sT_export = max((base_dir / \"sT-files\").glob(\"export-*\"), key=lambda x: x.stat().st_mtime, default=None)\n",
    "latest_EPIC_export = max((base_dir / \"EPIC-files\").glob(\"export-*\"), key=lambda x: x.stat().st_mtime, default=None)\n",
    "\n",
    "if latest_sT_export:\n",
    "    secuTrial_base_dir = latest_sT_export\n",
    "    REVASC_base_dir = secuTrial_base_dir / \"REVASC\"\n",
    "    logger.info(f\"Latest secuTrial export found: {secuTrial_base_dir}\")\n",
    "else:\n",
    "    logger.error(\"No valid secuTrial export directory found.\")\n",
    "    raise FileNotFoundError(\"No valid secuTrial export directory found.\")\n",
    "\n",
    "if latest_EPIC_export:\n",
    "    epic_base_dir = latest_EPIC_export\n",
    "    logger.info(f\"Latest EPIC export found: {epic_base_dir}\")\n",
    "else:\n",
    "    logger.error(\"No valid EPIC export directory found.\")\n",
    "    raise FileNotFoundError(\"No valid EPIC export directory found.\")\n",
    "\n",
    "# Define file paths\n",
    "file_path_secuTrial = secuTrial_base_dir / 'SSR_cases_of_2024.xlsx'\n",
    "file_path_REVASC = REVASC_base_dir / 'report_SSR01_20250218-105747.xlsx'\n",
    "file_path_EPIC = epic_base_dir / 'encounters.xlsx'\n",
    "\n",
    "# Read files\n",
    "df_secuTrial = safe_read_file(file_path_secuTrial, custom_reader=read_and_modify_secuTrial_export)\n",
    "df_REVASC = safe_read_file(file_path_REVASC, custom_reader=read_and_modify_secuTrial_export)\n",
    "df_EPIC = safe_read_file(file_path_EPIC)\n",
    "\n",
    "# Log data frame sizes\n",
    "if df_secuTrial is not None and df_EPIC is not None and df_REVASC is not None:\n",
    "    logger.info(f\"Data loaded successfully: secuTrial={df_secuTrial.shape}, REVASC={df_REVASC.shape}, EPIC={df_EPIC.shape}\")\n",
    "else:\n",
    "    logger.warning(\"One or more dataframes failed to load.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge all EPIC files into one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_single_epic_file(file_path, merge_column, merged_df, prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Merge a single EPIC file into the main DataFrame with optional column prefixing.\n",
    "    \"\"\"\n",
    "    df = safe_read_file(file_path)\n",
    "    if df is None:\n",
    "        logger.warning(f\"Failed to read {file_path.name}\")\n",
    "        return merged_df\n",
    "\n",
    "    if merge_column not in df.columns:\n",
    "        logger.warning(f\"Merge column '{merge_column}' not found in {file_path.name}\")\n",
    "        return merged_df\n",
    "\n",
    "    # Add prefix to all columns except the merge column\n",
    "    if prefix:\n",
    "        df = df.rename(columns={col: f\"{prefix}{col}\" for col in df.columns if col != merge_column})\n",
    "\n",
    "    # Merge logic\n",
    "    if merged_df.empty:\n",
    "        result_df = df.copy()\n",
    "        logger.info(f\"Using {file_path.name} as base: shape={result_df.shape}\")\n",
    "    else:\n",
    "        result_df = merged_df.merge(df, on=merge_column, how=\"outer\")\n",
    "        logger.info(f\"Merged {file_path.name}: shape={df.shape} â†’ total={result_df.shape}\")\n",
    "\n",
    "    return result_df\n",
    "\n",
    "def find_merge_column(directory):\n",
    "    \"\"\"Find the correct merge column by checking the first file\"\"\"\n",
    "    directory = Path(directory)\n",
    "    file_patterns = [\"*.xlsx\", \"*.xls\", \"*.csv\"]\n",
    "    all_files = [f for pattern in file_patterns for f in directory.glob(pattern)]\n",
    "    \n",
    "    if not all_files:\n",
    "        return None\n",
    "        \n",
    "    # Check first file for common merge columns\n",
    "    first_file = all_files[0]\n",
    "    df = safe_read_file(first_file)\n",
    "    if df is not None and len(df.columns) > 1:\n",
    "        possible_columns = ['PAT_ENC_CSN_ID', 'PatientID', 'ID', 'Patient_ID', 'CSN_ID']\n",
    "        for col in possible_columns:\n",
    "            if col in df.columns:\n",
    "                logger.info(f\"Found merge column: {col}\")\n",
    "                return col\n",
    "        \n",
    "        logger.info(f\"Available columns in {first_file.name}: {list(df.columns)}\")\n",
    "        # Return the first column that looks like an ID\n",
    "        for col in df.columns:\n",
    "            if any(word in col.upper() for word in ['ID', 'CSN', 'PATIENT']):\n",
    "                logger.info(f\"Using merge column: {col}\")\n",
    "                return col\n",
    "    \n",
    "    return 'PAT_ENC_CSN_ID'  # Default fallback\n",
    "\n",
    "def merge_all_epic_files(directory, merge_column=None):\n",
    "    \"\"\"\n",
    "    Merges all EPIC files in a directory based on a specific column, in a defined order.\n",
    "    \"\"\"\n",
    "    directory = Path(directory)\n",
    "    if not directory.exists():\n",
    "        logger.error(f\"Directory not found: {directory}\")\n",
    "        raise FileNotFoundError(f\"{directory} does not exist.\")\n",
    "\n",
    "    file_patterns = [\"*.xlsx\", \"*.xls\", \"*.csv\"]\n",
    "    all_files = [f for pattern in file_patterns for f in directory.glob(pattern)]\n",
    "    logger.info(f\"Found {len(all_files)} data files in {directory.name}\")\n",
    "\n",
    "    # Auto-detect merge column if not provided\n",
    "    if merge_column is None:\n",
    "        merge_column = find_merge_column(directory)\n",
    "        if merge_column is None:\n",
    "            logger.error(\"Could not find a suitable merge column\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    file_order = ['enc', 'flow', 'imag', 'img', 'lab', 'med', 'mon']\n",
    "\n",
    "    def file_priority(file_path):\n",
    "        name = file_path.stem.lower()\n",
    "        for i, keyword in enumerate(file_order):\n",
    "            if keyword in name:\n",
    "                return i\n",
    "        return len(file_order)\n",
    "\n",
    "    def get_prefix(filename):\n",
    "        name = filename.lower()\n",
    "        if 'enc' in name: return 'enct.'\n",
    "        if 'flow' in name: return 'flow.'\n",
    "        if 'imag' in name or 'img' in name: return 'img.'\n",
    "        if 'lab' in name: return 'lab.'\n",
    "        if 'med' in name: return 'med.'\n",
    "        if 'mon' in name: return 'mon.'\n",
    "        return \"\"\n",
    "\n",
    "    sorted_files = sorted(all_files, key=file_priority)\n",
    "\n",
    "    merged_df = pd.DataFrame()\n",
    "    for file_path in sorted_files:\n",
    "        prefix = get_prefix(file_path.stem)\n",
    "        merged_df = merge_single_epic_file(file_path, merge_column, merged_df, prefix)\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 14:25:59,484 - epic-validation - INFO - Listing files in EPIC export directory: /Users/yaskhanloo/Developer/bern-storke-center/EPIC-files/export-20250516\n",
      "2025-05-23 14:25:59,485 - epic-validation - INFO - Found 6 data files in export-20250516\n",
      "2025-05-23 14:25:59,758 - epic-validation - INFO - Using encounters.xlsx as base: shape=(2543, 18)\n",
      "2025-05-23 14:26:00,139 - epic-validation - INFO - Merged flowsheet.xlsx: shape=(2543, 31) â†’ total=(2543, 48)\n",
      "2025-05-23 14:26:00,337 - epic-validation - INFO - Merged imaging.xlsx: shape=(2543, 16) â†’ total=(2543, 63)\n",
      "2025-05-23 14:26:00,522 - epic-validation - INFO - Merged lab.xlsx: shape=(2543, 14) â†’ total=(2543, 76)\n",
      "2025-05-23 14:26:00,805 - epic-validation - INFO - Merged medication.xlsx: shape=(2543, 23) â†’ total=(2543, 98)\n",
      "2025-05-23 14:26:01,029 - epic-validation - INFO - Merged monitor.xlsx: shape=(2543, 18) â†’ total=(2543, 115)\n",
      "2025-05-23 14:26:01,030 - epic-validation - INFO - Final merged DataFrame shape: (2543, 115)\n",
      "2025-05-23 14:26:01,084 - epic-validation - INFO - Merged data saved to: /Users/yaskhanloo/Developer/bern-storke-center/EPIC-files/merged_epic_files/merged_epic_data.csv\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # List all files in the EPIC export directory\n",
    "    logger.info(f\"Listing files in EPIC export directory: {epic_base_dir}\")\n",
    "    all_files = list(Path(epic_base_dir).glob(\"*\"))\n",
    "    for file in all_files:\n",
    "        logger.debug(f\"  - {file.name}\")\n",
    "    \n",
    "    # Merge all EPIC files\n",
    "    df_EPIC_all = merge_all_epic_files(epic_base_dir, merge_column=\"PAT_ENC_CSN_ID\")\n",
    "    \n",
    "    if not df_EPIC_all.empty:\n",
    "        logger.info(f\"Final merged DataFrame shape: {df_EPIC_all.shape}\")\n",
    "\n",
    "        # Save the merged dataframe\n",
    "        output_path = Path(base_dir) / \"EPIC-files/merged_epic_files/merged_epic_data.csv\"\n",
    "        df_EPIC_all.to_csv(output_path, index=False)\n",
    "        logger.info(f\"Merged data saved to: {output_path}\")\n",
    "    else:\n",
    "        logger.warning(\"Merged DataFrame is empty. Nothing saved.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    logger.error(f\"Error: Directory not found - {e}\")\n",
    "except Exception as e:\n",
    "    logger.exception(f\"An unexpected error occurred during merging.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Merging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REVASC merge with sT - single year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_secuTrial_with_REVASC(df_secuTrial, df_REVASC, logger):\n",
    "    \"\"\"Merge REVASC data into secuTrial DataFrame.\"\"\"\n",
    "    try:\n",
    "        merged_df = df_secuTrial.merge(\n",
    "            df_REVASC,\n",
    "            how='left',\n",
    "            left_on='Case ID',\n",
    "            right_on='CaseID',\n",
    "            suffixes=('', '.revas')\n",
    "        )\n",
    "        merged_df.drop(columns=['CaseID'], inplace=True, errors='ignore')\n",
    "        merged_df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        logger.info(f\"Successfully merged secuTrial + REVASC: {merged_df.shape}\")\n",
    "        return merged_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"REVASC merge failed: {e}. Using secuTrial data only.\")\n",
    "        return df_secuTrial.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 14:26:01,108 - epic-validation - INFO - Successfully merged secuTrial + REVASC: (1803, 429)\n"
     ]
    }
   ],
   "source": [
    "df_secuTrial_w_REVAS = merge_secuTrial_with_REVASC(df_secuTrial, df_REVASC, logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add FID and SSR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_id_log(id_log_path, logger):\n",
    "    \"\"\"Load and process the ID log file.\"\"\"\n",
    "    try:\n",
    "        id_log = pd.read_excel(id_log_path)\n",
    "        logger.info(f\"ID log original columns: {list(id_log.columns)}\")\n",
    "        \n",
    "        # Set first row as headers\n",
    "        id_log.columns = id_log.iloc[0]\n",
    "        id_log = id_log.iloc[1:].reset_index(drop=True)\n",
    "        logger.info(f\"ID log columns after header fix: {list(id_log.columns)}\")\n",
    "        \n",
    "        # Map the actual column names we found\n",
    "        column_mapping = {}\n",
    "        for col in id_log.columns:\n",
    "            if pd.isna(col):  # Skip NaN columns\n",
    "                continue\n",
    "            col_str = str(col).strip()\n",
    "            if 'Fall-Nr.' in col_str:\n",
    "                column_mapping[col] = 'FID'\n",
    "            elif 'SSR Identification' in col_str:\n",
    "                column_mapping[col] = 'SSR'\n",
    "        \n",
    "        logger.info(f\"Column mapping: {column_mapping}\")\n",
    "        id_log.rename(columns=column_mapping, inplace=True)\n",
    "        \n",
    "        # Remove NaN columns\n",
    "        id_log = id_log.loc[:, ~id_log.columns.isna()]\n",
    "        \n",
    "        # Check if we have the required columns\n",
    "        if 'FID' not in id_log.columns or 'SSR' not in id_log.columns:\n",
    "            logger.error(f\"Required columns not found. Available: {list(id_log.columns)}\")\n",
    "            logger.error(\"Expected to find 'Fall-Nr.' and 'SSR Identification' columns\")\n",
    "            return None\n",
    "            \n",
    "        # Convert to appropriate data types\n",
    "        id_log['FID'] = pd.to_numeric(id_log['FID'], errors='coerce')\n",
    "        id_log['SSR'] = pd.to_numeric(id_log['SSR'], errors='coerce')\n",
    "        \n",
    "        # Remove rows with missing FID or SSR\n",
    "        initial_count = len(id_log)\n",
    "        id_log = id_log.dropna(subset=['FID', 'SSR'])\n",
    "        final_count = len(id_log)\n",
    "        \n",
    "        if final_count < initial_count:\n",
    "            logger.warning(f\"Removed {initial_count - final_count} rows with missing FID/SSR\")\n",
    "            \n",
    "        logger.info(f\"Loaded ID log with {final_count} valid entries\")\n",
    "        return id_log\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load ID log: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_patient_ids(df_epic, df_secuTrial, id_log, logger):\n",
    "    \"\"\"Add FID and SSR columns to both dataframes.\"\"\"\n",
    "    \n",
    "    # Add FID to EPIC data\n",
    "    if 'img.FID' in df_epic.columns:\n",
    "        df_epic['FID'] = df_epic['img.FID'].fillna(0).astype(int)\n",
    "        df_epic.insert(0, 'FID', df_epic.pop('FID'))\n",
    "    else:\n",
    "        logger.warning(\"img.FID column not found in EPIC data\")\n",
    "    \n",
    "    # Add SSR to secuTrial data\n",
    "    if 'Case ID' in df_secuTrial.columns:\n",
    "        df_secuTrial['SSR'] = df_secuTrial['Case ID'].str.extract(r'(\\d+)$').astype(int)\n",
    "        df_secuTrial.insert(1, 'SSR', df_secuTrial.pop('SSR'))\n",
    "        # Clean up any 'nan' columns\n",
    "        df_secuTrial = df_secuTrial.drop(columns=['nan'], errors='ignore')\n",
    "    else:\n",
    "        logger.warning(\"Case ID column not found in secuTrial data\")\n",
    "    \n",
    "    # Merge with ID log\n",
    "    if id_log is not None:\n",
    "        df_epic = df_epic.merge(id_log[['FID', 'SSR']], on='FID', how='left')\n",
    "        df_epic.insert(1, 'SSR', df_epic.pop('SSR'))\n",
    "        \n",
    "        df_secuTrial = df_secuTrial.merge(id_log[['SSR', 'FID']], on='SSR', how='left')\n",
    "        df_secuTrial.insert(0, 'FID', df_secuTrial.pop('FID'))\n",
    "        \n",
    "        logger.info(\"Successfully added patient IDs to both dataframes\")\n",
    "    \n",
    "    return df_epic, df_secuTrial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matching_patients(df_epic, df_secuTrial, logger):\n",
    "    \"\"\"Find patients that exist in both datasets.\"\"\"\n",
    "    \n",
    "    # Find common patients by FID and SSR\n",
    "    common_keys = df_secuTrial[['FID', 'SSR']].merge(\n",
    "        df_epic[['FID', 'SSR']], \n",
    "        on=['FID', 'SSR'], \n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    # Filter to matching patients only\n",
    "    df_epic_common = df_epic.merge(common_keys, on=['FID', 'SSR'], how='inner')\n",
    "    df_secuTrial_common = df_secuTrial.merge(common_keys, on=['FID', 'SSR'], how='inner')\n",
    "    \n",
    "    logger.info(f\"Found {len(common_keys)} matching patients\")\n",
    "    logger.info(f\"EPIC common shape: {df_epic_common.shape}\")\n",
    "    logger.info(f\"secuTrial common shape: {df_secuTrial_common.shape}\")\n",
    "    \n",
    "    return df_epic_common, df_secuTrial_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_missing_patients(df_epic, df_secuTrial, logger):\n",
    "    \"\"\"Find patients that exist in only one dataset.\"\"\"\n",
    "    \n",
    "    # Patients only in secuTrial\n",
    "    df_secuTrial_only = df_secuTrial.merge(\n",
    "        df_epic[['FID', 'SSR']], \n",
    "        on=['FID', 'SSR'], \n",
    "        how='left', \n",
    "        indicator=True\n",
    "    ).query('_merge == \"left_only\"').drop(columns=['_merge'])\n",
    "    \n",
    "    # Patients only in EPIC\n",
    "    df_epic_only = df_epic.merge(\n",
    "        df_secuTrial[['FID', 'SSR']], \n",
    "        on=['FID', 'SSR'], \n",
    "        how='left', \n",
    "        indicator=True\n",
    "    ).query('_merge == \"left_only\"').drop(columns=['_merge'])\n",
    "    \n",
    "    logger.info(f\"Patients only in secuTrial: {len(df_secuTrial_only)}\")\n",
    "    logger.info(f\"Patients only in EPIC: {len(df_epic_only)}\")\n",
    "    \n",
    "    return df_secuTrial_only, df_epic_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_patient_analysis(df_epic_common, df_secuTrial_common, \n",
    "                         df_epic_only, df_secuTrial_only, output_dir, logger):\n",
    "    \"\"\"Save patient matching analysis to Excel files.\"\"\"\n",
    "    \n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Save common patients\n",
    "    common_file = output_dir / f\"common_patients_{timestamp}.xlsx\"\n",
    "    with pd.ExcelWriter(common_file) as writer:\n",
    "        df_secuTrial_common.to_excel(writer, sheet_name=\"secuTrial_common\", index=False)\n",
    "        df_epic_common.to_excel(writer, sheet_name=\"EPIC_common\", index=False)\n",
    "    \n",
    "    # Save missing patients (with only relevant columns)\n",
    "    missing_file = output_dir / f\"missing_patients_{timestamp}.xlsx\"\n",
    "    \n",
    "    # Select relevant columns for comparison\n",
    "    secuTrial_cols = ['FID', 'SSR', 'Last name', 'First name', 'DOB', 'Arrival at hospital']\n",
    "    epic_cols = ['FID', 'SSR', 'enct.name_last', 'enct.name_first', 'enct.birth_date', 'enct.arrival_date']\n",
    "    \n",
    "    # Only include columns that exist\n",
    "    secuTrial_subset = df_secuTrial_only[[col for col in secuTrial_cols if col in df_secuTrial_only.columns]]\n",
    "    epic_subset = df_epic_only[[col for col in epic_cols if col in df_epic_only.columns]]\n",
    "    \n",
    "    with pd.ExcelWriter(missing_file) as writer:\n",
    "        secuTrial_subset.to_excel(writer, sheet_name=\"only_in_secuTrial\", index=False)\n",
    "        epic_subset.to_excel(writer, sheet_name=\"only_in_EPIC\", index=False)\n",
    "    \n",
    "    logger.info(f\"Patient analysis saved to {common_file} and {missing_file}\")\n",
    "    \n",
    "    return df_epic_common, df_secuTrial_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_patient_matching(df_epic, df_secuTrial, id_log_path, output_dir, logger):\n",
    "    \"\"\"\n",
    "    Complete patient matching workflow.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (df_epic_common, df_secuTrial_common) - datasets with only matching patients\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load ID log\n",
    "    id_log = load_and_process_id_log(id_log_path, logger)\n",
    "    if id_log is None:\n",
    "        return df_epic, df_secuTrial  # Return original data if ID log fails\n",
    "    \n",
    "    # Add patient IDs\n",
    "    df_epic, df_secuTrial = add_patient_ids(df_epic, df_secuTrial, id_log, logger)\n",
    "    \n",
    "    # Find matching and missing patients\n",
    "    df_epic_common, df_secuTrial_common = find_matching_patients(df_epic, df_secuTrial, logger)\n",
    "    df_secuTrial_only, df_epic_only = find_missing_patients(df_epic, df_secuTrial, logger)\n",
    "    \n",
    "    # Save analysis\n",
    "    df_epic_common, df_secuTrial_common = save_patient_analysis(\n",
    "        df_epic_common, df_secuTrial_common, \n",
    "        df_epic_only, df_secuTrial_only, \n",
    "        output_dir, logger\n",
    "    )\n",
    "    \n",
    "    return df_epic_common, df_secuTrial_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 14:26:01,415 - epic-validation - INFO - ID log original columns: ['Anzahl', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4', 'Unnamed: 5', 'Unnamed: 6', 'Unnamed: 7', 1777, 'Unnamed: 9', 'Unnamed: 10', 'Unnamed: 11', 'Unnamed: 12']\n",
      "2025-05-23 14:26:01,415 - epic-validation - INFO - ID log columns after header fix: ['Kommentar / Studie', 'Pat.nr.', 'SSR Identification SSR-INS-000....', 'Lysenr', 'Fall-Nr.', 'Name', 'Vorname', 'Geburts- datum', 'Eintritt', 'Geschlecht', 'Follow up done', nan, nan]\n",
      "2025-05-23 14:26:01,416 - epic-validation - INFO - Column mapping: {'SSR Identification SSR-INS-000....': 'SSR', 'Fall-Nr.': 'FID'}\n",
      "2025-05-23 14:26:01,418 - epic-validation - WARNING - Removed 2 rows with missing FID/SSR\n",
      "2025-05-23 14:26:01,418 - epic-validation - INFO - Loaded ID log with 1775 valid entries\n",
      "2025-05-23 14:26:01,446 - epic-validation - INFO - Successfully added patient IDs to both dataframes\n",
      "2025-05-23 14:26:01,461 - epic-validation - INFO - Found 1435 matching patients\n",
      "2025-05-23 14:26:01,461 - epic-validation - INFO - EPIC common shape: (1435, 117)\n",
      "2025-05-23 14:26:01,462 - epic-validation - INFO - secuTrial common shape: (1435, 430)\n",
      "2025-05-23 14:26:01,544 - epic-validation - INFO - Patients only in secuTrial: 368\n",
      "2025-05-23 14:26:01,545 - epic-validation - INFO - Patients only in EPIC: 1109\n",
      "2025-05-23 14:26:09,227 - epic-validation - INFO - Patient analysis saved to /Users/yaskhanloo/Developer/bern-storke-center/EPIC-export-validation/validation-files/common_patients_20250523_142601.xlsx and /Users/yaskhanloo/Developer/bern-storke-center/EPIC-export-validation/validation-files/missing_patients_20250523_142601.xlsx\n",
      "2025-05-23 14:26:09,230 - epic-validation - INFO - Patient matching completed successfully!\n"
     ]
    }
   ],
   "source": [
    "output_dir = base_dir / 'EPIC-export-validation/validation-files'\n",
    "id_log_path = base_dir / 'EPIC2sT-pipeline/Identification_log_SSR_2024_ohne PW_26.03.25.xlsx'\n",
    "\n",
    "df_epic_common, df_secuTrial_common = process_patient_matching(\n",
    "    df_EPIC_all, \n",
    "    df_secuTrial_w_REVAS, \n",
    "    id_log_path, \n",
    "    output_dir, \n",
    "    logger\n",
    ")\n",
    "\n",
    "logger.info(\"Patient matching completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map Key!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define reusable mappings with better variable names\n",
    "BOOLEAN_TO_YES_NO = {0: 'no', 1: 'yes', False: 'no', True: 'yes'}\n",
    "BILATERAL_LOCATION_MAPPING = {0: 'no', 1: '', 2: 'right', 3: 'left', 4: 'bilateral'}\n",
    "PROSTHETIC_VALVE_TYPES = {0: 'None', 1: 'Biological', 2: 'Mechanical'}\n",
    "\n",
    "# Image type mapping - all CT types = 1, all MRI types = 2\n",
    "IMAGE_TYPE_TO_NUMERIC = {\n",
    "    'CT': 1, \n",
    "    'MRI': 2, \n",
    "    'CT (external)': 1, \n",
    "    'MRI (external)': 2,\n",
    "    'CT-angiography': 1, \n",
    "    'MR-angiography': 2\n",
    "}\n",
    "\n",
    "TRANSPORT_METHODS = {1: 'Ambulance', 2: 'Helicopter', 3: 'Other (taxi,self,relatives,friends...)'}\n",
    "\n",
    "DISCHARGE_DESTINATIONS = {\n",
    "    1: 'Home', \n",
    "    3: 'Rehabilitation Hospital', \n",
    "    2: 'Other acute care hospital', \n",
    "    4: 'Nursing home, palliative care center, or other medical facility'\n",
    "}\n",
    "\n",
    "BOOLEAN_COLUMNS = [\n",
    "    'flow.iat_stentintracran', \n",
    "    'flow.iat_stentextracran', \n",
    "    'flow.stroke_pre', \n",
    "    'flow.tia_pre', \n",
    "    'flow.ich_pre',\n",
    "    'flow.hypertension', \n",
    "    'flow.diabetes', \n",
    "    'flow.hyperlipidemia', \n",
    "    'flow.smoking', \n",
    "    'flow.atrialfib', \n",
    "    'flow.chd',\n",
    "    'flow.lowoutput', \n",
    "    'flow.pad', \n",
    "    'flow.decompression', \n",
    "    'img.iat_mech', \n",
    "    'img.follow_mra', \n",
    "    'img.follow_cta',\n",
    "    'img.follow_ultrasound', \n",
    "    'img.follow_dsa', \n",
    "    'img.follow_tte', \n",
    "    'img.follow_tee', \n",
    "    'img.follow_holter',\n",
    "    'med.aspirin_pre', \n",
    "    'med.clopidogrel_pre', \n",
    "    'med.prasugrel_pre', \n",
    "    'med.ticagrelor_pre', \n",
    "    'med.dipyridamole_pre',\n",
    "    'med.vka_pre', \n",
    "    'med.rivaroxaban_pre', \n",
    "    'med.dabigatran_pre', \n",
    "    'med.apixaban_pre', \n",
    "    'med.edoxaban_pre',\n",
    "    'med.parenteralanticg_pre', \n",
    "    'med.antihypertensive_pre', \n",
    "    'med.antilipid_pre', \n",
    "    'med.hormone_pre',\n",
    "    'med.treat_antiplatelet', \n",
    "    'med.treat_anticoagulant', \n",
    "    'med.treat_ivt'\n",
    "]\n",
    "\n",
    "BILATERAL_ANATOMY_COLUMNS = ['flow.mca', 'flow.aca', 'flow.pca', 'flow.vertebrobasilar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_epic_value_mappings():\n",
    "    \"\"\"\n",
    "    Creates and returns the value mappings dictionary for EPIC data standardization.\n",
    "    This function is Docker-compatible and uses only standard Python libraries.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary mapping column names to their value transformation mappings\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start with specific column mappings\n",
    "    epic_value_mappings = {\n",
    "        'enct.non_swiss': {True: 'yes'},\n",
    "        'enct.sex': {1: 'Male', 2: 'Female'},\n",
    "        'enct.transport': TRANSPORT_METHODS,\n",
    "        'enct.discharge_destinat': DISCHARGE_DESTINATIONS,\n",
    "        'flow.firstangio_result': {2: 'no', 3: 'yes'},\n",
    "        'flow.prostheticvalves': PROSTHETIC_VALVE_TYPES,\n",
    "        'img.firstimage_type': IMAGE_TYPE_TO_NUMERIC,\n",
    "        'img.firstangio_type': IMAGE_TYPE_TO_NUMERIC\n",
    "    }\n",
    "    \n",
    "    # Add boolean mappings for all boolean columns\n",
    "    for column_name in BOOLEAN_COLUMNS:\n",
    "        epic_value_mappings[column_name] = BOOLEAN_TO_YES_NO\n",
    "    \n",
    "    # Add bilateral mappings for anatomy columns\n",
    "    for column_name in BILATERAL_ANATOMY_COLUMNS:\n",
    "        epic_value_mappings[column_name] = BILATERAL_LOCATION_MAPPING\n",
    "    \n",
    "    return epic_value_mappings\n",
    "\n",
    "# Create the mappings\n",
    "EPIC_VALUE_MAPPINGS = create_epic_value_mappings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_boolean_values(value: Any) -> Union[str, Any]:\n",
    "    \"\"\"\n",
    "    Standardize boolean values to consistent 'yes'/'no' format.\n",
    "    \n",
    "    Args:\n",
    "        value: Input value to standardize\n",
    "        \n",
    "    Returns:\n",
    "        Standardized value ('yes', 'no', or original value if not boolean-like)\n",
    "    \"\"\"\n",
    "    if pd.isna(value):\n",
    "        return pd.NA\n",
    "    \n",
    "    if isinstance(value, bool):\n",
    "        return \"yes\" if value else \"no\"\n",
    "    elif isinstance(value, (int, float)):\n",
    "        return \"yes\" if value else \"no\"\n",
    "    elif isinstance(value, str):\n",
    "        value_lower = value.lower().strip()\n",
    "        if value_lower in ['true', 'yes', 'y', '1', 't']:\n",
    "            return \"yes\"\n",
    "        elif value_lower in ['false', 'no', 'n', '0', 'f']:\n",
    "            return \"no\"\n",
    "    \n",
    "    return str(value)\n",
    "\n",
    "\n",
    "def convert_value_to_target_type(value: Any, target_type: str) -> Any:\n",
    "    \"\"\"\n",
    "    Convert value to specified target type with proper formatting.\n",
    "    \n",
    "    Args:\n",
    "        value: Input value to convert\n",
    "        target_type: Target data type (string specification)\n",
    "        \n",
    "    Returns:\n",
    "        Converted value or pd.NA if conversion fails\n",
    "    \"\"\"\n",
    "    if pd.isna(value):\n",
    "        return pd.NA\n",
    "        \n",
    "    # Handle empty strings\n",
    "    if value == '':\n",
    "        return pd.NA\n",
    "        \n",
    "    # Handle various data types\n",
    "    if not isinstance(target_type, str):\n",
    "        return value  # If no type specified, return as is\n",
    "        \n",
    "    target_type_lower = target_type.lower()\n",
    "        \n",
    "    # Check for float with decimal specification (e.g., float-1, float-2)\n",
    "    float_match = re.match(r'float-(\\d+)', target_type_lower)\n",
    "    if float_match:\n",
    "        try:\n",
    "            decimal_places = int(float_match.group(1))\n",
    "            float_val = float(value)\n",
    "            return round(float_val, decimal_places)\n",
    "        except (ValueError, TypeError):\n",
    "            return pd.NA\n",
    "    \n",
    "    # Integer types\n",
    "    if target_type_lower in ['int', 'integer', 'int64', 'int32']:\n",
    "        try:\n",
    "            return int(float(value))\n",
    "        except (ValueError, TypeError):\n",
    "            return pd.NA\n",
    "            \n",
    "    # Float types\n",
    "    elif target_type_lower in ['float', 'double', 'numeric', 'float64', 'float32']:\n",
    "        try:\n",
    "            return float(value)\n",
    "        except (ValueError, TypeError):\n",
    "            return pd.NA\n",
    "            \n",
    "    # Date/time types\n",
    "    elif target_type_lower in ['date', 'datetime', 'timestamp']:\n",
    "        try:\n",
    "            # Convert to datetime and then to yyyymmdd hh:mm format\n",
    "            dt = pd.to_datetime(value)\n",
    "            return dt.strftime('%Y%m%d %H:%M')\n",
    "        except (ValueError, TypeError, AttributeError):\n",
    "            return pd.NA\n",
    "            \n",
    "    # Boolean types\n",
    "    elif target_type_lower in ['bool', 'boolean']:\n",
    "        return standardize_boolean_values(value)\n",
    "        \n",
    "    # Default to string for text, categorical, etc.\n",
    "    else:\n",
    "        return str(value) if value is not None else pd.NA\n",
    "\n",
    "\n",
    "def values_are_equivalent(val1: Any, val2: Any, target_type: str) -> bool:\n",
    "    \"\"\"\n",
    "    Compare two values with type-aware equivalence checking.\n",
    "    \n",
    "    Args:\n",
    "        val1: First value to compare\n",
    "        val2: Second value to compare\n",
    "        target_type: Target data type for comparison context\n",
    "        \n",
    "    Returns:\n",
    "        True if values are equivalent, False otherwise\n",
    "    \"\"\"\n",
    "    # Handle NaN values consistently\n",
    "    if pd.isna(val1) and pd.isna(val2):\n",
    "        return True\n",
    "    elif pd.isna(val1) or pd.isna(val2):\n",
    "        return False\n",
    "        \n",
    "    target_type_lower = target_type.lower() if isinstance(target_type, str) else ''\n",
    "        \n",
    "    # Check for float with decimal specification (e.g., float-1, float-2)\n",
    "    float_match = re.match(r'float-(\\d+)', target_type_lower)\n",
    "    if float_match:\n",
    "        try:\n",
    "            decimal_places = int(float_match.group(1))\n",
    "            val1_rounded = round(float(val1), decimal_places)\n",
    "            val2_rounded = round(float(val2), decimal_places)\n",
    "            return val1_rounded == val2_rounded\n",
    "        except (ValueError, TypeError):\n",
    "            return False\n",
    "    \n",
    "    # Boolean comparison (standardized to yes/no)\n",
    "    if target_type_lower in ['bool', 'boolean']:\n",
    "        val1_std = standardize_boolean_values(val1)\n",
    "        val2_std = standardize_boolean_values(val2)\n",
    "        return val1_std == val2_std\n",
    "    \n",
    "    # Numeric types comparison\n",
    "    if isinstance(val1, (int, float)) and isinstance(val2, (int, float)):\n",
    "        try:\n",
    "            return abs(float(val1) - float(val2)) < 1e-6\n",
    "        except (ValueError, TypeError):\n",
    "            return False\n",
    "    \n",
    "    # Date comparison (already in string format)\n",
    "    if target_type_lower in ['date', 'datetime', 'timestamp']:\n",
    "        return str(val1) == str(val2)\n",
    "        \n",
    "    # String comparison (case insensitive)\n",
    "    elif isinstance(val1, str) and isinstance(val2, str):\n",
    "        return val1.strip().lower() == val2.strip().lower()\n",
    "        \n",
    "    # Default comparison\n",
    "    else:\n",
    "        return str(val1) == str(val2)\n",
    "\n",
    "\n",
    "def apply_value_mappings_to_dataframe(df: pd.DataFrame, value_mappings: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply value mappings to a DataFrame, transforming specified columns.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        value_mappings: Dictionary mapping column names to their value transformations\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with transformed values\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    modified_columns = []\n",
    "    for column_name, mapping_dict in value_mappings.items():\n",
    "        if column_name in df_copy.columns:\n",
    "            # Apply mapping, keeping original values for unmapped items\n",
    "            df_copy[column_name] = df_copy[column_name].map(\n",
    "                lambda x: mapping_dict.get(x, x) if not pd.isna(x) else x\n",
    "            )\n",
    "            modified_columns.append(column_name)\n",
    "    \n",
    "    print(f\"Applied value mappings to {len(modified_columns)} columns:\")\n",
    "    for col in modified_columns[:10]:  # Show first 10 modified columns\n",
    "        print(f\"  - {col}\")\n",
    "    if len(modified_columns) > 10:\n",
    "        print(f\"  ... and {len(modified_columns) - 10} more columns\")\n",
    "    \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare ep and sT dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### comparison application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mismatch report generation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
